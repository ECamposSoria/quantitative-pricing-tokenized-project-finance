# **GU√çA COMPLETA DE IMPLEMENTACI√ìN \- VERSI√ìN ACTUALIZADA**

## **Modelo Cuantitativo de Project Finance Tokenizado para Constelaci√≥n LEO IoT**

## **62 Tareas Totales \- Roadmap Rev. 4-Nov-2025**

---

## **üìã √çNDICE DE WORK PACKAGES**

* **WP-01:** Setup y Arquitectura (T-001, T-002, T-012)  
* **WP-02:** M√≥dulo CFADS y Ratios \- CORREGIDO (T-003, T-003B, T-004, T-005, T-005B, T-013, T-046, T-047)  
* **WP-03:** Waterfall de Pagos \- CON MRA (T-006, T-014, T-015, T-016, T-017, T-020)  
* **WP-04:** Pricing y Curvas (T-007, T-008, T-009, T-018, T-019)  
* **WP-05:** Riesgo Crediticio (T-010, T-033, T-034, T-035, T-037)  
* **WP-06:** Stress Testing \- AMPLIADO (T-038, T-038B, T-039, T-040, T-041, T-042, T-043)  
* **WP-07:** Simulaci√≥n Monte Carlo (T-021, T-022, T-023, T-024, T-025, T-029, T-030, T-031)  
* **WP-08:** Pricing Estoc√°stico (T-026, T-027, T-028, T-044)  
* **WP-09:** Visualizaciones (T-032)  
* **WP-10:** Optimizaci√≥n (T-036)  
* **WP-11:** Derivados (T-045)  
* **WP-14:** AMM Simplificado \- REDEFINIDO (T-053, TAMM01, TAMM02, TAMM05, TAMM06, TAMM09, TAMM11)  
* **WP-12:** Entregables (T-048, T-049, T-050)  
* **WP-13:** Testing y QA (T-051, T-052)

---

## **WP-01: SETUP Y ARQUITECTURA**

### **T-001: Setup inicial proyecto**

**¬øEn qu√© consiste?**  
 Crear la estructura base del repositorio Python con todas las carpetas, archivos de configuraci√≥n y control de versiones necesarios para el proyecto cuantitativo.

**Relaciones con otras tareas:**

* **Prerrequisito de:** TODAS las dem√°s tareas (T-002 a TAMM11)  
* **Habilita directamente:** T-002 (Documentaci√≥n de requerimientos), T-012 (Arquitectura de m√≥dulos)

**Qu√© se debe implementar:**

* Estructura completa de directorios incluyendo carpetas separadas para c√≥digo fuente del modelo cuantitativo, suite de tests unitarios y de integraci√≥n, datasets de entrada con par√°metros calibrados, resultados de simulaciones y outputs, notebooks de an√°lisis Jupyter, documentaci√≥n t√©cnica y acad√©mica  
* Sistema de gesti√≥n de dependencias mediante archivo requirements.txt que especifique todas las librer√≠as necesarias para computaci√≥n num√©rica cient√≠fica, validaci√≥n de datos mediante esquemas tipados, visualizaci√≥n avanzada de resultados financieros, testing automatizado y an√°lisis estad√≠stico  
* Configuraci√≥n de control de versiones Git con archivo de exclusiones apropiado para proyectos Python cuantitativos, excluyendo archivos compilados, caches de Python, resultados de simulaciones pesadas, datos sensibles de calibraci√≥n  
* Archivo setup.py para hacer el proyecto instalable como paquete Python denominado "pftoken" con definici√≥n de dependencias y metadata del proyecto  
* Inicializaci√≥n de repositorio Git local con estructura de branches para desarrollo, testing y producci√≥n  
* Creaci√≥n de entorno virtual Python aislado usando venv o conda para gesti√≥n independiente de dependencias sin contaminar entorno global  
* Configuraci√≥n b√°sica de integraci√≥n continua mediante archivo de configuraci√≥n para ejecutar tests autom√°ticamente en cada commit

---

### **T-002: Documentaci√≥n requerimientos**

**¬øEn qu√© consiste?**  
 Crear documentaci√≥n inicial exhaustiva que explique los objetivos acad√©micos del proyecto, alcance cuantitativo, m√©tricas clave de evaluaci√≥n y metodolog√≠a general del modelo de Project Finance.

**Relaciones con otras tareas:**

* **Depende de:** T-001 (necesita estructura de carpetas para almacenar documentaci√≥n)  
* **Se actualiza continuamente con:** T-048 (Notebook final integrado), T-049 (Informe PDF acad√©mico), T-050 (README y documentaci√≥n final)  
* **Informa a:** Todo el equipo de desarrollo sobre visi√≥n y alcance del trabajo pr√°ctico

**Qu√© se debe implementar:**

* Documento README principal conteniendo descripci√≥n ejecutiva del proyecto de constelaci√≥n satelital LEO para IoT, justificaci√≥n del caso de uso seleccionado, objetivos cuantitativos espec√≠ficos de comparaci√≥n entre estructura de financiamiento tradicional bancaria versus estructura tokenizada descentralizada, m√©tricas objetivo clave con valores umbrales espec√≠ficos como DSCR mayor a 1.25, LLCR mayor a 1.0, VaR al 95%, roadmap de alto nivel del proyecto con Work Packages principales  
* Documento de requerimientos funcionales especificando funcionalidades m√≠nimas viables del modelo como c√°lculo de CFADS con per√≠odos de gracia y ramping, implementaci√≥n de waterfall con MRA, simulaci√≥n Monte Carlo con al menos 10,000 escenarios, pricing de tranches usando curva spot, c√°lculo de m√©tricas de riesgo de cr√©dito, ejecuci√≥n de stress testing con m√≠nimo 4 escenarios adversos  
* Documento de requerimientos no funcionales especificando caracter√≠sticas t√©cnicas como performance del modelo ejecut√°ndose en menos de 5 minutos para 10k simulaciones, precisi√≥n num√©rica de al menos 6 decimales en c√°lculos de pricing, reproducibilidad mediante semillas aleatorias fijas, modularidad del c√≥digo con separaci√≥n clara de responsabilidades, documentaci√≥n inline con docstrings en formato NumPy  
* Documento de arquitectura inicial con diagrama conceptual de m√≥dulos principales mostrando flujo de datos desde par√°metros de entrada, pasando por c√°lculo de CFADS, ejecuci√≥n de waterfall, pricing determin√≠stico, simulaci√≥n estoc√°stica, hasta generaci√≥n de outputs y reportes finales  
* Especificaci√≥n de decisiones de dise√±o preliminares justificadas como uso de dataclasses para estructuras de datos inmutables, uso de NumPy para operaciones vectorizadas eficientes, uso de Pandas para manipulaci√≥n de series temporales de flujos de caja, uso de Matplotlib para visualizaciones de calidad acad√©mica

---

### **T-012: Arquitectura de m√≥dulos**

**¬øEn qu√© consiste?**  
 Dise√±ar en detalle la arquitectura completa de clases, interfaces y el flujo de datos entre todos los m√≥dulos del sistema cuantitativo. Define c√≥mo se comunican las diferentes partes del modelo y establece contratos claros entre componentes.

**Relaciones con otras tareas:**

* **Depende de:** T-001 (estructura base), T-002 (requerimientos definidos)  
* **Informa dise√±o de:** T-013 (Dataclass de par√°metros), T-014 (DebtStructure), T-021 (Motor Monte Carlo), T-026 (Pricing estoc√°stico), T-053 (Dise√±o AMM)  
* **Genera:** Diagramas UML de clases y arquitectura que gu√≠an todo el desarrollo posterior

**Qu√© se debe implementar:**

* Documento de arquitectura detallada incluyendo diagrama de flujo completo del modelo cuantitativo desde carga de datos de entrada calibrados, pasando por c√°lculo de CFADS con ajustes de grace period y ramping, ejecuci√≥n de waterfall con l√≥gica de prelaci√≥n y reservas MRA/DSRA, pricing determin√≠stico de tranches usando DCF con curva spot, simulaci√≥n Monte Carlo con variables estoc√°sticas correlacionadas, c√°lculo de m√©tricas de riesgo de cr√©dito por tramo, ejecuci√≥n de stress testing con escenarios adversos, an√°lisis AMM de mercado secundario, hasta generaci√≥n de reportes finales y visualizaciones  
* Definici√≥n de interfaces p√∫blicas entre m√≥dulos especificando qu√© datos se intercambian en qu√© formato, qu√© m√©todos son p√∫blicos versus privados, qu√© contratos de entrada y salida deben cumplirse, qu√© excepciones pueden lanzarse en cada operaci√≥n  
* Especificaci√≥n de responsabilidades √∫nicas de cada m√≥dulo para mantener principio de responsabilidad √∫nica: m√≥dulo CFADS solo calcula flujos de caja, m√≥dulo Waterfall solo ejecuta l√≥gica de distribuci√≥n, m√≥dulo Pricing solo val√∫a instrumentos de deuda, m√≥dulo MonteCarlo solo genera trayectorias estoc√°sticas, m√≥dulo Risk solo calcula m√©tricas de riesgo  
* Documentaci√≥n de decisiones de dise√±o fundamentales justificadas como uso de clases inmutables mediante dataclasses con frozen=True para estructuras de datos, uso de funciones puras sin efectos secundarios para operaciones de c√°lculo, estrategia de manejo de estado mediante objetos resultado que contienen toda la informaci√≥n calculada, separaci√≥n clara entre l√≥gica de negocio y presentaci√≥n de resultados  
* Especificaci√≥n de patrones de dise√±o aplicables como Builder para construcci√≥n incremental de objetos complejos como estructuras de deuda, Strategy para diferentes implementaciones de waterfall seg√∫n tipo de estructura, Factory para generaci√≥n de escenarios de stress testing, Observer para notificaci√≥n de eventos en simulaci√≥n  
* Configuraci√≥n de archivos **init**.py en cada paquete para definir la librer√≠a pftoken como paquete Python modular con namespaces claros  
* Definici√≥n de convenciones de nomenclatura siguiendo PEP 8: snake\_case para funciones y variables, PascalCase para clases, UPPER\_CASE para constantes  
* Especificaci√≥n obligatoria de anotaciones de tipos usando typing module para todos los par√°metros de funciones y valores de retorno  
* Especificaci√≥n de estrategia de manejo de errores mediante excepciones personalizadas heredando de Exception para errores de validaci√≥n de datos, errores de c√°lculo num√©rico, errores de convergencia de algoritmos  
* Estrategia de logging estructurado en puntos cr√≠ticos del flujo como inicio y fin de simulaci√≥n, eventos de default en waterfall, c√°lculo de m√©tricas de riesgo, detecci√≥n de arbitraje en AMM

---

## **WP-02: M√ìDULO CFADS Y RATIOS (CORREGIDO)**

### **T-013: Dataclass par√°metros**

**¬øEn qu√© consiste?**  
 Crear una estructura de datos tipada e inmutable que contenga todos los par√°metros de entrada del modelo cuantitativo, incluyendo par√°metros del proyecto LEO IoT, estructura de deuda, configuraci√≥n de waterfall y par√°metros de simulaci√≥n.

**Relaciones con otras tareas:**

* **Depende de:** T-001 (estructura de proyecto), T-012 (arquitectura definida)  
* **Utilizado por:** T-003 (CFADS), T-006 (Waterfall), T-021 (Monte Carlo), T-026 (Pricing), T-038 (Stress testing), T-053 (AMM)  
* **Es el contrato de datos:** para todo el modelo

**Qu√© se debe implementar:**

* Dataclass ProjectParams en archivo pftoken/core/params.py conteniendo par√°metros t√©cnicos del proyecto satelital como n√∫mero de sat√©lites en la constelaci√≥n, masa por sat√©lite en kg, costo unitario de fabricaci√≥n y lanzamiento, vida √∫til operativa en a√±os, tasa de degradaci√≥n anual de capacidad  
* Dataclass RevenueParams conteniendo par√°metros de generaci√≥n de ingresos como n√∫mero inicial de dispositivos IoT conectados, ARPU mensual promedio por dispositivo, tasa de crecimiento anual de base de usuarios, tasa de churn mensual estimada, precio por mensaje IoT transmitido  
* Dataclass CostParams conteniendo par√°metros de estructura de costos como OPEX fijo mensual por sat√©lite para operaciones y mantenimiento, OPEX variable por mensaje transmitido, costos de seguros anuales como porcentaje del valor de activos, costos de licencias de espectro y regulatorios  
* Dataclass CapexParams conteniendo estructura de inversiones de capital como CAPEX inicial en a√±o 0 para fabricaci√≥n y lanzamiento de constelaci√≥n, RCAPEX planificado para reemplazo de sat√©lites al final de vida √∫til, contingencias como porcentaje del CAPEX base, cronograma de desembolsos de CAPEX  
* Dataclass DebtParams conteniendo par√°metros de estructura de deuda como monto total de deuda a financiar, porcentajes de cada tramo Senior/Mezzanine/Subordinado, tasas de inter√©s base para cada tramo, spreads de riesgo ajustados, tenor de cada tramo en a√±os, cronograma de amortizaci√≥n, periodicidad de pagos  
* Dataclass WaterfallParams conteniendo configuraci√≥n de l√≥gica de waterfall como tama√±o objetivo de DSRA como fracci√≥n del pr√≥ximo servicio de deuda, tama√±o objetivo de MRA como fracci√≥n del pr√≥ximo RCAPEX, umbral de DSCR para distribuci√≥n de dividendos, reglas de fondeo de reservas, triggers de covenants financieros  
* Dataclass GracePeriodParams conteniendo par√°metros de per√≠odos especiales como duraci√≥n de grace period en meses donde solo se pagan intereses, duraci√≥n de ramping period en meses con amortizaci√≥n gradual de principal, funciones de ramping que definen c√≥mo crece el porcentaje de principal pagado  
* Dataclass SimulationParams conteniendo configuraci√≥n de simulaci√≥n Monte Carlo como n√∫mero de escenarios a generar, semilla aleatoria para reproducibilidad, distribuciones estad√≠sticas para cada variable aleatoria, matriz de correlaci√≥n entre variables estoc√°sticas, horizonte temporal de proyecci√≥n  
* Implementar validaciones autom√°ticas en **post\_init** de cada dataclass verificando consistencia de par√°metros como porcentajes sumando a 1.0, valores positivos donde corresponde, fechas en orden cronol√≥gico correcto, compatibilidad entre par√°metros relacionados  
* Implementar m√©todos de clase @classmethod para cargar par√°metros desde diferentes fuentes como from\_csv para leer desde archivo CSV calibrado, from\_dict para crear desde diccionario Python, from\_json para cargar desde archivo JSON de configuraci√≥n  
* Implementar m√©todo to\_dict para serializar par√°metros a diccionario Python para logging y debugging  
* Documentar cada campo con docstring explicando unidades de medida, rangos v√°lidos esperados, fuente de calibraci√≥n del par√°metro

### **T-003: C√°lculo CFADS**

**¬øEn qu√© consiste?**  
 Implementar la l√≥gica central de c√°lculo del flujo de caja disponible para servicio de deuda (Cash Flow Available for Debt Service) per√≠odo por per√≠odo, que es el coraz√≥n del modelo de Project Finance.

**Relaciones con otras tareas:**

* **Depende de:** T-013 (necesita ProjectParams, RevenueParams, CostParams)  
* **Utilizado por:** T-004 (ratios), T-006 (waterfall), T-021 (Monte Carlo), T-026 (pricing), T-038 (stress)  
* **Contin√∫a en:** T-003B (extensi√≥n con grace period y ramping)

**Qu√© se debe implementar:**

* Clase CFADSCalculator en archivo pftoken/core/cfads.py que encapsula toda la l√≥gica de c√°lculo de flujos de caja operativos  
* M√©todo calculate\_revenues que calcula ingresos per√≠odo por per√≠odo considerando base creciente de dispositivos IoT seg√∫n tasa de crecimiento aplicada compuestamente cada per√≠odo, churn mensual reduciendo base activa, ARPU multiplicado por base activa para obtener ingresos por subscripciones, ingresos adicionales por mensajes transmitidos seg√∫n volumen esperado  
* Implementar curva de adopci√≥n realista no lineal: r√°pido crecimiento inicial en primeros a√±os, desaceleraci√≥n gradual hacia madurez, modelar mediante funci√≥n log√≠stica o curva S para capturar din√°mica de penetraci√≥n de mercado  
* M√©todo calculate\_opex que calcula costos operativos considerando componente fijo por sat√©lite operativo en constelaci√≥n, componente variable proporcional a tr√°fico de mensajes, costos de mantenimiento preventivo y correctivo, seguros calculados como porcentaje del valor de activos depreciados, licencias y regulatorios  
* Implementar degradaci√≥n de eficiencia operativa: sat√©lites se vuelven menos eficientes con el tiempo, aumentando OPEX por unidad de servicio, modelar mediante factor de degradaci√≥n anual que incrementa OPEX  
* M√©todo calculate\_ebitda que resta OPEX de ingresos para obtener EBITDA (Earnings Before Interest, Taxes, Depreciation and Amortization) que representa flujo de caja operativo antes de cargos financieros  
* M√©todo calculate\_capex que aplica cronograma de CAPEX inicial en fase de construcci√≥n seg√∫n calendario de desembolsos, m√°s RCAPEX de reemplazo calculado como funci√≥n del n√∫mero de sat√©lites que alcanzan fin de vida √∫til cada per√≠odo  
* M√©todo calculate\_cfads\_before\_tax que resta CAPEX de EBITDA para obtener flujo de caja antes de impuestos disponible para servicio de deuda y distribuciones  
* M√©todo calculate\_tax aplicando tasa impositiva corporativa sobre base imponible considerando depreciaci√≥n de activos seg√∫n vida √∫til y reglas fiscales, cr√©ditos fiscales aplicables a proyectos de infraestructura de telecomunicaciones  
* M√©todo calculate\_cfads\_final que resta impuestos del CFADS before tax para obtener el flujo de caja neto disponible para el waterfall de pagos  
* Implementar manejo de p√©rdidas arrastrables: si hay EBITDA negativo en per√≠odos iniciales, acumular p√©rdidas fiscales que pueden compensarse contra utilidades futuras reduciendo impuestos  
* Crear estructura de datos CFADSResult como dataclass que contenga series temporales completas de ingresos por per√≠odo, OPEX por per√≠odo, EBITDA por per√≠odo, CAPEX por per√≠odo, impuestos por per√≠odo, CFADS final por per√≠odo, m√©tricas agregadas como NPV de flujos, IRR del proyecto  
* Implementar validaciones num√©ricas verificando que flujos de caja no contienen valores NaN o infinitos, que CFADS puede volverse negativo en algunos per√≠odos pero es realista, que suma de flujos descontados converge a valor finito  
* Implementar visualizaci√≥n de flujos mediante m√©todo plot\_cfads que genere gr√°fico de l√≠neas mostrando evoluci√≥n temporal de ingresos, OPEX, EBITDA y CFADS final destacando per√≠odos cr√≠ticos, utilizando Matplotlib con estilo profesional

---

### **T-003B: Grace Period y Ramping**

**¬øEn qu√© consiste?**  
 Extender el c√°lculo de CFADS y el servicio de deuda para incorporar expl√≠citamente la mec√°nica de per√≠odos de gracia (grace period) donde solo se pagan intereses y per√≠odos de ramping con amortizaci√≥n gradual del principal.

**Relaciones con otras tareas:**

* **Depende de:** T-003 (c√°lculo base de CFADS), T-013 (GracePeriodParams)  
* **Cr√≠tico para:** T-004 (ratios ajustados), T-006 (waterfall con fases), T-021 (Monte Carlo realista)  
* **Duraci√≥n estimada:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Extender clase CFADSCalculator agregando atributos que almacenen configuraci√≥n de grace period como duraci√≥n en meses del per√≠odo de gracia, opci√≥n de capitalizaci√≥n de intereses durante gracia (PIK \- Payment In Kind) versus pago en efectivo desde DSRA, tasa de inter√©s durante gracia  
* Implementar m√©todo calculate\_grace\_period\_service que para cada per√≠odo dentro de grace window calcule solo servicio de intereses sin amortizaci√≥n de principal, si PIK est√° activo entonces capitalize intereses sum√°ndolos al saldo de deuda principal incrementando deuda outstanding, si no PIK entonces pague intereses desde CFADS o extraiga desde DSRA  
* Implementar funci√≥n de ramping phi(t) que defina fracci√≥n del principal programado que efectivamente se amortiza en cada per√≠odo de ramping, usando funci√≥n lineal donde phi va desde 0 al inicio del ramping hasta 1 al final del per√≠odo de ramping, permitir configuraci√≥n de diferentes curvas de ramping como lineal, exponencial acelerada, o customizada  
* Modificar c√°lculo de servicio de deuda total en cada per√≠odo: durante grace period servicio \= solo intereses, durante ramping servicio \= intereses \+ principal √ó phi(t), despu√©s de ramping servicio \= intereses \+ principal completo  
* Implementar tracking de saldo de deuda ajustado: si hay capitalizaci√≥n PIK entonces saldo aumenta durante grace, durante ramping el saldo se amortiza gradualmente, despu√©s de ramping se amortiza al ritmo programado completo  
* Crear m√©todo calculate\_adjusted\_dscr que calcule ratio de cobertura ajustado por fase: durante grace DSCR\_grace \= CFADS / intereses con umbral objetivo \> 1.0x, durante ramping DSCR\_ramp \= CFADS / (intereses \+ principal √ó phi) con umbral \> 1.15x, en steady state DSCR \= CFADS / servicio total con umbral \> 1.25x  
* Implementar validaciones de consistencia verificando que duraci√≥n de grace \+ duraci√≥n de ramping no exceda tenor total de deuda, que CFADS durante grace sea suficiente para cubrir al menos intereses si no hay PIK, que DSRA tiene fondos suficientes si se requiere pagar intereses desde reserva  
* Generar reporte de transici√≥n de fases mostrando claramente en qu√© per√≠odos termina grace, en qu√© per√≠odos ocurre ramping, en qu√© per√≠odo se alcanza steady state con servicio completo  
* Implementar visualizaci√≥n espec√≠fica plotting servicio de deuda en el tiempo destacando fase de grace con barra de un color, fase de ramping con otro color, steady state con tercer color, superponer curva de CFADS para visualizar cobertura en cada fase  
* Documentar exhaustivamente la l√≥gica de grace y ramping con ecuaciones LaTeX en docstrings explicando notaci√≥n matem√°tica, referencias a Gatti (2018) y Yescombe (2013) como literatura est√°ndar de Project Finance

### **T-004: Ratios financieros DSCR/LLCR**

**¬øEn qu√© consiste?**  
 Implementar el c√°lculo de los ratios de cobertura de deuda m√°s cr√≠ticos en Project Finance: DSCR (Debt Service Coverage Ratio), LLCR (Loan Life Coverage Ratio) y PLCR (Project Life Coverage Ratio).

**Relaciones con otras tareas:**

* **Depende de:** T-003B (CFADS con grace y ramping completo)  
* **Utilizado por:** T-015 (covenants), T-021 (Monte Carlo con flags de default), T-038 (stress testing), T-041 (resultados de estr√©s)  
* **Contin√∫a en:** T-029 (DSCR por trayectoria en MC)

**Qu√© se debe implementar:**

* Clase RatioCalculator en archivo pftoken/core/ratios.py que centraliza c√°lculo de todos los ratios de solvencia  
* M√©todo calculate\_dscr que para cada per√≠odo t calcula DSCR\_t \= CFADS\_t / DebtService\_t donde DebtService incluye tanto intereses como principal del per√≠odo, considerando ajustes por fase de grace o ramping seg√∫n T-003B, retornando serie temporal completa de DSCR por per√≠odo  
* Implementar l√≥gica especial para grace period: durante grace DSCR \= CFADS / solo\_intereses con umbral reducido de 1.0x pues principal no se amortiza a√∫n  
* Implementar l√≥gica especial para ramping period: durante ramping DSCR \= CFADS / (intereses \+ principal √ó phi) donde phi es fracci√≥n de ramping con umbral intermedio de 1.15x  
* Implementar l√≥gica para steady state: despu√©s de ramping DSCR \= CFADS / servicio\_completo con umbral est√°ndar de 1.25x establecido en covenants  
* M√©todo calculate\_llcr que calcula Loan Life Coverage Ratio como cociente entre valor presente de CFADS futuros desde per√≠odo t hasta vencimiento de deuda y saldo de deuda pendiente en per√≠odo t: LLCR\_t \= NPV(CFADS\_future) / Outstanding\_Debt\_t, usando tasa de descuento igual al costo de deuda del tramo correspondiente  
* Implementar c√°lculo de NPV de flujos futuros aplicando factores de descuento correctos compuestos per√≠odo por per√≠odo: DF\_t \= 1 / (1 \+ r)^t donde r es costo de deuda anualizado  
* Calcular LLCR para cada tramo de deuda separadamente pues cada uno tiene costo y saldo diferente: LLCR\_senior, LLCR\_mezz, LLCR\_sub  
* Implementar interpretaci√≥n: LLCR \> 1.0 significa que valor presente de flujos futuros cubre completamente deuda restante, LLCR \< 1.0 indica insuficiencia de flujos proyectados para pagar deuda  
* M√©todo calculate\_plcr que calcula Project Life Coverage Ratio tomando valor presente de CFADS durante toda la vida del proyecto sin restricci√≥n a vida de pr√©stamo: PLCR \= NPV(CFADS\_proyecto\_completo) / Total\_Debt, usando misma tasa de descuento que LLCR  
* Implementar PLCR para evaluar sostenibilidad financiera del proyecto completo m√°s all√° del pr√©stamo espec√≠fico, PLCR \> 1.0 indica que proyecto genera m√°s valor que deuda total emitida  
* Implementar m√©todo de detecci√≥n de breaches: check\_covenant\_breach que verifique en cada per√≠odo si DSCR cae por debajo de umbral m√≠nimo definido en covenants, retornando lista de per√≠odos y magnitud de breach  
* Crear estructura de datos RatioResults como dataclass conteniendo series temporales de DSCR\_t, LLCR\_t, PLCR\_t por per√≠odo, flags indicando per√≠odos con covenant breach, estad√≠sticas agregadas como DSCR m√≠nimo observado, DSCR promedio ponderado por servicio, percentiles de distribuci√≥n  
* Implementar visualizaci√≥n de ratios mediante plotting: gr√°fico de l√≠neas mostrando evoluci√≥n de DSCR en el tiempo con l√≠nea horizontal en umbral de covenant 1.25x, sombreado en rojo para per√≠odos con breach, anotaciones destacando DSCR m√≠nimo  
* Generar gr√°fico separado para LLCR y PLCR mostrando evoluci√≥n y destacando momento donde LLCR alcanza 1.0 si es que ocurre  
* Implementar tests unitarios verificando que ratios se calculan correctamente en casos conocidos: si CFADS \= 150 y servicio \= 100 entonces DSCR \= 1.5x, verificar c√°lculo de NPV usando casos anal√≠ticos simples

### **T-005: Modelo Merton PD**

**¬øEn qu√© consiste?**  
 Implementar el modelo estructural de Merton para c√°lculo end√≥geno de probabilidad de default (PD) y loss given default (LGD) basado en comparaci√≥n estoc√°stica entre valor de activos del proyecto y valor de deuda outstanding.

**Relaciones con otras tareas:**

* **Depende de:** T-003B (CFADS base para valorar activos)  
* **Se integra en:** T-024 (Merton dentro de Monte Carlo para PD din√°mica)  
* **Utilizado por:** T-010 (m√©tricas de riesgo EL/VaR), T-033 (p√©rdida esperada por tramo)  
* **Contin√∫a en:** T-005B (Excel validation)

**Qu√© se debe implementar:**

* Clase MertonModel en archivo pftoken/risk/merton.py que implementa modelo estructural de valoraci√≥n de deuda riesgosa  
* M√©todo calculate\_asset\_value que valora activos del proyecto como valor presente de CFADS esperados futuros: V\_assets \= NPV(CFADS\_future), usando tasa de descuento apropiada que refleja riesgo del proyecto como WACC o costo de capital de activos  
* Implementar simulaci√≥n de trayectoria estoc√°stica de valor de activos: asumir que activos siguen proceso geom√©trico browniano dV/V \= Œº dt \+ œÉ dW donde Œº es drift esperado, œÉ es volatilidad de activos, dW es Wiener process  
* Calibrar par√°metros del proceso estoc√°stico: estimar Œº como retorno esperado del proyecto basado en EBITDA/valor activos, estimar œÉ como volatilidad hist√≥rica de proyectos comparables del sector telecomunicaciones satelitales  
* M√©todo calculate\_default\_probability que implementa l√≥gica de Merton: default ocurre si en horizonte T el valor de activos V\_T cae por debajo del valor facial de deuda D: PD \= P(V\_T \< D), calcular esta probabilidad usando f√≥rmula anal√≠tica de Black-Scholes-Merton para distancia a default  
* Implementar c√°lculo de distance to default: DD \= \[ln(V\_0/D) \+ (Œº \- œÉ¬≤/2)T\] / (œÉ‚àöT), este es n√∫mero de desviaciones est√°ndar que separan valor actual de activos del umbral de default  
* Calcular PD usando distribuci√≥n normal: PD \= Œ¶(-DD) donde Œ¶ es funci√≥n de distribuci√≥n acumulada normal est√°ndar  
* M√©todo calculate\_lgd que calcula Loss Given Default considerando que en caso de default los acreedores recuperan valor de activos residuales: Recovery \= V\_default / D, entonces LGD \= 1 \- Recovery  
* Implementar l√≥gica de prioridad de recuperaci√≥n: en estructura con m√∫ltiples tranches, Senior recupera primero hasta su monto nominal, luego Mezz recupera de lo restante, finalmente Sub recupera si queda algo  
* Calcular LGD espec√≠fica por tramo: LGD\_senior t√≠picamente baja (10-20%), LGD\_mezz media (30-50%), LGD\_sub alta (60-80%) reflejando subordinaci√≥n  
* M√©todo calculate\_expected\_loss que combina PD y LGD para obtener p√©rdida esperada: EL \= PD √ó LGD √ó Exposure, donde Exposure es saldo de deuda outstanding en momento de an√°lisis  
* Implementar ajuste por horizonte temporal: PD aumenta con horizonte m√°s largo, implementar curva de term structure de PD para diferentes tenores: PD\_1y, PD\_3y, PD\_5y, etc.  
* Crear estructura de datos MertonResult conteniendo PD estimada con intervalo de confianza, LGD por tramo, EL por tramo, distance to default, valor de activos simulado, sensibilidades del modelo a par√°metros clave  
* Implementar an√°lisis de sensibilidad: c√≥mo var√≠a PD ante cambios en volatilidad de activos œÉ, c√≥mo var√≠a ante cambios en leverage D/V, generar gr√°ficos de tornado mostrando sensibilidades  
* Documentar supuestos del modelo: distribuci√≥n lognormal de activos, volatilidad constante, deuda con valor facial fijo, ausencia de pagos intermedios en horizonte analizado  
* Incluir referencias acad√©micas: Merton (1973) para la teor√≠a de valoraci√≥n de opciones, Merton (1974) para su aplicaci√≥n a deuda corporativa, modelo KMV, y aplicaciones a Project Finance seg√∫n literatura

---

### **T-005B: Excel Validation Model**

**¬øEn qu√© consiste?**  
 Crear un modelo de validaci√≥n en Excel que replique los c√°lculos core del modelo Python para asegurar correcci√≥n num√©rica y servir como herramienta de debugging y explicaci√≥n acad√©mica.

**Relaciones con otras tareas:**

* **Cr√≠tico: Depende de:** T-004 (ratios), T-005 (Merton), necesita resultados para comparar  
* **Valida:** T-003, T-003B, T-004, T-005, T-006 (m√≥dulos core)  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt chart (tarea cr√≠tica)

**Qu√© se debe implementar:**

* Crear archivo Excel TP\_Quant\_Validation.xlsx con estructura modular en hojas separadas  
* Hoja "Inputs" conteniendo todos los par√°metros del modelo exactamente como en ProjectParams de T-013: par√°metros del proyecto satelital, par√°metros de ingresos y costos, estructura de deuda, configuraci√≥n de waterfall, par√°metros de grace y ramping  
* Usar validaci√≥n de datos en celdas de input para asegurar rangos v√°lidos: porcentajes entre 0-100%, montos positivos, fechas consistentes  
* Usar nombres de rango en Excel para hacer f√≥rmulas legibles: nombrar celda con CAPEX\_inicial, tasa\_senior, DSCR\_covenant, etc.  
* Hoja "CFADS\_Calc" replicando exactamente la l√≥gica de T-003 y T-003B: columnas para cada per√≠odo t desde 0 hasta horizonte, filas calculando ingresos por subscripciones IoT aplicando tasa de crecimiento compuesta y churn, ingresos por mensajes seg√∫n volumen, OPEX fijo y variable, EBITDA, CAPEX y RCAPEX, impuestos con depreciaci√≥n, CFADS final  
* Implementar c√°lculos de grace period: columnas adicionales mostrando saldo de deuda ajustado si hay capitalizaci√≥n PIK, intereses capitalizados sum√°ndose al principal  
* Implementar ramping: columna con funci√≥n phi(t) calculada seg√∫n per√≠odo, servicio de deuda ajustado por phi  
* Hoja "Ratios" calculando DSCR, LLCR y PLCR: DSCR simple dividiendo CFADS/servicio en cada per√≠odo, LLCR usando funci√≥n NPV de Excel para calcular valor presente de CFADS futuros desde cada per√≠odo hasta maturity, PLCR calculando NPV de flujos completos del proyecto  
* Resaltar en formato condicional per√≠odos donde DSCR \< 1.25 en color rojo indicando breach de covenant  
* Hoja "Merton\_PD" implementando modelo de T-005: calcular valor de activos como NPV de CFADS usando funci√≥n VNA de Excel, implementar c√°lculo de distance to default usando f√≥rmulas de Merton, calcular PD usando funci√≥n NORM.DIST de Excel  
* Calcular LGD asumiendo diferentes recovery rates por tramo seg√∫n jerarqu√≠a de prelaci√≥n  
* Hoja "Waterfall" mostrando paso a paso la distribuci√≥n de CFADS: empezar con CFADS disponible, restar OPEX, pagar intereses de cada tramo en orden de prelaci√≥n, fondear DSRA si est√° por debajo de target, fondear MRA si est√° por debajo de target, amortizar principal de cada tramo seg√∫n prelaci√≥n, distribuir dividendos solo si DSCR \> 1.3 y reservas completas  
* Usar formato condicional para resaltar flujos: positivos en verde, negativos en rojo, ceros en gris  
* Hoja "Comparison" que importe resultados del modelo Python (copiar/pegar desde CSV exportado) y calcule diferencias: diferencia absoluta y diferencia porcentual para cada m√©trica clave  
* Establecer tolerancia aceptable: diferencias \< 0.01% indican match perfecto, diferencias 0.01-0.1% son aceptables por redondeo, diferencias \> 0.1% requieren investigaci√≥n  
* Crear gr√°ficos en Excel replicando visualizaciones del modelo Python: gr√°fico de l√≠neas de CFADS en el tiempo, gr√°fico de DSCR con threshold, gr√°fico de waterfall en cascada mostrando flujo de pagos  
* Implementar macros VBA simples para automatizar comparaci√≥n: bot√≥n que ejecute macro para importar CSV de Python, calcular diferencias autom√°ticamente, generar reporte de validaci√≥n  
* Documentar exhaustivamente cada celda con comentarios explicando la f√≥rmula: qu√© calcula, por qu√© se calcula as√≠, qu√© supuestos se hacen  
* Crear hoja "Documentation" con tabla explicativa de cada m√©trica: nombre, f√≥rmula, interpretaci√≥n, rangos t√≠picos en Project Finance  
* Usar esta herramienta Excel para presentaciones acad√©micas mostrando paso a paso la l√≥gica del modelo de forma interactiva y visual

### **T-046: Dataset LEO IoT**

**¬øEn qu√© consiste?**  
 Crear y calibrar un dataset realista de par√°metros para el proyecto de constelaci√≥n LEO IoT que servir√° como caso de estudio base para todo el trabajo pr√°ctico.

**Relaciones con otras tareas:**

* **Depende de:** T-013 (estructura de params definida)  
* **Alimenta a:** Todas las tareas de c√°lculo (T-003, T-006, T-021, T-026, T-038)  
* **Se complementa con:** T-047 (calibraciones adicionales)

**Qu√© se debe implementar:**

* Investigar proyectos de constelaciones LEO comparables: analizar Starlink de SpaceX, OneWeb, Iridium Next como benchmarks para par√°metros t√©cnicos y financieros  
* Crear archivo CSV input\_data\_leo\_iot.csv con estructura columnar conteniendo todos los par√°metros necesarios  
* Calibrar par√°metros t√©cnicos del proyecto: n√∫mero de sat√©lites en constelaci√≥n inicial \= 150 para cobertura global de IoT, masa promedio por sat√©lite \= 200 kg t√≠pico de smallsats, costo unitario de fabricaci√≥n \= $500k por sat√©lite basado en producci√≥n en escala, costo de lanzamiento \= $1M por lanzamiento compartido rideshare, vida √∫til \= 7 a√±os considerando degradaci√≥n orbital en LEO  
* Calibrar par√°metros de mercado IoT: base inicial de dispositivos \= 500,000 unidades en a√±o 1, tasa de crecimiento \= 15% anual durante primeros 5 a√±os reflejando adopci√≥n de IoT, ARPU \= $5/mes por dispositivo competitivo con redes terrestres, tasa de churn \= 2% mensual t√≠pica de servicios M2M  
* Calibrar estructura de costos: OPEX fijo \= $50k/mes por sat√©lite para operaciones de control y telemetr√≠a, OPEX variable \= $0.01 por mensaje transmitido, seguros \= 2% anual del valor de activos, costos regulatorios \= $2M anuales por licencias de espectro  
* Calibrar CAPEX: inversi√≥n inicial \= $75M para fabricaci√≥n de 150 sat√©lites, $22.5M en lanzamientos, $10M en ground segment, total CAPEX inicial \= $107.5M, RCAPEX \= reemplazar 20% de flota cada a√±o a partir de a√±o 6  
* Calibrar estructura de deuda: monto total \= $90M equivalente a 85% del CAPEX inicial, tramo Senior \= 60% con tasa base \= 5.5%, tramo Mezzanine \= 25% con tasa base \= 8.0%, tramo Subordinado \= 15% con tasa base \= 12.0%, tenor \= 10 a√±os con amortizaci√≥n bullet modificada  
* Calibrar waterfall: DSRA target \= 100% del pr√≥ximo servicio semestral de deuda, MRA target \= 50% del pr√≥ximo RCAPEX anual, covenant DSCR m√≠nimo \= 1.25x, threshold para dividendos \= DSCR \> 1.30x  
* Calibrar grace period: duraci√≥n \= 24 meses durante fase de construcci√≥n y ramping inicial, capitalizaci√≥n de intereses (PIK) durante grace \= SI para conservar caja, ramping period \= 24 meses con phi lineal desde 0 a 1  
* Documentar fuentes de calibraci√≥n: citar fuentes p√∫blicas como presentaciones de earnings de SpaceX/OneWeb, reportes de analistas de la industria satelital como NSR o Euroconsult, benchmarking con proyectos de infraestructura comparables  
* Implementar an√°lisis de sensibilidad sobre par√°metros clave identificando cu√°les tienen mayor impacto en viabilidad: tasa de adopci√≥n de IoT, ARPU, costo de lanzamiento, tasa de inter√©s senior  
* Crear versiones alternativas del dataset: scenario\_optimista.csv con supuestos favorables (crecimiento 20%, ARPU $7, lanzamiento m√°s barato), scenario\_pesimista.csv con supuestos adversos (crecimiento 10%, ARPU $4, costos mayores)  
* Validar consistencia interna: verificar que deuda no excede capacity to pay del proyecto, que DSCR en escenario base sea \> 1.25 en mayor√≠a de per√≠odos, que proyecto alcanza IRR \> WACC indicando creaci√≥n de valor

---

### **T-047: Calibraciones adicionales**

**¬øEn qu√© consiste?**  
 Refinar y calibrar par√°metros adicionales m√°s sofisticados necesarios para simulaci√≥n Monte Carlo, modelo de Merton, pricing de tranches y stress testing.

**Relaciones con otras tareas:**

* **Depende de:** T-046 (dataset base)  
* **Alimenta:** T-021 (MC necesita distribuciones y correlaciones), T-005 (Merton necesita volatilidad), T-028 (pricing necesita spreads)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Calibrar par√°metros de simulaci√≥n estoc√°stica: distribuci√≥n estad√≠stica para volatilidad de ingresos \= lognormal con media \= esperado y desviaci√≥n est√°ndar \= 15% anual reflejando incertidumbre en adopci√≥n de IoT, distribuci√≥n para volatilidad de OPEX \= normal con desviaci√≥n \= 10% considerando variabilidad operativa  
* Estimar matriz de correlaci√≥n entre variables estoc√°sticas: correlaci√≥n entre ingresos y OPEX \= 0.4 positiva pues mayor tr√°fico implica mayores costos variables, correlaci√≥n entre tasa de inter√©s base y spread de riesgo \= \-0.3 negativa por flight to quality, correlaci√≥n entre churn y competencia de precios \= 0.6 positiva  
* Utilizar m√©todos estad√≠sticos para estimaci√≥n: si hay datos hist√≥ricos de proyectos comparables usar an√°lisis de regresi√≥n, si no hay datos usar juicio experto estructurado mediante Delphi method, triangular entre diferentes fuentes  
* Calibrar par√°metros del modelo de Merton: volatilidad de activos œÉ\_assets \= 25% anual basada en volatilidad de equity de empresas satelitales comparables ajustada por leverage usando f√≥rmula de Merton œÉ\_assets \= œÉ\_equity √ó E/(E+D), drift esperado Œº \= (EBITDA/Valor activos) \- tasa de depreciaci√≥n estimando retorno sobre activos  
* Validar calibraci√≥n de Merton: verificar que distance to default resultante es consistente con rating crediticio impl√≠cito del proyecto, comparar PD calculada con tasas de default hist√≥ricas de Project Finance en sector telecomunicaciones  
* Calibrar spreads de riesgo sobre curva base: spread senior \= 150 bps sobre SOFR consistente con deuda senior garantizada de infraestructura, spread mezz \= 400 bps reflejando subordinaci√≥n parcial, spread sub \= 800 bps por ser junior sin garant√≠as  
* Relacionar spreads con m√©tricas de riesgo: usar modelo simple spread \= base \+ Œ± √ó PD \+ Œ≤ √ó LGD donde Œ± y Œ≤ son coeficientes de sensibilidad calibrados mediante regresi√≥n sobre datos de mercado de bonos corporativos  
* Calibrar recovery rates para cada tramo: RR\_senior \= 75% dado garant√≠as sobre activos satelitales f√≠sicos y prioridad absoluta, RR\_mezz \= 50% por ser mezanine sin garant√≠a colateral espec√≠fica, RR\_sub \= 25% por ser equity-like y absorber primeras p√©rdidas  
* Justificar recovery rates: satelites tienen valor de reventa limitado una vez en √≥rbita (activos espec√≠ficos), pero licencias de espectro y contratos con clientes tienen valor recuperable, ground stations tienen valor de liquidaci√≥n  
* Calibrar par√°metros de stress testing: magnitud de shocks realistas como ca√≠da de demanda de 20-30% en crisis econ√≥mica, incremento de tasas de 200 bps en escenario de Fed hawkish, fallas de lanzamiento impactando 10-15% de flota, retraso de 12 meses en ramping operativo  
* Documentar incertidumbre de calibraci√≥n: reportar intervalos de confianza para cada par√°metro, realizar an√°lisis de sensibilidad sobre valores plausibles, identificar par√°metros con mayor incertidumbre que requieren escenarios m√∫ltiples

\#\#\# \*\*T-047: Dataset LEO IoT \- PAR√ÅMETROS CALIBRADOS SEG√öN EXCEL\*\* **\*\*¬øEn qu√© consiste?\*\*** Este dataset contiene los par√°metros EXACTOS del modelo Excel validado del proyecto LEO IoT. Estos n√∫meros garantizan DSCR \= 1.45x en a√±os operativos y replican el comportamiento financiero demostrado. \--- \#\# \*\*A. ESTRUCTURA DE DEUDA (Params)\*\* | Par√°metro | Valor | Unidad | Descripci√≥n | |-----------|-------|--------|-------------| | \`Principal\_Senior\` | 43.2 | MUSD | Tramo Senior (60% de deuda total) | | \`Principal\_Mezz\` | 18.0 | MUSD | Tramo Mezzanine (25% de deuda total) | | \`Principal\_Sub\` | 10.8 | MUSD | Tramo Subordinado (15% de deuda total) | | \`Debt\_Total\` | **\*\*72.0\*\*** | MUSD | Deuda total del proyecto | | \`Rate\_Senior\` | 0.060 | % anual | Tasa de inter√©s tramo Senior (6.0%) | | \`Rate\_Mezz\` | 0.085 | % anual | Tasa de inter√©s tramo Mezzanine (8.5%) | | \`Rate\_Sub\` | 0.110 | % anual | Tasa de inter√©s tramo Subordinado (11.0%) | | \`Grace\_Years\` | 4 | a√±os | Per√≠odo de gracia (solo intereses, a√±os 1-4) | | \`Tenor\_Years\` | 15 | a√±os | Plazo total del financiamiento | | \`DSCR\_Obj\` | 1.45 | ratio | DSCR objetivo para amortizaci√≥n esculpida | | \`Weight\_Senior\` | 0.70 | ratio | Peso relativo Senior | | \`Weight\_Mezz\` | 0.23 | ratio | Peso relativo Mezzanine | | \`Weight\_Sub\` | 0.07 | ratio | Peso relativo Subordinado | \--- \#\# \*\*B. PROYECCI√ìN DE CFADS (por a√±o, en MUSD)\*\* | A√±o | Revenue | OPEX | Maintenance | ŒîWC | Tax | RCAPEX | \*\*CFADS\*\* | |-----|---------|------|-------------|-----|-----|--------|-----------| | 1 | 0.00 | 0.60 | 0.00 | 0.00 | 0.00 | 0.00 | **\*\*-0.60\*\*** | | 2 | 0.00 | 1.20 | 0.00 | 0.00 | 0.00 | 0.00 | **\*\*-1.20\*\*** | | 3 | 3.00 | 2.40 | 0.10 | 0.20 | 0.00 | 0.00 | **\*\*0.30\*\*** | | 4 | 10.00 | 4.50 | 0.20 | 0.30 | 0.00 | 0.00 | **\*\*5.00\*\*** | | 5 | 20.00 | 6.50 | 0.30 | 0.50 | 0.30 | 0.00 | **\*\*12.40\*\*** | | 6 | 28.00 | 8.50 | 0.40 | 0.50 | 0.60 | 0.00 | **\*\*18.00\*\*** | | 7 | 34.00 | 10.00 | 0.50 | 0.50 | 0.90 | 0.00 | **\*\*22.10\*\*** | | 8 | 38.00 | 11.50 | 0.60 | 0.60 | 1.20 | 0.00 | **\*\*24.10\*\*** | | 9 | 40.00 | 12.00 | 0.70 | 0.60 | 1.40 | 0.00 | **\*\*25.30\*\*** | | 10 | 38.00 | 12.00 | 1.20 | 0.60 | 1.40 | 0.00 | **\*\*22.80\*\*** | | 11 | 35.00 | 11.50 | 0.80 | 0.50 | 1.20 | 0.00 | **\*\*21.00\*\*** | | 12 | 33.00 | 11.00 | 0.80 | 0.40 | 1.10 | 0.00 | **\*\*19.70\*\*** | | 13 | 30.00 | 10.50 | 0.90 | 0.30 | 0.90 | 0.00 | **\*\*17.40\*\*** | | 14 | 27.00 | 10.00 | 0.80 | 0.20 | 0.80 | 0.00 | **\*\*15.20\*\*** | | 15 | 24.00 | 9.50 | 0.60 | 0.20 | 0.70 | 0.00 | **\*\*13.00\*\*** | **\*\*Totales:\*\*** \- Revenue Total: **\*\*360.00 MUSD\*\*** \- CFADS Total: **\*\*214.50 MUSD\*\*** \- OPEX Total: **\*\*121.70 MUSD\*\*** \--- \#\# \*\*C. WATERFALL Y SERVICIO DE DEUDA (por a√±o, en MUSD)\*\* | A√±o | BOP\_Senior | BOP\_Mezz | BOP\_Sub | Int\_S | Int\_M | Int\_U | Amort\_S | Amort\_M | Amort\_U | \*\*Service\*\* | \*\*DSCR\*\* | |-----|------------|----------|---------|-------|-------|-------|---------|---------|---------|-------------|----------| | 1 | 43.20 | 18.00 | 10.80 | 2.59 | 1.53 | 1.19 | 0.00 | 0.00 | 0.00 | **\*\*5.31\*\*** | **\*\*-0.113\*\*** | | 2 | 43.20 | 18.00 | 10.80 | 2.59 | 1.53 | 1.19 | 0.00 | 0.00 | 0.00 | **\*\*5.31\*\*** | **\*\*-0.226\*\*** | | 3 | 43.20 | 18.00 | 10.80 | 2.59 | 1.53 | 1.19 | 0.00 | 0.00 | 0.00 | **\*\*5.31\*\*** | **\*\*0.057\*\*** | | 4 | 43.20 | 18.00 | 10.80 | 2.59 | 1.53 | 1.19 | 0.00 | 0.00 | 0.00 | **\*\*5.31\*\*** | **\*\*0.942\*\*** | | 5 | 43.20 | 18.00 | 10.80 | 2.59 | 1.53 | 1.19 | 2.27 | 0.75 | 0.23 | **\*\*8.55\*\*** | **\*\*1.450\*\*** ‚úì | | 6 | 40.93 | 17.25 | 10.57 | 2.46 | 1.47 | 1.16 | 5.13 | 1.69 | 0.51 | **\*\*12.41\*\*** | **\*\*1.450\*\*** ‚úì | | 7 | 35.80 | 15.57 | 10.06 | 2.15 | 1.32 | 1.11 | 7.46 | 2.45 | 0.75 | **\*\*15.24\*\*** | **\*\*1.450\*\*** ‚úì | | 8 | 28.34 | 13.12 | 9.31 | 1.70 | 1.11 | 1.02 | 8.95 | 2.94 | 0.89 | **\*\*16.62\*\*** | **\*\*1.450\*\*** ‚úì | | 9 | 19.39 | 10.18 | 8.42 | 1.16 | 0.87 | 0.93 | 10.15 | 3.33 | 1.01 | **\*\*17.45\*\*** | **\*\*1.450\*\*** ‚úì | | 10 | 9.24 | 6.84 | 7.40 | 0.55 | 0.58 | 0.81 | 9.24 | 3.56 | 0.96 | **\*\*15.72\*\*** | **\*\*1.450\*\*** ‚úì | | 11 | 0.00 | 3.28 | 6.44 | 0.00 | 0.28 | 0.71 | 0.00 | 3.28 | 6.44 | **\*\*10.71\*\*** | **\*\*1.962\*\*** ‚úì | | 12 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | **\*\*0.00\*\*** | **\*\*N/A\*\*** | **\*\*M√©tricas Clave:\*\*** \- DSCR Promedio (A√±os 5-11): **\*\*1.523\*\*** \- DSCR Objetivo: **\*\*1.450\*\*** (cumplido a√±os 5-10) \- Grace Period: A√±os 1-4 (solo intereses) \- Amortizaci√≥n completa: A√±o 11 \--- \#\# \*\*D. PAR√ÅMETROS IMPL√çCITOS DERIVADOS\*\* | Par√°metro | Valor Calculado | Fuente | |-----------|-----------------|--------| | CAPEX Total Impl√≠cito | \~90-120 MUSD | Deuda \= 72 MUSD @ 60-80% ratio | | EBITDA/Revenue Ratio | \~62% | (Revenue \- OPEX) / Revenue | | Tax Rate Efectiva | \~25-30% | Tax / (Revenue \- OPEX \- Depreciation) | | NPV CFADS @ 7.4% | \~150 MUSD | Descuento de flujos futuros | | LLCR (estimado) | \~2.0x | NPV(CFADS) / Deuda Outstanding | | PLCR (estimado) | \~2.1x | NPV(CFADS\_proyecto) / Deuda Inicial | \--- \#\# \*\*E. ORDEN DE PRELACI√ìN DEL WATERFALL\*\* \`\`\` 1\. OPEX (impl√≠cito, ya restado en CFADS) 2\. Intereses Senior (Int\_S) 3\. Intereses Mezzanine (Int\_M) 4\. Intereses Subordinado (Int\_U) 5\. Fondeo DSRA (target \= 100% pr√≥ximo servicio) 6\. Amortizaci√≥n Principal Senior (Amort\_S) 7\. Amortizaci√≥n Principal Mezzanine (Amort\_M) 8\. Amortizaci√≥n Principal Subordinado (Amort\_U) 9\. Fondeo MRA (target \= 50% pr√≥ximo RCAPEX) 10\. Cash Sweep (si DSCR \< 1.25) 11\. Dividendos (si DSCR \> 1.30) \`\`\` \--- \#\# \*\*F. F√ìRMULAS CLAVE IMPLEMENTADAS\*\* **\*\*CFADS:\*\*** \`\`\` CFADS\_t \= Revenue\_t \- OPEX\_t \- Maintenance\_t \- ŒîWC\_t \- Tax\_t \- RCAPEX\_t \`\`\` **\*\*DSCR:\*\*** \`\`\` DSCR\_t \= CFADS\_t / Service\_t donde: Service\_t \= (Int\_S \+ Int\_M \+ Int\_U) \+ (Amort\_S \+ Amort\_M \+ Amort\_U) \`\`\` **\*\*Amortizaci√≥n Esculpida (A√±os 5-10):\*\*** \`\`\` Service\_t ajustado para mantener DSCR\_t \= 1.45 Amort\_total\_t \= CFADS\_t / 1.45 \- Intereses\_t \`\`\` \--- \#\# \*\*G. VALIDACIONES IMPLEMENTADAS\*\* ‚úÖ **\*\*Grace Period:\*\*** A√±os 1-4 sin amortizaci√≥n de principal ‚úÖ **\*\*DSCR Target:\*\*** 1.45x mantenido consistentemente a√±os 5-10 ‚úÖ **\*\*Coherencia Matem√°tica:\*\*** DSCR\_manual \= CFADS/Service (error \< 0.0001) ‚úÖ **\*\*Prelaci√≥n de Pagos:\*\*** Senior ‚Üí Mezz ‚Üí Sub respetada ‚úÖ **\*\*Conservaci√≥n de Cash:\*\*** Suma(Pagos) \+ Residual \= CFADS\_inicial ‚úÖ **\*\*Repago Completo:\*\*** Deuda \= 0 en a√±o 11 \--- \#\# \*\*H. ESTRUCTURA DE ARCHIVOS CSV REQUERIDA\*\* \#\#\# \*\*project\_params\_excel.csv\*\* \`\`\`csv param,value,unit,description principal\_senior,43.2,MUSD,Tramo Senior principal\_mezz,18.0,MUSD,Tramo Mezzanine principal\_sub,10.8,MUSD,Tramo Subordinado rate\_senior,0.060,ratio,Tasa anual Senior rate\_mezz,0.085,ratio,Tasa anual Mezz rate\_sub,0.110,ratio,Tasa anual Sub grace\_years,4,years,Per√≠odo de gracia tenor\_years,15,years,Plazo total dscr\_obj,1.45,ratio,DSCR objetivo weight\_senior,0.70,ratio,Peso Senior weight\_mezz,0.23,ratio,Peso Mezz weight\_sub,0.07,ratio,Peso Sub \`\`\` \#\#\# \*\*cfads\_projection\_excel.csv\*\* \`\`\`csv year,revenue,opex,maintenance,wc\_change,tax,rcapex,cfads 1,0.0,0.6,0.0,0.0,0.0,0.0,\-0.6 2,0.0,1.2,0.0,0.0,0.0,0.0,\-1.2 3,3.0,2.4,0.1,0.2,0.0,0.0,0.3 4,10.0,4.5,0.2,0.3,0.0,0.0,5.0 5,20.0,6.5,0.3,0.5,0.3,0.0,12.4 6,28.0,8.5,0.4,0.5,0.6,0.0,18.0 7,34.0,10.0,0.5,0.5,0.9,0.0,22.1 8,38.0,11.5,0.6,0.6,1.2,0.0,24.1 9,40.0,12.0,0.7,0.6,1.4,0.0,25.3 10,38.0,12.0,1.2,0.6,1.4,0.0,22.8 11,35.0,11.5,0.8,0.5,1.2,0.0,21.0 12,33.0,11.0,0.8,0.4,1.1,0.0,19.7 13,30.0,10.5,0.9,0.3,0.9,0.0,17.4 14,27.0,10.0,0.8,0.2,0.8,0.0,15.2 15,24.0,9.5,0.6,0.2,0.7,0.0,13.0 \`\`\` \#\#\# \*\*debt\_schedule\_excel.csv\*\* \`\`\`csv year,bop\_senior,bop\_mezz,bop\_sub,int\_senior,int\_mezz,int\_sub,amort\_senior,amort\_mezz,amort\_sub,service,dscr 1,43.20,18.00,10.80,2.592,1.530,1.188,0.000,0.000,0.000,5.310,\-0.113 2,43.20,18.00,10.80,2.592,1.530,1.188,0.000,0.000,0.000,5.310,\-0.226 3,43.20,18.00,10.80,2.592,1.530,1.188,0.000,0.000,0.000,5.310,0.057 4,43.20,18.00,10.80,2.592,1.530,1.188,0.000,0.000,0.000,5.310,0.942 5,43.20,18.00,10.80,2.592,1.530,1.188,2.269,0.746,0.227,8.552,1.450 6,40.93,17.25,10.57,2.456,1.467,1.163,5.130,1.686,0.513,12.414,1.450 7,35.80,15.57,10.06,2.148,1.323,1.107,7.464,2.453,0.746,15.241,1.450 8,28.34,13.12,9.31,1.700,1.115,1.025,8.947,2.940,0.895,16.621,1.450 9,19.39,10.18,8.42,1.163,0.865,0.926,10.146,3.334,1.015,17.448,1.450 10,9.24,6.84,7.40,0.555,0.582,0.814,9.244,3.565,0.964,15.724,1.450 11,0.00,3.28,6.44,0.000,0.279,0.708,0.000,3.278,6.440,10.706,1.962 \`\`\` \--- \#\# \*\*‚úÖ USO DE ESTE DATASET\*\* **\*\*Para implementar el c√≥digo Python:\*\*** \`\`\`python *\# Cargar par√°metros exactos del Excel* params \= pd.read\_csv('project\_params\_excel.csv') cfads \= pd.read\_csv('cfads\_projection\_excel.csv') debt \= pd.read\_csv('debt\_schedule\_excel.csv') *\# Estos n√∫meros garantizan:* *\# \- DSCR \= 1.45 en a√±os operativos* *\# \- Repago completo en a√±o 11* *\# \- Coherencia matem√°tica validada* \`\`\` **\*\*Validaci√≥n cruzada:\*\*** \- Los tests deben comparar outputs del c√≥digo vs estos valores \- Tolerancia: ¬±0.01% para m√©tricas clave \- DSCR\_calculado debe igualar DSCR\_tabla con precisi√≥n de 4 decimales

---

## **WP-03: WATERFALL DE PAGOS (CON MRA)**

### **T-014: Clase DebtStructure**

**¬øEn qu√© consiste?**  
 Crear una clase que represente la estructura completa de deuda del proyecto incluyendo m√∫ltiples tranches con diferentes caracter√≠sticas de riesgo, prelaci√≥n, tasas y t√©rminos.

**Relaciones con otras tareas:**

* **Depende de:** T-013 (DebtParams)  
* **Utilizado por:** T-006 (Waterfall necesita estructura), T-007 (Pricing usa tranches), T-026 (Pricing estoc√°stico)  
* **Complementa:** T-016 (Comparador de estructuras)

**Qu√© se debe implementar:**

* Clase DebtStructure en archivo pftoken/waterfall/debt\_structure.py que encapsula toda la informaci√≥n de la estructura de deuda  
* Clase anidada Tranche representando cada tramo individual con atributos: name (Senior/Mezzanine/Subordinated), principal\_amount (monto nominal), interest\_rate (tasa nominal anual), spread\_over\_base (spread adicional sobre tasa base), tenor\_years (plazo en a√±os), amortization\_type (bullet/amortizing/sculpted), seniority\_level (nivel de prelaci√≥n num√©rico: 1=senior, 2=mezz, 3=sub), security\_type (secured/unsecured), covenants (lista de restricciones espec√≠ficas)  
* Implementar m√©todo en Tranche calculate\_periodic\_rate que convierta tasa nominal anual a tasa por per√≠odo considerando frecuencia de pagos: si semestral entonces rate\_period \= (1 \+ rate\_annual)^(1/2) \- 1  
* Implementar m√©todo en Tranche calculate\_amortization\_schedule que genere cronograma de amortizaci√≥n: para bullet solo un pago final, para amortizing cuotas constantes usando f√≥rmula de anualidad, para sculpted permitir customizaci√≥n de perfil de principal por per√≠odo  
* DebtStructure contiene lista de tranches ordenados por seniority: tranches\[0\] es m√°s senior, tranches\[-1\] es m√°s subordinado  
* M√©todo add\_tranche que agregue nuevo tramo validando que seniority levels sean consecutivos y √∫nicos, que suma de principals no exceda l√≠mite razonable del proyecto, que spreads sean crecientes con subordinaci√≥n (mezz \> senior, sub \> mezz)  
* M√©todo calculate\_wacd (Weighted Average Cost of Debt) que compute costo ponderado de deuda como suma de (weight\_i √ó rate\_i) donde weight \= principal\_i / total\_principal  
* M√©todo calculate\_blended\_spread que compute spread promedio ponderado sobre curva base √∫til para comparaciones  
* M√©todo get\_tranche\_by\_name que retorne objeto Tranche dado nombre como "Senior" para facilitar acceso  
* M√©todo get\_total\_principal que sume principals de todos los tranches retornando deuda total del proyecto  
* M√©todo compare\_structures que reciba otra DebtStructure y compute diferencias: delta en WACD, delta en monto total, delta en spreads por tramo, √∫til para comparar estructura tradicional vs tokenizada  
* Implementar validaciones: verificar que al menos hay un tranche senior, que seniority es estrictamente decreciente, que tasas reflejan riesgo con sub \> mezz \> senior  
* Crear estructura TraditionalStructure con factory method que genere estructura t√≠pica de Project Finance bancario: 70% senior secured, 30% mezz unsecured, sin tramo subordinado  
* Crear estructura TokenizedStructure con factory method que genere estructura granular: 50% senior, 30% mezz, 20% sub permitiendo m√°s diversificaci√≥n de riesgo  
* Implementar serializaci√≥n to\_dict y from\_dict para persistir estructuras y compartir configuraciones  
* Documentar cada tramo con justificaci√≥n de t√©rminos: por qu√© esa tasa, por qu√© ese plazo, qu√© tipo de inversor target

### **T-015: Covenants y triggers**

**¬øEn qu√© consiste?**  
 Implementar el sistema de covenants financieros y triggers que gobiernan el comportamiento del waterfall, limitando distribuciones cuando los ratios caen por debajo de umbrales cr√≠ticos.

**Relaciones con otras tareas:**

* **Depende de:** T-006 (Waterfall base), T-004 (ratios DSCR/LLCR)  
* **Utilizado por:** T-021 (MC verifica breaches), T-038 (stress testing eval√∫a covenants), T-041 (resultados de estr√©s)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase CovenantEngine que centraliza toda la l√≥gica de verificaci√≥n y enforcement de covenants  
* Definir estructura Covenant como dataclass conteniendo: name (identificador del covenant), covenant\_type (DSCR/LLCR/LTV/otros), threshold\_value (valor umbral cr√≠tico), comparison\_operator (\>=, \<=, \==), action\_if\_breach (qu√© hacer si se viola), severity\_level (low/medium/high/critical)  
* Implementar covenants est√°ndar de Project Finance: DSCR m√≠nimo \= 1.25x para permitir distribuci√≥n de dividendos, DSCR cr√≠tico \= 1.0x por debajo del cual se activa cash sweep, LLCR m√≠nimo \= 1.0x para evitar default t√©cnico, LTV m√°ximo (Loan to Value) \= 85% para limitar apalancamiento  
* M√©todo check\_covenant que para cada covenant verificar en cada per√≠odo si se cumple o se viola: obtener valor actual de la m√©trica (ej. DSCR\_t), comparar contra threshold usando operator, retornar CovenanResult indicando si hay breach  
* Implementar l√≥gica de acciones correctivas autom√°ticas: si DSCR \< 1.25x entonces suspender dividendos y retener cash en reservas, si DSCR \< 1.15x entonces activar cash sweep forzando toda distribuci√≥n residual a prepago de deuda, si DSCR \< 1.0x entonces declarar evento de default t√©cnico requiriendo renegociaci√≥n con acreedores  
* Crear jerarqu√≠a de severidad: breaches leves (DSCR entre 1.15-1.25) solo suspenden dividendos, breaches moderados (DSCR entre 1.0-1.15) activan cash sweep, breaches cr√≠ticos (DSCR \< 1.0) pueden resultar en aceleraci√≥n de deuda  
* Implementar cross-default provisions: si un tramo entra en default, otros tranches pueden declarar cross-default permitiendo aceleraci√≥n de toda la deuda  
* M√©todo apply\_covenant\_actions que ejecute autom√°ticamente las acciones definidas cuando hay breach: modificar waterfall para retener cash, bloquear distribuciones a equity, desviar flujos a prepago de senior debt  
* Implementar cure periods: dar al proyecto ventana de tiempo para remediar breach antes de que se active enforcement, t√≠picamente 60-90 d√≠as para corregir ratio por debajo de umbral  
* M√©todo track\_covenant\_history que mantenga historial completo de cumplimiento/violaci√≥n de covenants a lo largo del tiempo √∫til para an√°lisis post-mortem  
* Crear estructura CovenantResult conteniendo: per√≠odo evaluado, valor de m√©trica, threshold, si hay breach, acci√≥n tomada, severidad  
* Implementar visualizaci√≥n de covenants: gr√°fico mostrando evoluci√≥n de DSCR con bandas de color verde (cumple), amarillo (warning), rojo (breach), l√≠neas horizontales en thresholds cr√≠ticos  
* Generar reporte de covenant compliance: tabla resumen mostrando para cada covenant cu√°ntos per√≠odos se cumpli√≥, cu√°ntos se viol√≥, duraci√≥n promedio de breaches, m√°xima magnitud de violaci√≥n  
* Documentar est√°ndares de industria: citar documentos de IFC (International Finance Corporation), gu√≠as de IPFA (Infrastructure Project Finance Association) sobre niveles t√≠picos de covenants en Project Finance

### **T-016: Comparador estructuras**

**¬øEn qu√© consiste?**  
 Crear herramienta anal√≠tica que compare sistem√°ticamente estructura de deuda tradicional bancaria versus estructura tokenizada granular, evaluando diferencias en costo de capital, perfil de riesgo y flexibilidad.

**Relaciones con otras tareas:**

* **Depende de:** T-014 (DebtStructure con m√∫ltiples tranches)  
* **Utilizado por:** T-009 (c√°lculo WACD para comparaci√≥n), T-036 (optimizaci√≥n necesita comparar estructuras), T-048 (notebook final incluye comparaci√≥n)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase StructureComparator que encapsula toda la l√≥gica de comparaci√≥n entre estructuras alternativas  
* M√©todo compare\_cost que calcule diferencias en costo de capital: delta\_WACD entre tradicional y tokenizada, diferencias en spreads por tramo comparable, diferencias en tasas all-in efectivas considerando fees y comisiones  
* M√©todo compare\_risk\_profile que eval√∫e perfil de riesgo: comparar LGD esperada de cada estructura, comparar concentraci√≥n de riesgo medida mediante √≠ndice Herfindahl, comparar diversificaci√≥n de base de inversores  
* Implementar m√©tricas de concentraci√≥n: estructura tradicional t√≠picamente tiene 2-3 tranches con pocos acreedores grandes, estructura tokenizada puede tener 3-5 tranches con cientos de tenedores peque√±os, calcular √≠ndice Herfindahl H \= Œ£(share\_i)¬≤ donde menor H indica mayor diversificaci√≥n  
* M√©todo compare\_flexibility que eval√∫e flexibilidad operativa: estructura tradicional tiene covenants m√°s restrictivos pero permite renegociaci√≥n bilateral, estructura tokenizada tiene covenants m√°s laxos pero renegociaci√≥n requiere mayor√≠as calificadas dificultando restructuras  
* Implementar an√°lisis de secondary market liquidity: tokens pueden ser m√°s l√≠quidos si hay AMM o exchange, pr√©stamos bancarios tradicionales tienen mercado secundario limitado, evaluar bid-ask spreads esperados como proxy de liquidez  
* M√©todo compare\_governance que analice diferencias en gobernanza: estructura tradicional tiene agente √∫nico representando acreedores facilitando decisiones, estructura tokenizada requiere votaciones on-chain de tokenholders con mayor√≠as definidas en smart contract  
* M√©todo compare\_transaction\_costs que estime costos de transacci√≥n: estructura tradicional tiene costos legales y due diligence elevados pero one-time, estructura tokenizada tiene costos de emisi√≥n de tokens y setup de smart contracts con costos operativos recurrentes menores  
* Implementar c√°lculo de all-in cost considerando no solo tasa de inter√©s sino tambi√©n: arrangement fees de bancos, legal fees, auditor√≠a, costos de emisi√≥n de tokens, gas fees de blockchain, costos de market making si hay AMM  
* Crear estructura ComparisonResult como dataclass conteniendo: delta\_WACD absoluto y porcentual, diferencias en cada componente de costo, diferencias en m√©tricas de riesgo con niveles de significancia estad√≠stica, scores cualitativos de flexibilidad y gobernanza  
* M√©todo generate\_comparison\_table que produzca tabla lado a lado con todas las m√©tricas: columna para estructura tradicional, columna para estructura tokenizada, columna de diferencias, highlighting visual de diferencias significativas  
* Implementar visualizaci√≥n de waterfall comparativo: dos gr√°ficos de cascada lado a lado mostrando flujo de CFADS a trav√©s de cada estructura, destacando diferencias en retenci√≥n de reservas y timing de distribuciones  
* Generar radar chart comparativo: ejes para diferentes dimensiones (costo, riesgo, liquidez, flexibilidad, gobernanza), √°rea de estructura tradicional versus √°rea de tokenizada, visualizar trade-offs multidimensionales  
* Implementar sensitivity analysis sobre comparaci√≥n: c√≥mo cambia conclusi√≥n si spreads tokenizados son 50 bps mayores/menores, si costos de transacci√≥n son 20% mayores/menores  
* Documentar supuestos cr√≠ticos: suponer que mercado de tokens alcanza cierta madurez y liquidez, que costos de gas se mantienen en niveles actuales, que hay suficiente demanda de inversores retail para tokens  
* Generar reporte ejecutivo de comparaci√≥n: s√≠ntesis de una p√°gina destacando si tokenizaci√≥n reduce WACD y bajo qu√© condiciones, identificar casos de uso donde tokenizaci√≥n es claramente superior versus donde tradicional es preferible

### **T-017: Implementaci√≥n detallada waterfall**

**¬øEn qu√© consiste?**  
 Implementar la l√≥gica completa y detallada del mecanismo de waterfall que distribuye CFADS siguiendo estricta prelaci√≥n entre pagos de intereses, amortizaci√≥n de principal, fondeo de reservas y distribuci√≥n de dividendos.

**Relaciones con otras tareas:**

* **Depende de:** T-015 (covenants definen reglas de distribuci√≥n)  
* **Utilizado por:** T-006 (clase Waterfall principal), T-021 (MC ejecuta waterfall en cada trayectoria)  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt (tarea compleja)

**Qu√© se debe implementar:**

* Clase WaterfallEngine que ejecuta la l√≥gica completa de distribuci√≥n de flujos per√≠odo por per√≠odo  
* Definir orden estricto de prelaci√≥n siguiendo est√°ndares de Project Finance: (1) OPEX del per√≠odo actual, (2) Intereses tramo Senior, (3) Intereses tramo Mezz, (4) Intereses tramo Sub, (5) Fondeo DSRA hasta target, (6) Principal Senior seg√∫n schedule, (7) Principal Mezz seg√∫n schedule, (8) Principal Sub seg√∫n schedule, (9) Fondeo MRA hasta target, (10) Sweep prepago de Senior si covenant breach, (11) Distribuciones a equity solo si DSCR \> threshold  
* Implementar m√©todo execute\_waterfall que tome CFADS del per√≠odo y DebtStructure actual, ejecute pasos en orden secuencial, rastree disponibilidad de cash en cada paso, retorne WaterfallResult con detalle completo de distribuciones  
* Paso 1 \- Pago de OPEX: reservar monto necesario para cubrir costos operativos del per√≠odo, si CFADS insuficiente entonces extraer desde DSRA, si a√∫n insuficiente declarar evento de insolvencia operativa  
* Paso 2-4 \- Pago de intereses: para cada tranche en orden de seniority calcular intereses devengados del per√≠odo \= saldo\_outstanding √ó tasa\_per√≠odo, pagar intereses desde cash disponible, si insuficiente entonces extraer desde DSRA, si a√∫n insuficiente acumular intereses impagos para carry-forward, registrar evento de default si intereses quedan impagos  
* Implementar accrual de intereses impagos: si no se pueden pagar intereses en un per√≠odo, acumularlos en cuenta de intereses diferidos que se pagan con prioridad en per√≠odos futuros  
* Paso 5 \- Fondeo DSRA: calcular target de DSRA \= 100% del pr√≥ximo servicio de deuda (intereses \+ principal pr√≥ximo per√≠odo), comparar contra saldo actual de DSRA, si hay d√©ficit entonces aportar desde cash disponible hasta alcanzar target o agotar cash  
* Paso 6-8 \- Amortizaci√≥n de principal: para cada tranche en orden de seniority obtener monto de principal programado seg√∫n amortization schedule de T-014, durante grace period monto \= 0, durante ramping monto \= scheduled √ó phi(t), pagar principal desde cash disponible, si insuficiente entonces usar DSRA como √∫ltimo recurso, si a√∫n insuficiente registrar evento de default por principal impago  
* Implementar tracking de saldos: despu√©s de cada pago de principal actualizar outstanding balance de cada tranche reduciendo saldo, validar que saldo nunca se vuelve negativo  
* Paso 9 \- Fondeo MRA: calcular target de MRA \= 50% del pr√≥ximo RCAPEX planificado, comparar contra saldo actual de MRA, aportar desde cash residual hasta alcanzar target o agotar cash, MRA solo se fondea si DSCR \> 1.15 para priorizar servicio de deuda  
* Paso 10 \- Cash sweep: si hay breach de covenant DSCR \< umbral entonces activar mecanismo de sweep, todo cash residual despu√©s de fondeo de reservas se usa para prepago extraordinario de tranche Senior reduciendo saldo y acelerando amortizaci√≥n, cash sweep contin√∫a hasta que DSCR se recupera por encima de umbral  
* Paso 11 \- Distribuciones a equity: solo si DSCR \> 1.30 y todas las reservas est√°n en target completo y no hay covenants violados entonces distribuir cash residual como dividendos a equity holders, calcular dividend coverage ratio \= dividendos / equity contribution como m√©trica de retorno  
* Crear estructura WaterfallResult conteniendo: cash inicial disponible, monto pagado en cada paso de prelaci√≥n (opex, intereses por tramo, principal por tramo, aportes a reservas), cash final residual, flags indicando eventos especiales (default, sweep activado, distribuci√≥n bloqueada)  
* Implementar validaci√≥n de conservaci√≥n de cash: suma de todos los pagos y cash residual debe igualar cash inicial, cualquier discrepancia indica error en l√≥gica  
* M√©todo plot\_waterfall que genere visualizaci√≥n tipo cascada (waterfall chart) mostrando flujo de CFADS bajando por cada nivel de prelaci√≥n con barras horizontales indicando montos retenidos en cada paso, coloreadas por tipo de pago  
* Implementar tests de escenarios extremos: qu√© pasa si CFADS es negativo, qu√© pasa si es insuficiente para cubrir OPEX, qu√© pasa si solo alcanza para pagar intereses senior, verificar que l√≥gica no rompe en casos l√≠mite  
* Documentar exhaustivamente cada paso con referencias a documentos de Project Finance como Gatti (2018) "Project Finance in Theory and Practice", incluir diagramas de flujo mostrando decisiones en cada nodo del waterfall  
* Generar reporte detallado de ejecuci√≥n: tabla mostrando per√≠odo por per√≠odo el resultado completo del waterfall, tracking de saldos de deuda, evoluci√≥n de reservas, distribuciones acumuladas

### **T-006: Clase Waterfall con MRA**

**¬øEn qu√© consiste?**  
 Crear la clase principal que orquesta el mecanismo completo de waterfall integrando c√°lculo de CFADS, aplicaci√≥n de prelaci√≥n, gesti√≥n de reservas DSRA y MRA, y verificaci√≥n de covenants.

**Relaciones con otras tareas:**

* **Cr√≠tico: Depende de:** T-003B (CFADS), T-005B (validaci√≥n), T-014 (DebtStructure), T-017 (l√≥gica detallada)  
* **Utilizado por:** T-021 (Monte Carlo), T-026 (Pricing), T-038 (Stress testing), T-040 (motor de estr√©s)  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt (tarea cr√≠tica del path cr√≠tico)

**Qu√© se debe implementar:**

* Clase Waterfall que coordina todos los componentes del waterfall de pagos  
* Constructor que reciba ProjectParams, DebtStructure, WaterfallParams y CFADSResult como inputs, inicialice reservas DSRA y MRA en cero al comienzo del proyecto, configure covenants seg√∫n T-015  
* M√©todo run\_full\_waterfall que ejecute waterfall completo para todos los per√≠odos del horizonte de proyecci√≥n: iterar sobre cada per√≠odo t, obtener CFADS\_t del CFADSResult, ejecutar WaterfallEngine de T-017 para distribuir el flujo, actualizar saldos de deuda y reservas, verificar covenants mediante CovenantEngine de T-015, acumular resultados en estructura de datos  
* Implementar gesti√≥n de DSRA (Debt Service Reserve Account): reserva que acumula cash para cubrir servicio de deuda en per√≠odos con CFADS insuficiente, target t√≠picamente \= 100% del pr√≥ximo servicio semestral, en cada per√≠odo verificar si DSRA est√° por debajo de target y fondear con prioridad despu√©s de pagar intereses, permitir extracci√≥n de DSRA solo para cubrir d√©ficit en pagos de deuda  
* Implementar gesti√≥n de MRA (Maintenance Reserve Account): reserva espec√≠fica para RCAPEX de reemplazo de sat√©lites, target t√≠picamente \= 50-100% del pr√≥ximo RCAPEX planificado, fondear MRA solo despu√©s de cumplir servicio de deuda y alcanzar target de DSRA, extraer de MRA cuando llega per√≠odo de RCAPEX para financiar reemplazo sin impactar CFADS operativo  
* Implementar l√≥gica de liberaci√≥n de reservas: al final del proyecto o cuando deuda est√° completamente pagada, liberar fondos acumulados en DSRA y MRA para distribuci√≥n a equity, calcular retorno total a equity \= dividendos acumulados \+ liberaci√≥n de reservas \+ valor residual de activos  
* M√©todo calculate\_distributions\_to\_equity que sume todos los flujos que llegaron a equity holders: dividendos en per√≠odos normales, distribuciones finales al vencimiento, retorno de reservas sobrantes  
* Implementar tracking de eventos de default: registrar si en alg√∫n per√≠odo hubo impago de intereses o principal, duraci√≥n del default, monto acumulado de pagos diferidos, eventual recuperaci√≥n de default mediante cash sweep en per√≠odos posteriores  
* Crear estructura FullWaterfallResult conteniendo: serie temporal completa de distribuciones per√≠odo por per√≠odo, evoluci√≥n de saldos de deuda por tramo, evoluci√≥n de reservas DSRA y MRA, registro de eventos de covenant breach y default, m√©tricas agregadas como equity IRR  
* M√©todo calculate\_equity\_irr que compute tasa interna de retorno para equity holders considerando equity contribution inicial como outflow negativo y todas las distribuciones como inflows positivos, usar algoritmo de Newton-Raphson para resolver ecuaci√≥n de NPV \= 0  
* M√©todo calculate\_debt\_coverage\_metrics que para cada tramo compute m√©tricas de performance: porcentaje de per√≠odos con intereses pagados completamente, porcentaje de per√≠odos con principal amortizado seg√∫n schedule, monto total de prepagos extraordinarios mediante cash sweep, recovery rate final si hubo default  
* Implementar an√°lisis de sensibilidad del waterfall: c√≥mo cambian distribuciones si CFADS aumenta/disminuye en 10%, c√≥mo cambia equity IRR si DSRA target es 50% versus 150%, generar tornado chart mostrando sensibilidades  
* M√©todo compare\_waterfall\_scenarios que ejecute waterfall bajo diferentes supuestos y compare resultados lado a lado: escenario base, escenario con grace period m√°s largo, escenario sin MRA, escenario con DSCR covenant m√°s estricto  
* Implementar visualizaci√≥n integrada: dashboard con m√∫ltiples paneles mostrando (1) CFADS en el tiempo, (2) waterfall de un per√≠odo representativo, (3) evoluci√≥n de reservas, (4) distribuciones acumuladas por categor√≠a, (5) saldos de deuda amortiz√°ndose  
* Generar reporte ejecutivo de waterfall: resumen de una p√°gina con m√©tricas clave (equity IRR, DSCR m√≠nimo observado, per√≠odos con default, total distribuido a equity, WACD efectivo), gr√°fico principal mostrando flujo de fondos agregado, tabla con escenarios comparativos  
* Documentar mec√°nica completa: incluir diagrama de flujo del waterfall, tabla explicativa de cada paso de prelaci√≥n, glosario de t√©rminos t√©cnicos (DSRA, MRA, cash sweep, PIK), referencias a est√°ndares de Project Finance

### **T-020: Gobernanza digital**

**¬øEn qu√© consiste?**  
 Dise√±ar el framework conceptual de c√≥mo funcionar√≠a la gobernanza on-chain de una estructura de deuda tokenizada, incluyendo voting mechanisms, quorum requirements y enforcement autom√°tico mediante smart contracts.

**Relaciones con otras tareas:**

* **Depende de:** T-017 (waterfall implementado que ser√≠a automatizado)  
* **Complementa:** T-016 (comparaci√≥n incluye diferencias en gobernanza)  
* **Informa:** T-053 (dise√±o AMM necesita considerar gobernanza)  
* **Duraci√≥n:** 2 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Documento de dise√±o de gobernanza digital especificando roles y responsabilidades: token holders tienen derechos de voto proporcionales a holdings, servicer ejecuta operaciones rutinarias del waterfall, trustee o agente verifica compliance con covenants, oracle provider suministra datos externos como tasas de inter√©s  
* Dise√±ar mecanismo de votaci√≥n on-chain: cada token representa un voto, propuestas requieren m√≠nimo de tokens para ser sometidas (ej. 5% del total), votaci√≥n abierta durante per√≠odo definido (ej. 7 d√≠as), aprobaci√≥n requiere quorum m√≠nimo (ej. 30% de participaci√≥n) y mayor√≠a calificada (ej. 66% a favor)  
* Definir tipos de decisiones que requieren votaci√≥n: cambios a waterfall parameters como ajustar target de DSRA, modificaci√≥n de covenants como relajar DSCR m√≠nimo, aprobaci√≥n de CAPEX extraordinario, restructura de deuda en caso de distress, reemplazo de servicer o auditores  
* Implementar jerarqu√≠a de mayor√≠as requeridas: decisiones rutinarias (cambio de servicer) \= mayor√≠a simple 50%, decisiones importantes (ajuste de covenants) \= mayor√≠a calificada 66%, decisiones cr√≠ticas (restructura de deuda) \= s√∫per mayor√≠a 75%, cambios fundamentales (modificaci√≥n de prelaci√≥n) \= unanimidad 100%  
* Dise√±ar enforcement autom√°tico mediante smart contracts: covenant breaches detectados autom√°ticamente por smart contract monitoreando DSCR calculado por oracle, activaci√≥n autom√°tica de cash sweep cuando DSCR \< threshold sin necesidad de votaci√≥n, bloqueo autom√°tico de distribuciones a equity cuando reservas por debajo de target  
* Implementar sistema de or√°culos para datos off-chain: oracle suministra CFADS realizado cada per√≠odo desde sistema contable del proyecto, oracle suministra tasas de inter√©s de referencia para floating rate tranches, m√∫ltiples oracles con mecanismo de mediaci√≥n para evitar manipulaci√≥n, penalizaciones para oracles que reportan datos incorrectos  
* Dise√±ar representaci√≥n de seniority on-chain: diferentes clases de tokens representan diferentes tranches (SeniorToken, MezzToken, SubToken), smart contract enforza prelaci√≥n en waterfall autom√°ticamente transfiriendo primero a holders de SeniorToken, holders de tranches subordinados solo reciben distribuci√≥n despu√©s que senior est√° completo  
* Implementar programabilidad de distribuciones: holders pueden configurar si reinvertir autom√°ticamente distribuciones en m√°s tokens, si transferir a wallet externa, si convertir a stablecoin mediante DEX autom√°tico, si usar para pagar fees de protocolo  
* Dise√±ar mecanismo de resoluci√≥n de disputes: si hay desacuerdo sobre datos de oracle o ejecuci√≥n de waterfall, token holders pueden proponer arbitraje on-chain, √°rbitros independientes (seleccionados mediante votaci√≥n) revisan evidencia y emiten ruling, ruling ejecutado autom√°ticamente por smart contract con distribuci√≥n retroactiva si necesario  
* Implementar transparency y auditability: toda la informaci√≥n de CFADS, distribuciones, saldos y eventos est√° on-chain como eventos emitidos por smart contract, cualquiera puede verificar historial completo y reproducir c√°lculos, auditor√≠a en tiempo real sin necesidad de disclosure peri√≥dico  
* Crear especificaci√≥n t√©cnica de interfaces de smart contracts: interfaz IERC20 extendida para tokens de deuda con metadatos adicionales (tranche, seniority, maturity), interfaz IWaterfall con m√©todos executeDistribution y checkCovenants, interfaz IGovernance con m√©todos propose, vote y execute  
* Documentar limitaciones y trade-offs: gobernanza on-chain es m√°s transparente y autom√°tica pero menos flexible que tradicional, cambios requieren mayor√≠as calificadas dificultando restructuras urgentes, dependencia de oracles introduce risk de manipulaci√≥n o fallo, costos de gas de ejecutar l√≥gica compleja on-chain pueden ser significativos  
* Comparar con gobernanza tradicional: en estructura bancaria agente puede tomar decisiones r√°pidas en nombre de acreedores mediante waiver, en estructura tokenizada cada cambio requiere proceso de votaci√≥n que puede tomar semanas, analizar casos donde cada enfoque es superior  
* Dise√±ar governance minimization: decidir qu√© debe estar on-chain versus off-chain, c√°lculos complejos como Monte Carlo se ejecutan off-chain con resultados publicados on-chain mediante oracle, l√≥gica simple como comparar DSCR contra threshold se ejecuta on-chain, balancear descentralizaci√≥n contra costo y complejidad  
* Generar diagrama de arquitectura de gobernanza: mostrar interacci√≥n entre smart contracts, oracles, token holders y sistemas externos, flujo de informaci√≥n desde proyecto hasta blockchain, mecanismos de votaci√≥n y enforcement  
* Incluir referencias a protocolos DeFi existentes: analizar MakerDAO governance para inspiraci√≥n sobre voting mechanisms, analizar Aave governance sobre propuestas y ejecuci√≥n, analizar Compound timelock para seguridad de cambios cr√≠ticos

## **WP-04: PRICING Y CURVAS**

### **T-007: M√≥dulo valuaci√≥n tramos**

**¬øEn qu√© consiste?**  
 Implementar el m√≥dulo core de pricing que val√∫a cada tramo de deuda usando metodolog√≠a de Discounted Cash Flow (DCF) con factores de descuento derivados de curva spot calibrada.

**Relaciones con otras tareas:**

* **Depende de:** T-006 (waterfall genera flujos por tramo), T-014 (DebtStructure define tranches)  
* **Utilizado por:** T-008 (curva spot para descontar), T-009 (WACD usa prices), T-026 (pricing estoc√°stico extiende)  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase PricingEngine que centraliza toda la l√≥gica de valuaci√≥n de instrumentos de deuda  
* M√©todo price\_tranche que tome un Tranche y WaterfallResult, extraiga flujos espec√≠ficos que van a ese tranche (intereses \+ principal), descuente cada flujo usando tasa apropiada, sume valores presentes para obtener precio del tranche  
* Implementar extracci√≥n de flujos por tranche: dado WaterfallResult que contiene distribuciones totales por per√≠odo, filtrar solo pagos que corresponden al tranche espec√≠fico seg√∫n prelaci√≥n y seniority, construir serie temporal de cash flows esperados para ese tranche  
* M√©todo calculate\_discount\_factors que tome curva spot como input y compute factores de descuento para cada horizonte temporal: DF(t) \= 1 / (1 \+ r(t))^t donde r(t) es tasa spot para maturity t, manejar convenci√≥n de conteo de d√≠as (Actual/360 versus 30/360), ajustar por frecuencia de cupones  
* Implementar pricing usando ecuaci√≥n fundamental: PV\_tranche \= Œ£\[CF\_t √ó DF\_t\] sumando sobre todos los flujos esperados del tranche, donde CF\_t incluye cupones de inter√©s y repagos de principal, DF\_t son factores de descuento apropiados  
* Calcular yield to maturity (YTM) del tranche: dado precio de mercado (o calculado mediante DCF), resolver iterativamente para la tasa que hace PV \= precio, usar algoritmo de Newton-Raphson o bisecci√≥n, validar que converge a soluci√≥n √∫nica  
* Implementar c√°lculo de spread sobre curva base: dado YTM del tranche y curva risk-free de referencia, calcular spread \= YTM \- curva\_libre\_riesgo(maturity), este spread refleja primas por riesgo de cr√©dito, liquidez y complejidad  
* M√©todo calculate\_tranche\_duration que compute Duraci√≥n de Macaulay: suma ponderada de tiempos de flujos donde pesos \= PV de cada flujo / PV total, Duration \= Œ£\[t √ó PV\_t / PV\_total\], mide sensibilidad del precio a cambios en tasa de inter√©s  
* Implementar c√°lculo de Modified Duration: ModDur \= MacDur / (1 \+ y) donde y es yield, mide cambio porcentual en precio por cambio de 1 bp en yield: ŒîP/P ‚âà \-ModDur √ó Œîy  
* M√©todo calculate\_convexity que compute segunda derivada de precio respecto a yield: Convexity \= Œ£\[t √ó (t+1) √ó PV\_t / PV\_total\] / (1 \+ y)¬≤, captura curvatura de relaci√≥n precio-yield, mejora aproximaci√≥n lineal de duration  
* Implementar ajuste de pricing por riesgo de cr√©dito: incorporar PD y LGD de modelo Merton de T-005, calcular Expected Loss \= PD √ó LGD √ó Exposure, ajustar precio restando valor presente de p√©rdidas esperadas  
* M√©todo calculate\_risk\_adjusted\_price: precio\_ajustado \= precio\_libre\_riesgo \- PV(Expected\_Loss), donde PV de p√©rdidas esperadas se calcula descontando EL en cada horizonte con tasa libre de riesgo  
* Implementar pricing de tranches con prepayment: si waterfall incluye cash sweep que acelera prepago de principal, modelar prepayment risk usando CPR (Conditional Prepayment Rate), ajustar flujos esperados incorporando probabilidad de prepago anticipado  
* Crear estructura PricingResult conteniendo: precio calculado (PV), yield to maturity, spread sobre curva base, duration, convexity, precio ajustado por riesgo, sensibilidades a par√°metros clave  
* M√©todo price\_entire\_structure que compute precio de todos los tranches simult√°neamente: iterar sobre cada tranche de DebtStructure, calcular precio individual, sumar para obtener valor total de deuda, comparar contra par value para detectar over/under valuation  
* Implementar validaci√≥n de pricing: verificar que precio est√° entre 0 y valor nominal ajustado, que yield es positivo, que duration es positivo y menor que maturity, que convexity es positivo  
* M√©todo calculate\_pricing\_sensitivities que compute sensibilidad de precio a cambios en inputs clave: sensibilidad a CFADS (cu√°nto cambia precio si CFADS aumenta 10%), sensibilidad a tasa de descuento (cu√°nto cambia si curva sube 100 bps), sensibilidad a PD (cu√°nto cae si PD se duplica)  
* Generar tabla de pricing: mostrar para cada tranche su precio, YTM, spread, duration y m√©tricas de riesgo en formato profesional con redondeo apropiado  
* Implementar visualizaci√≥n de curva de precio-yield: graficar relaci√≥n entre precio del tranche y diferentes niveles de yield, mostrar curvatura (convexity) y tangente (duration), marcar punto actual de mercado  
* Documentar metodolog√≠a de pricing: explicar supuestos (flujos descontados a YTM, ausencia de arbitraje, mercados l√≠quidos), limitaciones (no considera riesgo de liquidez, asume cumplimiento de covenants), referencias a literatura de fixed income como Fabozzi "Bond Markets, Analysis and Strategies"

### **T-008: Curva zero-coupon**

**¬øEn qu√© consiste?**  
 Construir curva spot zero-coupon que sirva como referencia para descontar flujos de caja, calibrada a partir de instrumentos de mercado observables y extrapolada para horizontes largos del proyecto.

**Relaciones con otras tareas:**

* **Depende de:** T-007 (pricing necesita curva)  
* **Utilizado por:** T-007 (descuenta flujos), T-027 (duration usa curva), T-028 (calibraci√≥n de spreads)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase SpotCurve que encapsula curva de tasas spot zero-coupon  
* M√©todo build\_curve\_from\_market\_data que tome como input instrumentos observables de mercado: tasas de dep√≥sitos para tenores cortos (1M, 3M, 6M), tasas de swaps de tasa de inter√©s para tenores medios (1Y, 2Y, 3Y, 5Y, 7Y), yields de bonos corporativos para tenores largos (10Y, 15Y), extraer tasas spot impl√≠citas mediante bootstrapping  
* Implementar bootstrapping recursivo: para tenor m√°s corto (ej. 6M), tasa spot \= tasa observada directamente de dep√≥sito, para tenores subsiguientes, usar precios de swaps o bonos con cupones para despejar tasa spot que hace precio te√≥rico \= precio observado, resolver iterativamente construyendo curva de corto a largo  
* Implementar interpolaci√≥n para tenores no observados: si tenemos tasas spot para 1Y y 2Y pero necesitamos 1.5Y, usar interpolaci√≥n spline c√∫bica que garantiza suavidad de curva, verificar que curva interpolada no tiene arbitrajes (forward rates positivos)  
* M√©todo get\_spot\_rate que retorne tasa spot para cualquier tenor solicitado: si tenor est√° en nodos de curva retornar directamente, si no interpolar usando spline, extraer hacia horizontes m√°s largos usando extrapolaci√≥n plana o con decay exponencial hacia long-term rate  
* Implementar extrapolaci√≥n para proyectos con tenores \> 10Y: asumir que tasa spot converge a long-term rate estimado como tasa de crecimiento nominal de largo plazo de econom√≠a (ej. 4-5%), usar funci√≥n de transici√≥n suave desde √∫ltimo punto observado hasta long-term rate en horizonte de 20-30 a√±os  
* M√©todo get\_forward\_rate que calcule tasa forward impl√≠cita entre dos fechas futuras: f(t1, t2) \= \[(1 \+ s(t2))^t2 / (1 \+ s(t1))^t1\]^(1/(t2-t1)) \- 1, donde s(t) son tasas spot, forwards √∫tiles para proyectar tasas de refinanciamiento  
* Implementar ajuste por convexidad en derivaci√≥n de forwards: ajustar por sesgo de convexidad cuando se usan instrumentos con optionalidad embedded  
* M√©todo shift\_curve que aplique shock paralelo a toda la curva: sumar/restar bps uniformemente a todas las tasas, √∫til para stress testing sensibilidad de pricing a movimientos de tasas de inter√©s  
* Implementar non-parallel shifts: twist (steepening/flattening donde short end sube menos que long end), butterfly (middle of curve sube m√°s que extremos), simular diferentes scenarios de cambio en shape de curva  
* M√©todo calculate\_discount\_factors que convierta toda la curva spot a factores de descuento: DF(t) \= 1 / (1 \+ s(t))^t para cada nodo de la curva, almacenar como array de numpy para operaciones vectorizadas eficientes  
* Crear estructura CurveResult conteniendo: array de tenores en a√±os, array de tasas spot correspondientes, array de discount factors, m√©todo de interpolaci√≥n usado, fecha de construcci√≥n de curva  
* Implementar validaci√≥n de curva construida: verificar que tasas spot son estrictamente positivas, que curva es monot√≥nicamente creciente o tiene forma razonable, que forward rates impl√≠citos son positivos (no arbitrage condition)  
* M√©todo plot\_curves que genere visualizaci√≥n de curva spot, curva de forwards y factores de descuento en mismo gr√°fico con m√∫ltiples ejes Y, mostrar nodos observados versus interpolados  
* Implementar comparaci√≥n con curvas de mercado reales: importar curvas oficiales de Tesoro USA o Swaps de Bloomberg, comparar nuestra curva construida contra benchmark, calcular spreads  
* Documentar convenciones de mercado: especificar day count convention usado (Actual/360 para USD swaps, 30/360 para bonos corporativos), especificar business day convention para ajuste de fechas, documentar fuentes de datos de mercado  
* Incluir an√°lisis de sensibilidad: c√≥mo var√≠a pricing de proyecto si curva sube 100 bps paralelo, si hay steepening de 50 bps, generar tornado chart de impactos  
* Referencias acad√©micas: m√©todos de bootstrapping seg√∫n literatura de fixed income, papers sobre interpolaci√≥n de curvas como papers de BIS sobre construcci√≥n de curvas centrales

### **T-009: WACD**

**¬øEn qu√© consiste?**  
 Calcular el Weighted Average Cost of Debt (WACD) de la estructura completa considerando diferentes tasas y pesos de cada tramo, comparar WACD entre estructura tradicional y tokenizada.

**Relaciones con otras tareas:**

* **Depende de:** T-007 (pricing de tranches con yields), T-014 (DebtStructure con weights)  
* **Utilizado por:** T-016 (comparaci√≥n de estructuras usa WACD), T-036 (optimizaci√≥n minimiza WACD)  
* **Duraci√≥n:** 2 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase WACDCalculator que compute costo promedio ponderado de deuda  
* M√©todo calculate\_wacd que tome DebtStructure con m√∫ltiples tranches, extraiga peso de cada tramo \= principal\_tramo / principal\_total, extraiga costo de cada tramo \= yield o tasa contractual, compute WACD \= Œ£(weight\_i √ó cost\_i)  
* Implementar c√°lculo usando yields de mercado: si hay pricing disponible de T-007, usar yield to maturity de cada tranche como costo para reflejar precio de mercado actual, si no hay precio usar tasa contractual nominal  
* Implementar ajuste por fees y costos de transacci√≥n: incluir arrangement fees, commitment fees, legal costs amortizados sobre vida de deuda, calcular all-in WACD que refleja costo total efectivo para el proyecto  
* M√©todo calculate\_wacd\_traditional que compute WACD de estructura tradicional bancaria con par√°metros t√≠picos: 70% senior secured a 6%, 30% mezz unsecured a 9%, WACD\_trad \= 0.7√ó6% \+ 0.3√ó9% \= 6.9%  
* M√©todo calculate\_wacd\_tokenized que compute WACD de estructura tokenizada granular: 50% senior a 5.5%, 30% mezz a 8%, 20% sub a 12%, WACD\_token \= 0.5√ó5.5% \+ 0.3√ó8% \+ 0.2√ó12% \= 7.55%  
* Implementar an√°lisis de diferencial: delta\_WACD \= WACD\_tokenized \- WACD\_traditional, interpretar si tokenizaci√≥n reduce o aumenta costo de capital, explicar drivers de diferencia (m√°s granularidad implica m√°s sub que es m√°s caro, pero senior puede ser m√°s barato por mayor transparencia)  
* M√©todo calculate\_wacd\_after\_tax que ajuste por deductibilidad fiscal de intereses: WACD\_after\_tax \= WACD\_pretax √ó (1 \- tax\_rate), refleja ahorro fiscal por escudo tributario de deuda  
* Implementar c√°lculo de WACC (Weighted Average Cost of Capital) completo: incluir no solo deuda sino tambi√©n costo de equity, WACC \= (E/V)√ór\_equity \+ (D/V)√ór\_debt√ó(1-tax) donde E=equity, D=debt, V=valor total, √∫til para comparar contra retorno del proyecto  
* M√©todo compare\_wacd\_scenarios que compute WACD bajo diferentes supuestos de spreads: escenario optimista con spreads 50 bps menores, escenario pesimista con spreads 100 bps mayores, analizar sensibilidad de conclusi√≥n sobre tokenizaci√≥n  
* Crear estructura WACDResult conteniendo: WACD de estructura tradicional, WACD de estructura tokenizada, diferencial absoluto y porcentual, breakdown por tramo mostrando contribuci√≥n de cada uno, WACD after-tax  
* Implementar an√°lisis de valor creado: calcular NPV del proyecto usando WACC como tasa de descuento, comparar contra inversi√≥n inicial, calcular excess return \= IRR \- WACC como medida de creaci√≥n de valor  
* M√©todo plot\_wacd\_breakdown que genere gr√°fico de barras apiladas mostrando contribuci√≥n de cada tramo al WACD total, comparar lado a lado estructura tradicional versus tokenizada  
* Generar tabla de sensibilidad: mostrar c√≥mo var√≠a WACD si spread de cada tramo cambia en ¬±50 bps, identificar cu√°l tramo tiene mayor impacto en WACD total (t√≠picamente senior por tener mayor peso)  
* Documentar interpretaci√≥n de WACD: WACD menor indica estructura de capital m√°s eficiente, pero debe balancearse contra flexibilidad y riesgo, WACD no es √∫nico criterio de selecci√≥n de estructura  
* Incluir benchmarking: comparar WACD calculado contra WACDs t√≠picos de Project Finance en sector telecomunicaciones satelitales (rango 6-9% seg√∫n grado de inversi√≥n del proyecto)  
* Referencias: papers sobre optimal capital structure en Project Finance, an√°lisis de trade-off entre tax shield y costs of financial distress

### **T-018: Pricing multimoneda**

**¬øEn qu√© consiste?**  
 Extender el framework de pricing para manejar estructuras de deuda denominadas en m√∫ltiples monedas, incorporando riesgo cambiario y curvas de descuento espec√≠ficas por moneda.

**Relaciones con otras tareas:**

* **Depende de:** T-008 (curva spot base que se extiende para cada moneda)  
* **Extiende:** T-007 (pricing con descuentos multimoneda)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase MultiCurrencyPricer que generaliza pricing para tranches en diferentes monedas  
* Implementar concepto de currency denomination: cada tranche puede estar denominado en USD, EUR, GBP u otra moneda, CFADS del proyecto t√≠picamente en una moneda base (USD para proyecto satelital), requiere conversi√≥n de flujos  
* M√©todo build\_currency\_curves que construya curva spot separada para cada moneda relevante: curva USD basada en SOFR swaps, curva EUR basada en EURIBOR swaps, curva GBP basada en SONIA swaps, calibrar cada curva independientemente usando instrumentos de esa moneda  
* Implementar pricing de tranche en moneda extranjera: convertir flujos proyectados de moneda base a moneda de denominaci√≥n del tranche usando forward FX rates, descontar flujos convertidos usando curva apropiada para esa moneda, obtener precio en moneda del tranche  
* Calcular forward FX rates usando paridad de tasas de inter√©s cubiertas (Covered Interest Rate Parity): F(t) \= S\_0 √ó (1 \+ r\_domestic)^t / (1 \+ r\_foreign)^t, donde S\_0 es spot FX rate, r son tasas libres de riesgo en cada moneda, F(t) es forward rate para delivery en t  
* Implementar modelo simple de riesgo cambiario: asumir que proyecto genera CFADS en USD pero tranche est√° denominado en EUR, entonces flujos en USD deben convertirse a EUR, exposici√≥n a variaciones en FX spot rate introduce volatilidad adicional al valor del tranche  
* M√©todo calculate\_fx\_adjusted\_yield que compute yield del tranche incorporando costo impl√≠cito de FX hedging: si proyecto hace hedge mediante forwards FX, costo de hedge se refleja en diferencial de tasas forward versus spot  
* Implementar an√°lisis de cobertura cambiaria: comparar costo de dejar exposici√≥n sin cubrir versus costo de hedge completo mediante forwards FX o cross-currency swaps, calcular breakeven de volatilidad FX donde hedging se vuelve √≥ptimo  
* M√©todo price\_with\_fx\_hedging que compute precio asumiendo que proyecto implementa hedge: flujos en moneda extranjera se fijan mediante forwards, elimina riesgo de FX pero introduce costo del hedge, precio m√°s alto pero menos vol√°til  
* M√©todo price\_without\_hedging que compute precio asumiendo exposici√≥n sin cubrir: flujos en moneda extranjera convertidos usando expectativa de spot FX rate futuro, agregar prima de riesgo de FX a tasa de descuento, precio m√°s bajo pero mayor incertidumbre  
* Implementar c√°lculo de basis spreads entre monedas: observar spreads de cross-currency basis swaps en mercado, ajustar pricing por desviaciones de paridad de tasas de inter√©s debido a fricciones de mercado, demanda/oferta de funding en diferentes monedas  
* Crear estructura MultiCurrencyPricingResult conteniendo: precio en moneda original del tranche, precio equivalente en USD usando spot FX actual, yield en moneda del tranche, yield equivalente en USD, sensibilidad a movimientos de FX  
* Implementar an√°lisis de optimal currency mix: evaluar si mix de monedas en estructura de deuda puede reducir WACD aprovechando diferenciales de tasas entre pa√≠ses, considerar trade-off entre costo menor y riesgo de FX mayor  
* M√©todo plot\_currency\_risk que genere gr√°fico mostrando impacto de diferentes escenarios de FX: apreciaci√≥n/depreciaci√≥n de 10%, 20%, 30% de moneda extranjera versus USD, c√≥mo var√≠a valor de tranche y distribuciones a equity  
* Documentar limitaciones: modelo asume mercados FX l√≠quidos con acceso a hedging instruments, en pr√°ctica hedging de largo plazo (10 a√±os) puede ser costoso o il√≠quido, basis spreads pueden ser significativos en momentos de stress  
* Incluir caso de uso: analizar si tranche denominado en EUR para atraer inversores europeos reduce WACD suficiente para justificar asumir riesgo de FX o costo de hedging  
* Referencias: literatura de international finance sobre paridad de tasas de inter√©s, papers sobre basis spreads en cross-currency swaps, an√°lisis de corporate hedging strategies

### **T-019: Ajuste LGD colateral**

**¬øEn qu√© consiste?**  
 Refinar el c√°lculo de Loss Given Default (LGD) considerando el valor de colateral espec√≠fico del proyecto (sat√©lites, licencias, contratos) y su recuperabilidad en escenarios de default.

**Relaciones con otras tareas:**

* **Depende de:** T-010 (m√©tricas de riesgo base con LGD), T-005 (modelo Merton usa LGD)  
* **Utilizado por:** T-007 (pricing ajusta por expected loss), T-033 (p√©rdida esperada por tramo)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase CollateralAnalyzer que eval√∫a valor recuperable de activos en escenario de default  
* M√©todo classify\_collateral que categorice activos del proyecto por recuperabilidad: activos f√≠sicos (sat√©lites en √≥rbita, ground stations), activos intangibles (licencias de espectro, contratos con clientes), working capital (cuentas por cobrar, inventario), cash y reservas  
* Implementar valuaci√≥n de sat√©lites en distress: sat√©lites en √≥rbita operativa tienen valor de reventa limitado pues son assets-in-place espec√≠ficos, estimar valor de liquidaci√≥n como porcentaje del costo (t√≠picamente 20-30% dado especificidad), sat√©lites manufacturados pero no lanzados tienen mayor valor de reventa (50-60%)  
* Valuaci√≥n de licencias de espectro: frecuencias radioel√©ctricas tienen valor significativo y son transferibles, estimar valor de mercado de licencias basado en transacciones comparables, t√≠picamente 40-60% del costo original dependiendo de remaining life  
* Valuaci√≥n de contratos con clientes: contratos de largo plazo con dispositivos IoT tienen valor de going concern, estimar como NPV de flujos futuros descontados a tasa apropiada, aplicar haircut por riesgo de churn en transici√≥n, t√≠picamente 30-50% del valor te√≥rico  
* M√©todo calculate\_recovery\_value que sume valores de liquidaci√≥n de todos los activos: Recovery \= Œ£(valor\_liquidacion\_i), comparar contra deuda total outstanding en momento de default, calcular recovery rate \= Recovery / Debt\_outstanding  
* Implementar jerarqu√≠a de prelaci√≥n en liquidaci√≥n: senior secured tiene claim sobre activos espec√≠ficos pignoreados, senior unsecured comparte pro-rata en assets residuales, mezz subordinado solo recupera despu√©s de senior, sub absorbe primeras p√©rdidas  
* Calcular LGD por tramo considerando prelaci√≥n: LGD\_senior \= 1 \- (min(Recovery, Debt\_senior) / Debt\_senior), si recovery completa excede deuda senior entonces LGD\_senior \= 0, remainder disponible para tranches subordinados  
* M√©todo simulate\_default\_scenarios que modele diferentes moments de default: default en a√±o 2 cuando proyecto inmaduro y colateral tiene menos valor, default en a√±o 5 cuando proyecto est√° operando y contratos tienen valor going concern, default en a√±o 8 cerca de maturity  
* Implementar factor de tiempo en valuaci√≥n de colateral: activos se deprecian con el tiempo reduciendo recovery value, sat√©lites tienen degradaci√≥n f√≠sica y tecnol√≥gica, licencias pierden valor cerca de expiraci√≥n, generar curva de recovery value en funci√≥n de time to default  
* M√©todo calculate\_lgd\_with\_priority que implemente waterfall de liquidaci√≥n: realizar activos obteniendo cash proceeds, pagar costos de bankruptcy y liquidaci√≥n (t√≠picamente 5-10% de proceeds), distribuir remainder seg√∫n absolute priority rule empezando por senior secured  
* Implementar an√°lisis de recovery rate scenarios: optimista con recovery de 70% reflejando going concern sale, base case con recovery de 50% mediante piece-meal liquidation, pesimista con recovery de 30% en fire sale durante market downturn  
* Crear estructura CollateralResult conteniendo: valor total de colateral, breakdown por categor√≠a de activo, recovery rate esperado, LGD por tramo considerando prelaci√≥n, sensibilidad de LGD a valor de colateral  
* M√©todo plot\_lgd\_waterfall que visualice c√≥mo recovery value se distribuye entre tranches: gr√°fico de cascada mostrando recovery total en tope, sustracci√≥n de costs, distribuci√≥n a senior hasta satisfacer claim, remainder a mezz, etc.  
* Implementar an√°lisis de covenant de collateral: verificar si valor de colateral se mantiene por encima de threshold m√≠nimo definido en loan agreement, t√≠picamente LTV (Loan to Value) \< 80%, trigger de covenant breach si valor de activos cae por debajo  
* Documentar supuestos: valuaci√≥n de colateral asume que hay compradores dispuestos en momento de default, en pr√°ctica liquidation puede tomar tiempo extendido con costos de carrying, mercados de activos espec√≠ficos pueden ser il√≠quidos  
* Incluir benchmarking: comparar recovery rates calculados contra estad√≠sticas hist√≥ricas de defaults en Project Finance, seg√∫n Moody's recovery medio en infraestructura es 60-70% para senior secured, 30-40% para mezz  
* Referencias: literatura sobre bankruptcy y liquidation values, papers sobre asset specificity y recovery rates, gu√≠as de Banco Mundial sobre valuation de colateral en Project Finance

## **WP-05: RIESGO CREDITICIO**

### **T-010: M√≥dulo EL/VaR/CVaR**

**¬øEn qu√© consiste?**  
 Implementar el m√≥dulo central de m√©tricas de riesgo crediticio calculando Expected Loss (EL), Value at Risk (VaR) y Conditional Value at Risk (CVaR) para cada tramo de deuda.

**Relaciones con otras tareas:**

* **Depende de:** T-006 (waterfall con eventos de default), T-005 (Merton para PD/LGD)  
* **Utilizado por:** T-033 (p√©rdida esperada agregada), T-034 (m√©tricas de cola), T-038 (stress testing)  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase RiskMetricsCalculator que centraliza c√°lculo de todas las m√©tricas de riesgo de cr√©dito  
* M√©todo calculate\_expected\_loss que compute p√©rdida esperada por tramo: EL\_tramo \= PD √ó LGD √ó EAD, donde PD es probabilidad de default de T-005, LGD es loss given default de T-019, EAD (Exposure At Default) es saldo outstanding esperado en momento de default  
* Implementar c√°lculo de EAD term structure: EAD var√≠a en el tiempo conforme deuda se amortiza, construir curva de EAD\_t \= saldo\_outstanding\_t para cada per√≠odo futuro, calcular EL\_t \= PD\_t √ó LGD\_t √ó EAD\_t para cada horizonte, sumar ELs descontados para obtener p√©rdida esperada total  
* M√©todo calculate\_var que compute Value at Risk al nivel de confianza especificado: ordenar distribuci√≥n de p√©rdidas de menor a mayor (del Monte Carlo o modelo Merton), encontrar percentil correspondiente a nivel de confianza, VaR(Œ±) \= p√©rdida en percentil (1-Œ±), ej. VaR(95%) es p√©rdida que se excede solo en 5% peor de escenarios  
* Implementar VaR para diferentes horizontes temporales: VaR a 1 a√±o t√≠pico para gesti√≥n de riesgo anual, VaR a maturity para evaluar riesgo acumulado durante vida completa del proyecto, escalar VaR usando regla de ra√≠z cuadrada del tiempo si se asume i.i.d.  
* M√©todo calculate\_cvar que compute Conditional Value at Risk (Expected Shortfall): CVaR(Œ±) \= E\[P√©rdida | P√©rdida \> VaR(Œ±)\], promedio de p√©rdidas en el Œ±% peor de escenarios, m√°s conservador que VaR pues captura severidad de tail events  
* Implementar diferencia entre VaR y CVaR: VaR solo indica threshold de p√©rdida m√°xima al nivel de confianza pero no informa magnitud de p√©rdidas peores, CVaR captura tail risk completo, √∫til para escenarios de crisis donde p√©rdidas extremas importan  
* M√©todo calculate\_marginal\_risk que compute contribuci√≥n de cada tramo al riesgo total de portafolio: marginal EL de tramo i \= ‚àÇEL\_total/‚àÇweight\_i, identifica qu√© tranches aportan m√°s riesgo y podr√≠an reducirse para mejorar perfil  
* Implementar c√°lculo de component VaR: descomponer VaR total en contribuciones de cada tramo considerando correlaciones, suma de component VaRs \= VaR total por propiedades de coherencia de CVaR  
* M√©todo calculate\_diversification\_benefit que cuantifique beneficio de diversificaci√≥n en estructura con m√∫ltiples tranches: compare riesgo de portafolio agregado versus suma de riesgos standalone de cada tramo, benefit \= Œ£(Risk\_i) \- Risk\_portfolio, positivo si hay correlaci√≥n imperfecta  
* Crear estructura RiskMetricsResult conteniendo: EL por tramo y total, VaR al 95% y 99%, CVaR al 95% y 99%, component risk por tramo, diversification benefit  
* Implementar stress testing de m√©tricas de riesgo: c√≥mo var√≠a EL si PD se duplica, c√≥mo var√≠a VaR si volatilidad aumenta 50%, generar tornado chart mostrando sensibilidades de m√©tricas de riesgo a inputs clave  
* M√©todo calculate\_risk\_adjusted\_returns que compute retornos ajustados por riesgo: RAROC (Risk Adjusted Return on Capital) \= (Expected Return \- EL) / Economic Capital, donde Economic Capital \= VaR o CVaR al nivel de confianza deseado  
* Implementar c√°lculo de capital econ√≥mico regulatorio: estimar capital que instituci√≥n financiera deber√≠a reservar para cubrir p√©rdidas inesperadas seg√∫n Basilea III, Regulatory Capital \= K √ó EAD donde K es factor derivado de PD y LGD usando IRB approach  
* M√©todo plot\_loss\_distribution que grafique distribuci√≥n completa de p√©rdidas: histograma de p√©rdidas simuladas, marcar EL con l√≠nea vertical, marcar VaR y CVaR con l√≠neas de diferente color, sombrear tail region m√°s all√° de VaR  
* Generar reporte de riesgo por tramo: tabla con m√©tricas key de cada tranche (EL, VaR, CVaR, RAROC), ranking de tranches por riskiness, identificar red flags donde m√©tricas exceden thresholds  
* Documentar interpretaci√≥n de m√©tricas: EL es costo esperado del riesgo que debe incorporarse en pricing, VaR/CVaR son medidas de worst-case √∫tiles para stress testing y capital allocation, CVaR preferido por reguladores por ser coherent risk measure  
* Incluir benchmarking: comparar m√©tricas calculadas contra est√°ndares de industria, seg√∫n estudios de Project Finance EL t√≠pico para senior es 0.5-1%, para mezz 2-3%, para sub 5-7%  
* Referencias: papers sobre credit risk modeling de Altman, Saunders et al, documentos de Basilea III sobre regulatory capital, literatura sobre coherent risk measures de Artzner et al

### **T-033: C√°lculo p√©rdida esperada**

**¬øEn qu√© consiste?**  
 Calcular la p√©rdida esperada agregada del portafolio de deuda considerando la estructura completa de tranches y sus interdependencias, √∫til para an√°lisis de diversificaci√≥n y allocation de capital.

**Relaciones con otras tareas:**

* **Depende de:** T-030 (probabilidades de breach de Monte Carlo), T-010 (EL por tramo base)  
* **Utilizado por:** T-034 (m√©tricas de cola), T-041 (resultados de stress con p√©rdidas)  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase AggregateRiskCalculator que compute riesgo crediticio a nivel de portafolio completo  
* M√©todo calculate\_portfolio\_el que agregue p√©rdidas esperadas de todos los tranches: EL\_portfolio \= Œ£(EL\_i) donde i indexa tranches, pero considerando interdependencias pues default de proyecto afecta a todos los tranches simult√°neamente  
* Implementar modelo de default conjunto: reconocer que si proyecto entra en default todos los tranches se ven impactados aunque con diferentes LGDs seg√∫n prelaci√≥n, modelar default como evento com√∫n que activa p√©rdidas correlacionadas  
* M√©todo calculate\_joint\_pd que estime probabilidad de que m√∫ltiples tranches experimenten p√©rdidas simult√°neamente: usar copulas para modelar dependencia entre eventos de default, si default del proyecto es √∫nico evento entonces PD\_joint \= PD\_proyecto para todos los tranches  
* Implementar waterfall de p√©rdidas en escenario de default: dado recovery value de T-019, distribuir p√©rdidas empezando por tranche m√°s subordinado: sub absorbe primeras p√©rdidas hasta su nominal, excess loss va a mezz, solo si p√©rdidas exceden mezz+sub entonces impacta senior  
* M√©todo calculate\_loss\_allocation que determine cu√°nto pierde cada tranche en escenario de default: Loss\_sub \= min(Total\_Loss, Principal\_sub), Loss\_mezz \= min(max(0, Total\_Loss \- Principal\_sub), Principal\_mezz), Loss\_senior \= max(0, Total\_Loss \- Principal\_sub \- Principal\_mezz)  
* Crear distribuci√≥n emp√≠rica de p√©rdidas por tramo usando simulaciones Monte Carlo: para cada trayectoria de MC, determinar si hay default, calcular p√©rdida total en ese escenario, aplicar waterfall de p√©rdidas, obtener p√©rdida espec√≠fica por tramo, repetir para todas las trayectorias  
* M√©todo calculate\_correlation\_of\_losses que estime correlaci√≥n entre p√©rdidas de diferentes tranches: en pr√°ctica senior y sub tienen correlaci√≥n alta pues ambos dependen de default del proyecto, calcular matriz de correlaci√≥n entre p√©rdidas de tranches  
* Implementar an√°lisis de tranching efficiency: evaluar si estructura de tranches efectivamente diversifica riesgo entre inversores con diferentes apetitos, calcular ratio de p√©rdida esperada senior/total versus ratio de principal senior/total, si ratio de p√©rdida es menor entonces tranching es efectivo  
* M√©todo calculate\_diversification\_effect que cuantifique beneficio de tener m√∫ltiples tranches: compare EL de portafolio agregado versus EL de estructura sin tranching (toda deuda es pari passu), positivo si tranching reduce EL total mediante mejor risk allocation  
* Crear estructura AggregateRiskResult conteniendo: EL total del portafolio, breakdown de EL por tramo, correlaci√≥n de p√©rdidas entre tranches, m√©tricas de efficiency del tranching  
* Implementar an√°lisis de seniority impact: c√≥mo cambia EL de senior si se aumenta proportion de sub que act√∫a como cushion, encontrar optimal tranching que minimiza EL de senior subject a constraint de WACD total  
* M√©todo plot\_loss\_waterfall que visualice distribuci√≥n de p√©rdidas por tranche en escenario de default: stacked bar chart mostrando principal de cada tramo, overlay de p√©rdidas esperadas, highlighting que sub absorbe mayor√≠a de EL  
* Generar tabla de contribuci√≥n al riesgo: mostrar para cada tranche su contribution a portfolio EL como porcentaje, identificar si hay concentraci√≥n de riesgo en alg√∫n tramo o si est√° bien diversificado  
* Documentar implicaciones para pricing: tranches con mayor EL deben tener spreads mayores para compensar risk, verificar que spreads calibrados son consistentes con ELs calculados, se√±alar incoherencias  
* Incluir an√°lisis de capital allocation: c√≥mo deber√≠a banco o inversor institucional allocar capital econ√≥mico entre tranches, usar component risk de T-010 para determinar allocation proporcional  
* Referencias: literatura sobre structured finance y tranching como papers de Gorton & Pennacchi sobre asset securitization, an√°lisis de CDOs (Collateralized Debt Obligations) que usan tranching similar

### **T-034: M√©tricas de cola VaR/CVaR**

**¬øEn qu√© consiste?**  
 Refinar el c√°lculo de m√©tricas de riesgo de cola (VaR y CVaR) usando distribuci√≥n emp√≠rica de Monte Carlo, evaluar tail risk para diferentes niveles de confianza y horizones temporales.

**Relaciones con otras tareas:**

* **Depende de:** T-033 (p√©rdidas agregadas), T-021 (Monte Carlo genera distribuci√≥n)  
* **Utilizado por:** T-035 (frontera eficiente), T-041 (resultados de stress)  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase TailRiskAnalyzer que especializa en an√°lisis de riesgo de cola extrema  
* M√©todo calculate\_empirical\_var usando distribuci√≥n completa de Monte Carlo: ordenar p√©rdidas de N trayectorias, encontrar √≠ndice correspondiente a percentil (1-Œ±), VaR\_emp√≠rico(Œ±) \= p√©rdida en ese √≠ndice, ej. para Œ±=95% y N=10,000 usar p√©rdida ranked 9,500  
* Implementar c√°lculo de VaR para m√∫ltiples niveles de confianza: calcular VaR(90%), VaR(95%), VaR(99%), VaR(99.9%) para capturar tail risk creciente, mostrar c√≥mo p√©rdida m√°xima aumenta con nivel de confianza  
* M√©todo calculate\_empirical\_cvar usando tails emp√≠ricos: seleccionar todas las trayectorias donde p√©rdida \> VaR(Œ±), promediar esas p√©rdidas para obtener CVaR(Œ±), CVaR siempre ‚â• VaR por construcci√≥n  
* Implementar c√°lculo de Expected Shortfall completo: ES(Œ±) \= CVaR(Œ±) √ó Œ± \+ VaR(Œ±) √ó (1-Œ±), representa p√©rdida esperada total considerando toda la distribuci√≥n ponderada por probabilidad  
* M√©todo calculate\_tail\_risk\_by\_horizon que compute VaR y CVaR para diferentes horizontes: VaR(1Y), VaR(3Y), VaR(5Y), VaR(maturity), mostrar c√≥mo tail risk se acumula con tiempo, longer horizons tienen mayor VaR por cumulative probability de default  
* Implementar an√°lisis de tail dependence: evaluar si p√©rdidas extremas de diferentes tranches est√°n correlacionadas, calcular coeficiente de tail dependence entre senior y sub, alto tail dependence indica que en crisis todos sufren simult√°neamente  
* M√©todo fit\_extreme\_value\_distribution a cola de distribuci√≥n emp√≠rica: usar Generalized Pareto Distribution (GPD) o Generalized Extreme Value (GEV) para modelar tail behavior m√°s all√° de datos observados, extrapolar VaR para niveles de confianza muy altos como 99.99%  
* Implementar estimaci√≥n de par√°metros EVD mediante Maximum Likelihood: ajustar shape parameter Œæ y scale parameter œÉ de GPD a excesos sobre threshold, usar para estimar VaR de eventos s√∫per raros  
* M√©todo calculate\_var\_backtesting que eval√∫e calidad de estimaciones de VaR: si VaR(95%) es correcto entonces violaciones (p√©rdidas \> VaR) deben ocurrir en \~5% de casos, test de Kupiec para verificar cobertura, test de Christoffersen para verificar independencia de violaciones  
* Crear estructura TailRiskResult conteniendo: VaR y CVaR para m√∫ltiples niveles de confianza, tail dependence entre tranches, par√°metros de EVD ajustados, resultados de backtesting  
* Implementar stress testing de tail risk: c√≥mo var√≠a VaR si volatilidad de inputs aumenta 50%, c√≥mo var√≠a si hay m√°s fat tails en distribuci√≥n (mayor kurtosis), sensitivity analysis de m√©tricas de cola  
* M√©todo plot\_tail\_comparison que visualice tails de distribuciones de p√©rdidas para diferentes tranches: overlay de histogramas en regi√≥n de cola, mostrar que sub tiene tail m√°s gordo que mezz que a su vez es m√°s gordo que senior  
* Generar gr√°fico de Quantile-Quantile (QQ plot) comparando tail emp√≠rico versus distribuci√≥n normal: verificar si tail es fat tailed (desviaci√≥n positiva de normal en extremos), justificar uso de EVD si hay evidencia de fat tails  
* Documentar diferencias entre VaR y CVaR: VaR puede ser non-coherent risk measure (no es subadditive), CVaR es coherent y preferido por Basel III, explicar por qu√© CVaR es m√°s apropiado para portfolios  
* Implementar an√°lisis de tail scenarios: identificar qu√© combinaci√≥n de shocks produce p√©rdidas en el tail, ej. p√©rdidas extremas resultan de simult√°nea ca√≠da de demanda \+ incremento de OPEX \+ fallas t√©cnicas, √∫til para dise√±ar stress scenarios de T-038  
* Incluir benchmarking de tail risk: comparar VaR/CVaR calculados contra niveles t√≠picos en Project Finance, seg√∫n est√°ndares internos de bancos target es CVaR(99%) \< 20% de exposici√≥n  
* Referencias: literatura sobre extreme value theory de Embrechts et al "Modelling Extremal Events", papers sobre coherent risk measures, documentos de Basel III sobre market risk capital y use of Expected Shortfall

### **T-035: Frontera eficiente**

**¬øEn qu√© consiste?**  
 Analizar trade-off riesgo-retorno de diferentes configuraciones de estructura de deuda construyendo frontera eficiente en espacio (riesgo, rendimiento).

**Relaciones con otras tareas:**

* **Depende de:** T-034 (m√©tricas de riesgo completadas), T-028 (spreads calibrados)  
* **Relacionado con:** T-036 (optimizaci√≥n que busca punto √≥ptimo)  
* **Duraci√≥n:** 2 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase EfficientFrontierAnalysis en archivo pftoken/analysis/efficient\_frontier.py  
* M√©todo generate\_portfolio\_configurations que genere conjunto de estructuras de deuda con diferentes mezclas de tranches: variar peso\_senior de 40% a 80% en incrementos de 5%, variar peso\_mezz y peso\_sub proporcionalmente manteniendo suma \= 100%, generar grid de \~50-100 configuraciones posibles  
* Para cada configuraci√≥n calcular: expected\_return \= WACD (costo para equity holders es retorno para debtholders), risk \= CVaR\_95 del portfolio como medida de downside risk, alternativamente usar volatilidad de CFADS o EL como medida de riesgo  
* M√©todo calculate\_sharpe\_ratio que compute (Return \- Rf) / Risk para cada configuraci√≥n: configuraciones con Sharpe ratio alto son superiores, identificar configuraci√≥n con m√°ximo Sharpe ratio \= tangency portfolio  
* M√©todo plot\_efficient\_frontier que grafique en espacio (Risk, Return): eje X \= CVaR o volatilidad, eje Y \= WACD, plotear cada configuraci√≥n como punto, conectar puntos en boundary superior formando frontera eficiente, configuraciones por debajo de frontera son dominadas  
* Identificar configuraciones notables: minimum variance portfolio (menor riesgo), maximum Sharpe portfolio (mejor risk-adjusted return), current structure (baseline para comparaci√≥n)  
* Implementar an√°lisis de dominancia: estructura A domina B si tiene menor riesgo y mayor return, marcar puntos dominados en gris, resaltar frontera eficiente en color  
* M√©todo find\_optimal\_allocation que dado preferencia de risk aversion del inversor, encuentre punto √≥ptimo en frontera: maximizar Utility \= Return \- Œª √ó Risk donde Œª es coeficiente de risk aversion, Œª alto implica preferencia por bajo riesgo, Œª bajo implica agresividad  
* Implementar an√°lisis de corner portfolios: identificar puntos donde composici√≥n de frontera cambia cualitativamente, √∫til para entender transition de strategies conservativas a agresivas  
* M√©todo compare\_structures\_on\_frontier que plotee estructura Traditional y Tokenized en mismo gr√°fico de frontera: evaluar si tokenizaci√≥n mueve estructura hacia frontera o mejora su posici√≥n, cuantificar improvement en Sharpe ratio o reduction en CVaR para mismo return  
* Implementar constraints en optimizaci√≥n: peso m√≠nimo de senior \= 40% por requerimientos de lenders, peso m√°ximo de sub \= 25% por appetite de market, WACD m√°ximo aceptable \= 10% por viabilidad de proyecto, incorporar constraints en b√∫squeda de frontera  
* Crear sensitivity analysis: c√≥mo se mueve frontera si aumenta volatilidad de CFADS, si cambian correlaciones entre tranches, si se modifican recovery rates, mostrar robustez de recomendaciones  
* Documentar interpretaci√≥n: explicar que puntos en frontera son Pareto-efficient (no se puede mejorar return sin aumentar risk), decisi√≥n final depende de risk appetite del equity sponsor, presentar range de opciones viables  
* Visualizaci√≥n interactiva: si es posible, crear plot interactivo donde al hacer hover sobre punto muestre composici√≥n exacta de tranches y m√©tricas de riesgo/retorno

---

### **T-037: √çndice Herfindahl**

**¬øEn qu√© consiste?**  
 Calcular el √çndice de Herfindahl-Hirschman (HHI) de concentraci√≥n de riesgo entre tranches para evaluar si estructura diversifica riesgo efectivamente o lo concentra.

**Relaciones con otras tareas:**

* **Depende de:** T-034 (distribuci√≥n de riesgo por tramo conocida)  
* **Utilizado en:** T-016 (comparaci√≥n de estructuras), T-048 (an√°lisis final)  
* **Duraci√≥n:** 2 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase RiskConcentrationAnalysis en archivo pftoken/analysis/concentration.py  
* M√©todo calculate\_herfindahl\_index que compute HHI de concentraci√≥n de riesgo: HHI \= Œ£\[w\_i¬≤\] donde w\_i es fracci√≥n del riesgo total asumida por tranche i, calcular w\_i \= EL\_i / EL\_total usando Expected Loss como medida de riesgo  
* Interpretar HHI: valor entre 0 y 1, HHI cercano a 1 indica alta concentraci√≥n (un tranche asume casi todo el riesgo), HHI cercano a 1/N indica diversificaci√≥n perfecta entre N tranches, t√≠picamente HHI \= 0.4-0.6 es realista en structured finance  
* Ejemplo: si EL\_senior \= $3M, EL\_mezz \= $5M, EL\_sub \= $2M entonces w\_senior \= 3/10, w\_mezz \= 5/10, w\_sub \= 2/10, HHI \= (0.3)¬≤ \+ (0.5)¬≤ \+ (0.2)¬≤ \= 0.09 \+ 0.25 \+ 0.04 \= 0.38  
* M√©todo calculate\_participation\_by\_tranche que compute porcentaje de riesgo total por tranche: tabla mostrando para cada tranche su share of total risk, √∫til para identificar tranches que concentran riesgo  
* Implementar comparaci√≥n de HHI entre estructuras: HHI\_traditional t√≠picamente m√°s alto (2 tranches \= senior/mezz concentran riesgo), HHI\_tokenized m√°s bajo (3 tranches \= senior/mezz/sub diversifican), cuantificar reduction en HHI como beneficio de tokenizaci√≥n  
* M√©todo calculate\_equivalent\_n que compute n√∫mero equivalente de tranches homog√©neos: N\_equiv \= 1 / HHI, si HHI \= 0.38 entonces N\_equiv \= 2.6 significa que nivel de diversificaci√≥n equivale a 2.6 tranches id√©nticos, √∫til para interpretaci√≥n intuitiva  
* Implementar an√°lisis din√°mico de HHI: calcular HHI en cada per√≠odo del proyecto, t√≠picamente HHI decrease durante ramping a medida que proyecto se estabiliza, puede increase cerca de maturity cuando senior ya amortiz√≥ y solo quedan tranches junior  
* M√©todo decompose\_concentration\_sources que identifique por qu√© hay concentraci√≥n: es por diferencia en tama√±os de tranches o por diferencia en riesgo per dollar, separar efecto de allocation (pesos) vs efecto de risk intensity (p√©rdidas por peso)  
* Implementar threshold analysis: si HHI \> 0.6 emitir warning de excessive concentration, si HHI \> 0.8 emitir alert de dangerous concentration, recomendar restructuring para reducir concentraci√≥n  
* Crear visualizaci√≥n de concentraci√≥n: pie chart mostrando share de cada tranche en riesgo total, bar chart comparando HHI\_traditional vs HHI\_tokenized, time series de evoluci√≥n de HHI durante vida de proyecto  
* Implementar an√°lisis de contagion risk: alta concentraci√≥n implica que default de un tranche grande puede desestabilizar estructura completa, baja concentraci√≥n distribuye p√©rdidas, cuantificar systemic risk usando network analysis  
* M√©todo calculate\_systemic\_importance que identifique tranches que son systemically important: tranche es systemic si su default causa cascade defaults en otros tranches, medir mediante correlation de defaults y share of total debt  
* Documentar interpretaci√≥n de HHI: explicar que bajo HHI es deseable desde perspectiva de diversificaci√≥n pero puede complicar governance, relacionar con literatura de systemic risk en structured finance  
* Referencias: usar literatura de securitization donde HHI es m√©trica est√°ndar de calidad de tranching, comparar con HHI observados en CDOs, CLOs, MBS

---

## **WP-06: STRESS TESTING (AMPLIADO)**

### **T-038: Dise√±o escenarios estr√©s**

**¬øEn qu√© consiste?**  
 Dise√±ar conjunto comprehensivo de escenarios de estr√©s que representen eventos adversos plausibles para el proyecto de constelaci√≥n LEO IoT, que ser√°n usados para stress testing.

**Relaciones con otras tareas:**

* **Depende de:** T-002 (requerimientos definen riesgos clave)  
* **Utilizado por:** T-040 (motor de estr√©s ejecuta estos escenarios), T-041 (resultados de estr√©s)  
* **Se extiende con:** T-038B (escenario S6 CAPEX Failure), T-039 (escenarios combinados)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase StressScenarioLibrary en archivo pftoken/stress/scenarios.py que centraliza definici√≥n de escenarios  
* Clase base StressScenario con atributos: name (identificador √∫nico), description (narrativa del escenario), shock\_parameters (dict con ajustes a par√°metros), severity (mild/moderate/severe), probability (likelihood estimado), rationale (justificaci√≥n acad√©mica)  
* **Escenario S1 \- Demand Shock:** contracci√≥n severa en demanda de IoT por recesi√≥n econ√≥mica global, shocks: revenue\_growth \= \-20% respecto a base, churn\_rate \= \+50% (m√°s clientes cancelan servicio), ARPU \= \-15% por presi√≥n competitiva, duraci√≥n \= 2 a√±os antes de recovery gradual, rationale: crisis como COVID-19 redujo inversi√≥n corporativa en IoT  
* **Escenario S2 \- Interest Rate Shock:** aumento abrupto de tasas de inter√©s por pol√≠tica monetaria restrictiva de Fed, shocks: base\_rate \= \+200bps (de 5% a 7%), spread\_mezz \= \+100bps adicionales, spread\_sub \= \+150bps adicionales por flight to quality, aplicar instant√°neamente en a√±o 2, duraci√≥n permanente (tasas no revierten), rationale: ciclo de normalizaci√≥n post-QE puede elevar tasas estructuralmente  
* **Escenario S3 \- Launch Failure:** falla catastr√≥fica en lanzamiento destruyendo fracci√≥n de sat√©lites, shocks: capacidad\_inicial \= \-15% (p√©rdida de sat√©lites), CAPEX\_recovery \= \+$10M para lanzamiento de reemplazo, delay \= 12 meses en inicio de operaciones comerciales, ingresos \= 0 durante delay, rationale: hist√≥rico de fallas de lanzamiento en 3-5% de misiones  
* **Escenario S4 \- Operational Degradation:** degradaci√≥n acelerada de sat√©lites reduciendo vida √∫til, shocks: lifetime \= \-2 a√±os (de 7 a 5 a√±os), RCAPEX adelantado \= comenzar reemplazo en a√±o 4 en lugar de a√±o 6, OPEX \= \+20% por mantenimiento correctivo incrementado, rationale: ambiente espacial m√°s hostil que esperado (radiaci√≥n, debris)  
* **Escenario S5 \- Regulatory Change:** cambio regulatorio aumentando costos de compliance, shocks: regulatory\_fees \= \+$3M anuales por nuevas licencias, insurance\_rate \= \+50% por nuevos requerimientos, delay\_approval \= 6 meses en obtener permisos para expansi√≥n, rationale: tendencia hacia mayor regulaci√≥n de megaconstelaciones por Space Sustainability  
* M√©todo define\_shock\_profile que especifique timing y magnitude de shocks: algunos shocks son instant√°neos (interest rate jump), otros son graduales (demand decline over 2 years), algunos son permanentes (new baseline), otros son transitorios (recovery after shock)  
* Implementar mecanismo de recovery paths: despu√©s de shock, par√°metros pueden revertir parcialmente a baseline (V-shaped recovery), permanecer en nivel deprimido (L-shaped), o tener bounce-back seguido de nueva normalidad (U-shaped)  
* M√©todo combine\_scenarios que permita combinar m√∫ltiples shocks simult√°neos para escenarios m√°s severos: S1+S2 \= demanda cae Y tasas suben simult√°neamente (double whammy), calibrar correlaci√≥n entre eventos (demand shocks y rate hikes a menudo ocurren juntos en crisis)  
* Crear library de escenarios parametrizados: poder generar variantes como S1\_mild (-10% demand), S1\_moderate (-20% demand), S1\_severe (-30% demand) para an√°lisis de sensibilidad  
* Documentar precedentes hist√≥ricos: para cada escenario citar eventos reales del pasado donde ocurrieron shocks similares, ej: crisis satelital de Iridium en 2000, crisis financiera 2008 para rate shock, SpaceX failures para launch risks  
* Implementar stress test de inverso: reverse stress testing identifica qu√© combinaci√≥n de shocks causar√≠a default inevitable, √∫til para identificar vulnerabilidades extremas  
* Calibrar probabilidades subjetivas: asignar probabilidad anual a cada escenario basado en an√°lisis hist√≥rico y juicio experto, ej: launch failure \= 5% anual, severe demand shock \= 10% en 5 a√±os, rate shock 200bps \= 20% en 10 a√±os

---

### **T-038B: Escenario S6 CAPEX Failure**

**¬øEn qu√© consiste?**  
 Agregar escenario de estr√©s espec√≠fico modelando falla cr√≠tica de CAPEX donde MRA es insuficiente para cubrir RCAPEX necesario, forzando refinanciamiento de emergencia o degradaci√≥n acelerada.

**Relaciones con otras tareas:**

* **Depende de:** T-040 (motor de estr√©s debe estar funcional)  
* **Extiende:** T-038 (librer√≠a de escenarios)  
* **Duraci√≥n:** 2 d√≠as seg√∫n Gantt (tarea NUEVA en versi√≥n actualizada)

**Qu√© se debe implementar:**

* Crear StressScenario S6\_CAPEX\_Failure con narrativa: "Reemplazo masivo inesperado de sat√©lites por falla sist√©mica de componente cr√≠tico requiere RCAPEX de emergencia que excede MRA disponible"  
* Especificar shocks: RCAPEX\_unexpected \= $25M en a√±o 5 (vs $10M planeado), MRA\_available \= solo $12M acumulado (shortfall de $13M), consecuencias: degradaci√≥n de servicio si no se reemplaza, potential default t√©cnico si se viola covenants por usar CFADS para CAPEX en lugar de deuda service  
* Implementar l√≥gica de decisi√≥n en waterfall: si RCAPEX\_required \> MRA\_available entonces opciones: A) diferir CAPEX y aceptar degradaci√≥n de servicio reduciendo capacidad y afectando revenues futuro, B) desviar CFADS prioritariamente a CAPEX violando waterfall y causando payment default, C) buscar refinanciamiento de emergencia con t√©rminos punitivos  
* Modelar opci√≥n A \- diferir CAPEX: si diferimos reemplazo, capacidad operativa decline \= \-10% por a√±o sin satelites reemplazados, revenues afectados proporcionalmente, espiral negativa donde menores revenues hacen m√°s dif√≠cil acumular MRA futuro  
* Modelar opci√≥n B \- violaci√≥n de waterfall: desviar CFADS a CAPEX emergency \= immediate default t√©cnico en pagos de deuda, activar cross-default clauses, potencial aceleraci√≥n de toda la deuda, lenders toman control del proyecto  
* Modelar opci√≥n C \- refinanciamiento de emergencia: conseguir bridge loan de $13M con t√©rminos caros: rate \= 15% (vs 6-12% de estructura base), upfront fee \= 3%, tenor \= 3 a√±os, agregar este nuevo tramo al waterfall con super-senior priority, incremento permanente de WACD  
* Implementar an√°lisis de trade-offs: comparar NPV de equity bajo cada opci√≥n, opci√≥n A puede ser mejor si degradaci√≥n es temporal y recuperable, opci√≥n C mejor si permite mantener operaci√≥n pero erode returns, opci√≥n B lleva a liquidation  
* Calcular probability of MRA insufficiency: desde simulaciones Monte Carlo, contar escenarios donde RCAPEX realizado \> MRA acumulado, estimar probabilidad \= \~5-10% en proyectos t√≠picos, cuantificar expected shortfall conditional on insuficiencia  
* Implementar estrategias de mitigaci√≥n preventivas: aumentar target de MRA de 50% a 75% de RCAPEX esperado, hacer fondeo acelerado de MRA en a√±os buenos, contratar insurance para CAPEX overruns (aunque caro)  
* Crear visualizaci√≥n de trayectorias: plot mostrando evoluci√≥n de MRA saldo vs RCAPEX requerido en diferentes escenarios, identificar momentos de crisis donde shortfall ocurre, mostrar impact en DSCR  
* Documentar justificaci√≥n del escenario: citar caso de OneWeb bankruptcy parcialmente atribuido a underestimation de CAPEX requirements, argumentar que es riesgo material en proyectos satelitales con tecnolog√≠a unproven

---

### **T-039: Escenarios combinados**

**¬øEn qu√© consiste?**  
 Dise√±ar escenarios de estr√©s combinados donde m√∫ltiples shocks ocurren simult√°neamente o en secuencia, representando crisis sist√©micas m√°s realistas que shocks aislados.

**Relaciones con otras tareas:**

* **Depende de:** T-038 (escenarios individuales definidos)  
* **Utilizado por:** T-040 (motor ejecuta combinaciones), T-042 (reverse stress testing)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Extender StressScenarioLibrary con factory method create\_combined\_scenario  
* **Combo C1 \- Perfect Storm:** combinar S1 Demand \+ S2 Interest Rate \+ S4 Degradation simult√°neamente, narrativa: "Crisis econ√≥mica global deprime demanda de IoT justo cuando Fed sube tasas y problemas t√©cnicos aumentan OPEX", efecto compounding: menores revenues dificultan pagar mayores intereses mientras costos aumentan, triple presi√≥n sobre CFADS  
* **Combo C2 \- Launch Failure \+ Refinancing Crisis:** combinar S3 Launch Failure (a√±o 1\) seguido por S2 Rate Shock (a√±o 3), narrativa: "Falla de lanzamiento retrasa proyecto y depleta capital, justo cuando proyecto busca refinanciamiento las tasas han subido dram√°ticamente", timing adverso maximiza impacto: proyecto debilitado enfrenta mercado hostil  
* **Combo C3 \- Operational Cascade:** combinar S4 Degradation \+ S6 CAPEX Failure \+ S5 Regulatory, narrativa: "Degradaci√≥n acelerada requiere RCAPEX imprevisto en momento donde nuevas regulaciones aumentan costos, MRA insuficiente", efecto domino: un problema operativo trigger crisis financiera  
* Implementar l√≥gica de correlaci√≥n temporal: modelar que algunos shocks aumentan probabilidad de otros, ej: demand shock puede llevar a que proyecto corte mantenimiento preventivo causando operational degradation posterior, rate shock puede ocurrir como respuesta a demand shock (Fed tightening)  
* M√©todo sample\_correlated\_shocks que genere pares de shocks con correlaci√≥n realista: usar copulas para modelar dependencia en tails, demand shocks y rate shocks tienen correlaci√≥n negativa de \-0.3, operational problems y capex overruns tienen correlaci√≥n positiva de 0.6  
* Implementar escalamiento temporal de crisis: combo scenario comienza con shock moderate que deteriora m√©tricas, luego second shock hit when project es vulnerable, efecto amplificado vs suma de shocks individuales  
* M√©todo calculate\_compounding\_factor que mida sinergias negativas: si shock A causa p√©rdida de $5M y shock B causa $7M individualmente, combo A+B puede causar $15M por efectos no-lineales, compounding \= (Loss\_combo \- Loss\_A \- Loss\_B) / (Loss\_A \+ Loss\_B)  
* Crear matriz de interacciones: tabla mostrando para cada par de escenarios cu√°l es effect size de combinaci√≥n vs suma de efectos individuales, identificar pares con mayor synergy negativa  
* Implementar historical precedent mapping: identificar crisis reales donde ocurrieron combos similares, ej: Iridium 1999-2000 tuvo tech problems \+ demand underestimation \+ refinancing crisis simult√°neamente  
* M√©todo generate\_stress\_narrative que cree storyline convincente explicando c√≥mo se desarrolla crisis combinada: inicio con evento trigger, propagaci√≥n a trav√©s de sistema, puntos de inflexi√≥n, posibles recovery paths  
* Calibrar severity levels de combos: C1 Perfect Storm es severity \= extreme (\> 3 sigma event), C2 es severe (2-3 sigma), C3 es moderate (1-2 sigma), asignar probabilidades conjuntas usando joint probability  
* Implementar an√°lisis de vulnerabilidad period-specific: identificar ventanas temporales donde proyecto es m√°s vulnerable a combos, t√≠picamente a√±os 2-4 durante ramping cuando DSCR a√∫n bajo y reservas no completamente fondadas  
* Crear visualizaci√≥n de combo scenarios: timeline diagram mostrando timing de cada shock component, c√≥mo se propagan efectos, peaks de stress en DSCR, momento de potential default  
* Documentar lessons para design de estructura: qu√© covenants o buffers ser√≠an necesarios para sobrevivir combos severos, c√≥mo mejorar resilience mediante diversificaci√≥n de revenue streams o hedging instruments

---

### **T-040: Motor de estr√©s**

**¬øEn qu√© consiste?**  
 Implementar engine que ejecute escenarios de estr√©s aplicando shocks a par√°metros del modelo, re-ejecutando CFADS \+ Waterfall \+ Pricing, y comparando resultados vs caso base.

**Relaciones con otras tareas:**

* **Cr√≠tico: Depende de:** T-031 (pipeline Monte Carlo), T-038 (escenarios definidos)  
* **Genera inputs para:** T-041 (resultados), T-042 (reverse testing), T-043 (stress h√≠bridas)  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase StressTestEngine en archivo pftoken/stress/stress\_engine.py  
* M√©todo apply\_stress\_scenario que reciba StressScenario y ProjectParams base, aplique shocks especificados creando ProjectParams\_stressed, shocks pueden ser multiplicative (√ó1.2), additive (+$5M), o replacement (new value)  
* Implementar l√≥gica de aplicaci√≥n temporal de shocks: algunos shocks aplican desde cierto per√≠odo en adelante, otros tienen profile temporal espec√≠fico, mantener timeline de cuando cada shock se activa y desactiva  
* M√©todo run\_stressed\_simulation que ejecute pipeline completo con par√°metros estresados: calcular CFADS\_stressed usando T-003, ejecutar Waterfall\_stressed usando T-006, calcular ratios\_stressed usando T-004, calcular pricing\_stressed usando T-007, recolectar todos los outputs en StressResult object  
* Implementar comparison engine: dado baseline\_result y stressed\_result, calcular deltas y ratios: delta\_CFADS \= CFADS\_stressed \- CFADS\_base, delta\_DSCR, delta\_prices, identify\_breaches (qu√© covenants se violan bajo stress que no se violan en base)  
* M√©todo calculate\_stress\_metrics que compute indicadores de resilience: time\_to\_default \= n√∫mero de per√≠odos hasta primer covenant breach bajo stress (infinito si nunca breach), severity\_of\_breach \= m√°xima magnitud de DSCR undershoot, recovery\_time \= per√≠odos desde shock hasta recuperaci√≥n de DSCR \> covenant  
* Implementar batch stress testing: ejecutar m√∫ltiples escenarios en loop, comparar resultados side-by-side, rank escenarios por severity (cu√°l causa mayor da√±o), identify most vulnerable period del proyecto  
* M√©todo generate\_stress\_dashboard que cree tabla comprehensiva: filas \= escenarios, columnas \= m√©tricas clave (min DSCR, EL, default flag, recovery time), formato condicional resaltando failures en rojo  
* Implementar probabilistic stress testing: en lugar de deterministic shock, shock parameters son random variables con distribuci√≥n, ejecutar Monte Carlo dentro de stress test, obtener distribuci√≥n de outcomes bajo stress scenario  
* M√©todo calculate\_stress\_var que compute VaR conditional on stress scenario: P(Loss | Stress S1), distribuci√≥n de p√©rdidas dado que ocurre S1 es diferente de distribuci√≥n incondicional, √∫til para risk planning  
* Crear m√≥dulo de stress propagation: rastrear c√≥mo shock inicial se propaga a trav√©s del modelo, ej: demand shock ‚Üí menores revenues ‚Üí menor CFADS ‚Üí menor DSCR ‚Üí breach covenant ‚Üí trigger reserve funding ‚Üí menor disponible para dividendos, visualizar cascade of effects  
* Implementar comparative stress testing: ejecutar mismo stress en estructura Traditional vs Tokenized, identificar cu√°l estructura es m√°s resilient, argumentar sobre design features que mejoran robustness  
* M√©todo simulate\_management\_actions que modele respuestas end√≥genas al stress: si DSCR \< covenant, management puede cortar OPEX discrecional para preservar solvency, incorporar estas countermeasures en simulaci√≥n para realismo  
* Implementar break-even analysis: para cada escenario, identificar qu√© magnitude de shock causar√≠a break-even donde DSCR \= covenant exactly, ej: demand puede caer hasta 18% antes de breach, √∫til para determinar safety margin  
* Crear visualizaci√≥n de stress waterfall: gr√°fico comparativo mostrando waterfall normal vs waterfall bajo stress, destacar qu√© pagos se sacrifican, d√≥nde aparecen shortfalls  
* Documentar metodolog√≠a de stress testing: citar Basel Committee guidelines sobre stress testing de capital structure, explicar diferencia entre sensitivity analysis (small perturbations) vs stress testing (plausible but extreme shocks)

---

### **T-041: Resultados stress**

**¬øEn qu√© consiste?**  
 Compilar, analizar e interpretar resultados de stress testing generando reportes comprehensivos con visualizaciones y conclusiones sobre resilience de cada estructura.

**Relaciones con otras tareas:**

* **Depende de:** T-038B (todos los escenarios ejecutados), T-040 (motor produjo resultados)  
* **Utilizado en:** T-048 (notebook final), T-049 (informe acad√©mico)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase StressResultsAnalyzer en archivo pftoken/stress/results\_analyzer.py  
* M√©todo aggregate\_stress\_results que consolide outputs de m√∫ltiples escenarios: crear DataFrame comprehensivo con fila por escenario y columnas para cada m√©trica relevante, incluir baseline\_case como primera fila para comparaci√≥n  
* Implementar m√©tricas de resiliencia por estructura: count n√∫mero de escenarios donde estructura sobrevive sin default, average DSCR under stress, worst-case DSCR observed, fraction of scenarios triggering covenant breach, tiempo promedio hasta recovery despu√©s de shock  
* M√©todo rank\_scenarios\_by\_severity que ordene escenarios desde m√°s a menos da√±inos: usar score compuesto combinando impact en DSCR, EL, default probability, NPV destruction, identificar "killer scenarios" donde proyecto fails irrecuperablemente  
* Implementar an√°lisis de break points: identificar umbrales cr√≠ticos donde proyecto transition de stressed pero viable a default, ej: proyecto sobrevive demand shock hasta \-25% pero \-30% causa cascade failure  
* M√©todo compare\_structure\_resilience que eval√∫e Traditional vs Tokenized bajo estr√©s: count scenarios where Tokenized survives but Traditional fails (or vice versa), compute relative performance \= (DSCR\_tokenized \- DSCR\_traditional) / DSCR\_traditional under each stress, argue cu√°l estructura es more robust  
* Crear tabla de resiliencia comparativa: filas \= escenarios, columna "Traditional" con DSCR m√≠nimo bajo ese escenario, columna "Tokenized" con DSCR m√≠nimo, columna "Advantage" mostrando cu√°l es mejor, formato condicional resaltando winner  
* Implementar vulnerability heatmap: matriz con per√≠odos en eje X, escenarios en eje Y, celdas coloreadas por magnitude de DSCR breach, identificar visualmente "danger zones" donde proyecto es vulnerable  
* M√©todo calculate\_expected\_recovery que compute tiempo y costo de recovery post-shock: si covenant breach ocurre, cu√°ntos per√≠odos hasta DSCR vuelve \> threshold, cu√°nto CFADS acumulado se pierde durante recovery, estimar NPV destruction del shock  
* Crear distribuci√≥n de time-to-default: histograma mostrando en qu√© per√≠odo ocurre primer default bajo cada escenario, moda indica per√≠odo m√°s vulnerable, √∫til para timing de hedges o refinanciamiento preventivo  
* Implementar an√°lisis de near-misses: identificar escenarios donde proyecto casi hace default pero sobrevive por narrow margin, ej: DSCR llega a 1.26x (apenas above 1.25 covenant), estos near-misses indican fragilidad  
* M√©todo generate\_executive\_summary que produzca reporte en lenguaje no-t√©cnico: explicar cu√°les son mayores amenazas al proyecto, bajo qu√© circunstancias proyecto podr√≠a fallar, qu√© mitigaciones se recomiendan, presentar en formato de slides  
* Crear visualizaciones comprehensivas: radar chart comparando resilience de structures en m√∫ltiples dimensiones (severity of breach, recovery time, default probability, EL increase), tornado chart mostrando sensibilidad de DSCR a diferentes shocks, spaghetti plot de trayectorias de DSCR bajo cada escenario  
* Implementar confidence intervals en resultados: dado que algunos stress tests usan MC interno, reportar no solo mean outcome sino tambi√©n percentiles 10/90, reconocer incertidumbre incluso dentro de stress scenario  
* Documentar conclusiones acad√©micas: argumentar qu√© features de estructura contribuyen a resilience (ej: mayor DSRA, menor leverage inicial, diversificaci√≥n de tranches), relacionar findings con literatura de project finance sobre failure determinants

---

### **T-042: Reverse stress testing**

**¬øEn qu√© consiste?**  
 Implementar reverse stress testing que en lugar de partir de escenarios predefinidos, identifica qu√© combinaci√≥n de shocks causar√≠a default inevitable del proyecto.

**Relaciones con otras tareas:**

* **Depende de:** T-041 (entender resultados de stress forward)  
* **Complementa:** T-040 (motor de estr√©s usado en modo inverso)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase ReverseStressTester en archivo pftoken/stress/reverse\_stress.py  
* M√©todo find\_breaking\_point que dado par√°metro espec√≠fico (ej: revenue\_growth), busque valor que causa DSCR \= covenant threshold exactly, usar bisection o Newton method para converger eficientemente, ej: encontrar que revenue\_growth \= \-24.3% causa DSCR \= 1.25 exactly  
* Implementar b√∫squeda multidimensional: encontrar combinaci√≥n de (demand\_shock, rate\_shock, opex\_increase) que causa default, problema de optimizaci√≥n: minimize magnitude of shocks subject to DSCR \< 1.25, usar optimizer como scipy.optimize.minimize con constraint  
* M√©todo identify\_minimal\_fatal\_combo que encuentre "smallest" combinaci√≥n de shocks que mata proyecto: definir size como weighted sum of shock magnitudes normalized, solve para combo que minimiza size while causing default  
* Implementar monte carlo search para breaking points: sample random combinations de shocks, ejecutar stress test para cada combo, identify cu√°les causan default, cluster combos letales y buscar patrones  
* M√©todo map\_failure\_surface que discretize espacio de shocks en grid, evaluar outcome en cada punto del grid, crear surface plot mostrando regi√≥n de parameter space donde proyecto survives vs region donde defaults, boundary es critical frontier  
* Crear visualizaci√≥n 2D de safe/unsafe regions: plot con demand\_shock en X-axis, rate\_shock en Y-axis, regi√≥n verde donde proyecto viable, regi√≥n roja donde proyecto fails, boundary curve entre regiones es breaking line  
* Implementar an√°lisis de distance-to-failure: dado par√°metros base, calcular minimum distance en parameter space hasta failure boundary, larger distance indica mayor robustness margin, √∫til para quantificar safety buffer  
* M√©todo simulate\_gradual\_deterioration que modele decline gradual en lugar de shock √∫nico: empezar desde base case, incrementar stress magnitude step-by-step, identificar tipping point donde sistema colapsa abruptamente vs deterioro gradual  
* Implementar an√°lisis de puntos de inflexi√≥n no-lineales: en algunos casos peque√±o shock adicional causa disproportionate damage (threshold effects), identificar estos non-linearities que son peligrosos  
* M√©todo generate\_vulnerability\_map que categorice proyecto en zones: safe zone (m√∫ltiples shocks requeridos para default), caution zone (single severe shock puede causar distress), danger zone (sistema es fr√°gil y peque√±as perturbaciones fatal)  
* Crear narrative de cada breaking scenario: explicar storyline plausible que llevar√≠a a esa combinaci√≥n de shocks, ej: "Trade war reduce demanda global de IoT en 20% mientras simult√°neamente Fed sube tasas 250bps para combatir inflaci√≥n importada", evaluar plausibility vs extremity  
* Implementar probability weighting: asignar probabilidades a diferentes breaking scenarios basado en historical precedent, integrate para calcular probability of project failure \= P(entering failure region), useful para risk reporting  
* Documentar insights estrat√©gicos: identificar cu√°les par√°metros cuando shocking causan mayor vulnerabilidad (ej: proyecto m√°s sensible a demand que a costs), informar sobre qu√© variables hedging o covenants deber√≠an focus

---

### **T-043: Stress h√≠bridas**

**¬øEn qu√© consiste?**  
 Implementar h√≠bridos entre stress testing determin√≠stico y simulaci√≥n Monte Carlo estoc√°stica, permitiendo evaluar riesgo bajo condiciones espec√≠ficas de stress.

**Relaciones con otras tareas:**

* **Depende de:** T-041 (stress results) \+ T-031 (MC pipeline)  
* **Sintetiza:** Stress determin√≠stico \+ MC estoc√°stico  
* **Duraci√≥n:** 2 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase HybridStressTester en archivo pftoken/stress/hybrid\_stress.py que combine ambos enfoques  
* M√©todo stress\_conditional\_mc que ejecute Monte Carlo conditional en escenario de stress: fijar algunos par√°metros en valores estresados (ej: base\_rate \= 7% instead of 5%), mientras otros par√°metros permanecen estoc√°sticos con sus distribuciones (ej: demand growth \~ N(Œº, œÉ)), ejecutar MC completo bajo esta condici√≥n  
* Ejemplo: "Dado que tasas suben 200bps (deterministic), cu√°l es distribuci√≥n de outcomes considerando incertidumbre normal en demanda y costos (stochastic)", produce distribuci√≥n de DSCR conditional on rate stress  
* Implementar conditional VaR: calcular VaR\_95 bajo diferentes stress conditions, comparar VaR\_baseline vs VaR\_given\_rate\_shock vs VaR\_given\_demand\_shock, identificar cu√°l stress amplifica tail risk m√°s  
* M√©todo progressive\_stress\_mc que modele stress escalating over time: empezar con mild stress en a√±o 2, increase severity gradualmente hasta a√±o 5, overlay esto sobre trayectorias estoc√°sticas de MC, capturar interaction entre stress trend y noise estoc√°stico  
* Implementar path-dependent stress: magnitud de stress en per√≠odo t depende de outcomes en per√≠odo t-1, ej: si DSCR\_t-1 \< 1.5 entonces demand\_stress\_t incrementa por feedback negativo (clients lose confidence), modelar spirals downward  
* M√©todo adaptive\_stress\_scenario que ajuste severity of stress basado en resilience observed: si proyecto surviving well bajo moderate stress, escalate to severe, continuar hasta encontrar stress level que causa distress, √∫til para calibrar robustness  
* Crear familia de stress-conditional distributions: para cada escenario S1-S6, generar histograma completo de outcomes considerando variabilidad estoc√°stica adem√°s del shock determin√≠stico, comparar dispersi√≥n de outcomes bajo cada stress  
* Implementar an√°lisis de probability of default under stress: PD\_unconditional vs PD\_conditional\_on\_S1, quantify cu√°nto aumenta PD ante cada escenario, useful para stress testing of risk metrics  
* M√©todo combine\_historical\_stress\_with\_stochastic: usar shocks hist√≥ricos observados (ej: COVID demand drop de 2020\) como central tendency, agregar noise estoc√°stico around eso, produce realistic "could have been worse" scenarios  
* Crear visualizaci√≥n de stress+uncertainty: fan chart mostrando percentiles de outcomes bajo stress scenario, wider fan indica mayor incertidumbre, overlap entre fans de different stresses indica regions donde outcomes are ambiguous  
* Implementar variance decomposition: dado outcome bajo hybrid stress, descomponer variance en component debido a deterministic shock vs component debido a stochastic noise, identificar si stress dominate o noise dominate  
* Documentar casos de uso: hybrid stress √∫til para stress testing donde queremos evaluar specific risk (deterministic shock) mientras reconociendo que other risks remains uncertain (stochastic), m√°s realista que pure deterministic stress testing

---

## **WP-07: SIMULACI√ìN MONTE CARLO**

### **T-021: Motor MC estoc√°stico**

**¬øEn qu√© consiste?**  
 Implementar el motor central de simulaci√≥n Monte Carlo que genera trayectorias estoc√°sticas de par√°metros del proyecto, ejecuta CFADS+Waterfall en cada trayectoria, y recolecta distribuci√≥n completa de outcomes.

**Relaciones con otras tareas:**

* **Cr√≠tico: Depende de:** T-004 (ratios), T-005B (Merton validado)  
* **Utilizado por:** T-026 (pricing estoc√°stico), T-033 (p√©rdida esperada), T-038 (stress testing)  
* **Se extiende con:** T-022 (variables aleatorias), T-023 (correlaciones), T-024 (Merton integrado)  
* **Duraci√≥n:** 5 d√≠as seg√∫n Gantt (tarea core)

**Qu√© se debe implementar:**

* Clase MonteCarloEngine en archivo pftoken/simulation/monte\_carlo.py que orquesta simulaci√≥n completa  
* M√©todo run\_simulation que ejecute N simulaciones configurables: t√≠picamente N \= 10,000 para balance entre precisi√≥n y runtime, cada simulaci√≥n \= un escenario futuro posible  
* Implementar seeding para reproducibilidad: permitir fijar random seed en SimulationParams, mismo seed produce mismos resultados, cr√≠tico para debugging y comparaciones  
* Implementar arquitectura vectorizada para performance: usar NumPy broadcasting para generar m√∫ltiples trayectorias simult√°neamente, evitar loops Python expl√≠citos cuando posible, meta \= ejecutar 10k simulaciones en \< 5 minutos  
* M√©todo generate\_scenario que sample un conjunto de par√°metros estoc√°sticos: revenue\_growth\_realizado, OPEX\_realizado, base\_rate\_realizado, etc., usando distribuciones definidas en T-022  
* Para cada scenario generado, ejecutar pipeline completo: calcular CFADS\_scenario usando T-003B, ejecutar Waterfall\_scenario usando T-006, calcular ratios\_scenario usando T-004, detectar defaults si DSCR \< covenant, calcular losses si default ocurre  
* Implementar estructura de datos para almacenar resultados: SimulationResult conteniendo arrays de N elementos: CFADS\_t para cada per√≠odo t y escenario, DSCR\_t, default\_flags, losses\_senior, losses\_mezz, losses\_sub, prices\_por\_tramo  
* M√©todo aggregate\_results que compute estad√≠sticas sobre distribuci√≥n: mean, median, std dev, percentiles (5th, 25th, 75th, 95th), histogram bins, todo esto para cada m√©trica de inter√©s  
* Implementar convergence checking: verificar que estad√≠sticas se estabilizan a medida que N aumenta, plotear mean y variance como funci√≥n de N, detectar si N es suficiente o se requieren m√°s simulaciones  
* M√©todo calculate\_confidence\_intervals que compute intervals using bootstrap: resample simulaciones con replacement, recalculate statistics en cada bootstrap sample, percentiles de bootstrap distribution dan confidence intervals  
* Implementar parallel execution: si computadora tiene m√∫ltiples cores, distribuir simulaciones across cores usando multiprocessing, reducir runtime proporcionalmente a n√∫mero de cores disponibles  
* Crear progress tracking: mostrar progress bar durante simulaci√≥n indicando porcentaje completo y ETA, √∫til para simulaciones largas  
* Implementar checkpointing: guardar resultados parciales cada 1000 simulaciones, permite resumir simulaci√≥n si es interrumpida, evita perder progreso en caso de crash  
* M√©todo export\_results que exporte resultados completos a CSV y/o HDF5 para an√°lisis posterior, incluir metadata como par√°metros usados, timestamp, version del c√≥digo  
* Implementar validation checks post-simulation: verificar que distribuci√≥n de outcomes es razonable (no todos los escenarios default, no todos son perfectos), detectar anomal√≠as que podr√≠an indicar bugs  
* Crear visualizaci√≥n de trayectorias: spaghetti plot mostrando CFADS\_t de m√∫ltiples scenarios superpuestos, transparencia para ver densidad, resaltar percentiles clave como bandas  
* Documentar design decisions: por qu√© usar distribuciones particulares, c√≥mo se calibraron par√°metros, qu√© correlaciones se asumieron, limitaciones del modelo

---

### **T-022: Variables aleatorias**

**¬øEn qu√© consiste?**  
 Especificar distribuciones estad√≠sticas para cada variable estoc√°stica del modelo y implementar sampling eficiente de estas distribuciones.

**Relaciones con otras tareas:**

* **Depende de:** T-021 (motor MC necesita distributions)  
* **Informado por:** T-047 (calibraci√≥n de par√°metros estoc√°sticos)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase StochasticVariables en archivo pftoken/simulation/distributions.py  
* Para revenue growth: usar distribuci√≥n lognormal con par√°metros Œº y œÉ calibrados en T-047, lognormal asegura growth rate positivo, capture right-skew t√≠pico de uncertainty econ√≥mica  
* Implementar sampling: growth\_rate\_sample \= np.random.lognormal(mean=Œº, sigma=œÉ, size=N) para N simulaciones  
* Para ARPU (Average Revenue Per User): usar normal truncada bounded entre \[ARPU\_min, ARPU\_max\], evita valores negativos o absurdamente altos, reflejar competencia mantiene ARPU en rango  
* Para churn rate: usar beta distribution bounded en \[0,1\] apropiado para rates/probabilities, shape parameters Œ± y Œ≤ calibrados para match mean y variance observados  
* Para OPEX: modelar como funci√≥n de CFADS con noise aditivo, OPEX \= Œ± √ó CFADS \+ N(Œº, œÉ), captura que costos tienden a escalar con actividad pero tienen component fijo  
* Para base interest rate: usar proceso de mean reversion (Ornstein-Uhlenbeck), r\_t \= r\_t-1 \+ Œ∫(Œ∏ \- r\_t-1)dt \+ œÉdW, tasas tienden a revertir hacia long-run mean Œ∏ con speed Œ∫  
* Implementar Geometric Brownian Motion para variables de precio: dS/S \= Œºdt \+ œÉdW, usado para simular valor de activos en modelo Merton de T-024  
* Para eventos discretos como launch failure: usar Bernoulli distribution con probabilidad p calibrada desde hist√≥rico, sample \= np.random.binomial(n=1, p=p\_failure)  
* Implementar mixture distributions para capturar regime changes: en 95% de casos distribution A (normal times), en 5% distribution B (crisis), sample primero el regime y luego condicional en regime sample value  
* M√©todo validate\_distributions que verifique samples generados tienen propiedades correctas: mean emp√≠rico ‚âà mean te√≥rico, variance emp√≠rica ‚âà variance te√≥rica, no outliers absurdos  
* Crear visualizaci√≥n de distribuciones: histograma de samples generados vs curva de PDF te√≥rica superpuesta, Q-Q plot para verificar goodness of fit  
* Implementar antithetic variates technique: para reducir variance de estimadores MC, generar pares de samples (X, \-X) sim√©tricos, reduce variance sin aumentar n√∫mero de simulaciones  
* Documentar elecci√≥n de distribuciones: justificar por qu√© lognormal para growth (prevent negative rates), por qu√© beta para proportions, por qu√© mean reversion para tasas (reflect monetary policy)

---

### **T-023: Matriz correlaci√≥n**

**¬øEn qu√© consiste?**  
 Especificar matriz de correlaci√≥n entre variables estoc√°sticas y implementar generaci√≥n de muestras correlacionadas usando Cholesky decomposition.

**Relaciones con otras tareas:**

* **Depende de:** T-022 (variables aleatorias definidas)  
* **Calibrada en:** T-047 (estimaci√≥n de correlaciones)  
* **Duraci√≥n:** 2 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase CorrelationMatrix en archivo pftoken/simulation/correlation.py  
* Definir matriz de correlaci√≥n œÅ de dimensi√≥n (n\_vars √ó n\_vars) donde n\_vars \= n√∫mero de variables estoc√°sticas  
* Ejemplo de correlaciones importantes: œÅ(revenue\_growth, OPEX) \= 0.4 positiva (m√°s actividad ‚Üí m√°s costos), œÅ(revenue\_growth, base\_rate) \= \-0.2 negativa (baja actividad ‚Üí Fed baja tasas), œÅ(OPEX, degradation) \= 0.5 positiva (mayor degradaci√≥n ‚Üí mayor mantenimiento)  
* Implementar validaci√≥n de matriz: verificar que œÅ es sim√©trica (œÅ\_ij \= œÅ\_ji), diagonal \= 1, eigenvalues todos positivos (positive definite condition), si no es positive definite hacer ajuste para ensure it  
* M√©todo generate\_correlated\_samples usando Cholesky decomposition: descomponer œÅ \= L L^T donde L es lower triangular, generar samples independientes Z \~ N(0,1) de dimensi√≥n n\_vars, transformar X \= L Z para obtener samples correlacionados con matriz de correlaci√≥n œÅ  
* Implementar para caso multivariado: generar matriz de N√ón\_vars donde N \= n√∫mero de simulaciones, aplicar Cholesky, cada fila es un escenario con variables correlacionadas  
* M√©todo transform\_to\_target\_distributions: samples X est√°n en espacio normal est√°ndar, transformar cada variable a su distribution target usando quantile transformation, ej: para lognormal usar X\_log \= F\_lognormal^(-1)(Œ¶(X\_normal))  
* Implementar copula approach como alternativa: Gaussian copula separa estructura de correlaci√≥n (copula) de distribuciones marginales, m√°s flexible que asumir todo es Gaussian  
* Validar samples generados: calcular correlation matrix emp√≠rica de samples, comparar con œÅ target, verificar que match es close (tolerancia \~0.05)  
* Implementar sensibilidad a correlaci√≥n: correr simulaciones con diferentes assumptions de œÅ, mostrar c√≥mo resultados (VaR, DSCR, etc) cambian con correlaci√≥n, t√≠picamente higher correlation ‚Üí higher tail risk  
* Crear visualizaci√≥n de correlaciones: heatmap de matriz œÅ con color intensity indicando strength, scatter plots pairwise de variables mostrando correlaci√≥n visualmente  
* Documentar fuentes de calibraci√≥n: c√≥mo se estimaron correlaciones, si desde datos hist√≥ricos o juicio experto, incertidumbre en estimates, impacto de correlation errors en resultados

### **T-024: Merton en MC**

**¬øEn qu√© consiste?**  
 Integrar modelo de Merton de T-005 dentro del loop de Monte Carlo para calcular PD y LGD end√≥genamente en cada trayectoria simulada.

**Relaciones con otras tareas:**

* **Depende de:** T-021 (MC engine), T-005 (modelo Merton)  
* **Genera:** PD din√°mica por trayectoria  
* **Duraci√≥n:** 2 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Extender MonteCarloEngine para incluir c√°lculo de Merton en cada scenario  
* En cada iteraci√≥n de MC: simular trayectoria de valor de activos V(t) usando GBM desde T-022, simular trayectoria de saldo de deuda D(t) que decrece por amortization seg√∫n waterfall  
* En cada per√≠odo t de la simulaci√≥n: calcular distance to default: DD\_t \= \[ln(V\_t / D\_t) \+ (Œº \- œÉ¬≤/2)œÑ\] / (œÉ‚àöœÑ) donde œÑ es time to maturity desde t  
* Calcular PD\_t \= Œ¶(-DD\_t) que es probability of default en horizonte œÑ starting from t  
* Si V\_t \< D\_t en alg√∫n momento ‚Üí immediate default triggered, marcar default\_flag \= True, calcular recovery \= V\_t / D\_t, LGD \= 1 \- recovery  
* Implementar l√≥gica de prelaci√≥n en recovery: si default ocurre y V\_default disponible, senior recupera primero hasta su claim, mezz recupera de residuo, sub obtiene √∫ltimo, calcular recovery\_senior, recovery\_mezz, recovery\_sub por separado  
* M√©todo calculate\_dynamic\_pd que retorne time series de PD\_t durante trayectoria de simulaci√≥n: muestra c√≥mo PD evoluciona a medida que V y D cambian, t√≠picamente PD increase cuando leverage aumenta o performance se deteriora  
* M√©todo aggregate\_pd\_across\_simulations que compute PD promedio en cada per√≠odo agregando sobre N simulaciones: PD\_aggregated\_t \= mean(PD\_scenario\_t) over all scenarios  
* Implementar term structure de PD: extraer PD para diferentes horizontes (1y, 3y, 5y, 10y) desde simulaciones, mostrar que PD increasing con horizon reflejando cumulative risk  
* Calcular EL end√≥gena: EL\_scenario \= PD\_scenario √ó LGD\_scenario √ó EAD\_scenario, agregar EL over scenarios para obtener expected loss total  
* Comparar EL end√≥gena de Merton con EL ex√≥gena fija de T-005: verificar si modelar PD din√°micamente cambia significativamente resultados vs usar PD constante  
* Crear visualizaci√≥n de trayectorias V vs D: plot mostrando evoluci√≥n de valor de activos y valor de deuda en m√∫ltiples scenarios, resaltar scenarios donde V cruza por debajo de D (default events), mostrar distribuci√≥n de timing de defaults  
* Implementar feedback loop entre defaults y correlaci√≥n: si un tranche defaults, puede aumentar correlaci√≥n de otros tranches por contagion, modelar este systemic effect  
* Documentar ventajas de approach end√≥geno: captura naturaleza path-dependent del riesgo de cr√©dito, refleja que PD no es est√°tica sino evoluciona con fundamentals del proyecto, m√°s realista que fixed PD approach

### **T-025: Flags de default**

**¬øEn qu√© consiste?**  
 Implementar sistema robusto de detecci√≥n y clasificaci√≥n de eventos de default durante simulaciones, diferenciando tipos de default y severidad.

**Relaciones con otras tareas:**

* **Depende de:** T-024 (Merton detecta algunos defaults)  
* **Utilizado por:** T-030 (probabilidades de breach), T-033 (c√°lculo de p√©rdidas)  
* **Duraci√≥n:** 2 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase DefaultDetector en archivo pftoken/simulation/default\_detection.py  
* Enum DefaultType definiendo categor√≠as: TECHNICAL\_DEFAULT (breach de covenant sin p√©rdida econ√≥mica inmediata), PAYMENT\_DEFAULT (no pago de intereses o principal programado), CROSS\_DEFAULT (default triggered por breach en otra deuda), BANKRUPTCY (insolvencia total con liquidaci√≥n)  
* M√©todo detect\_default\_events que escanee cada per√≠odo de simulaci√≥n verificando m√∫ltiples condiciones  
* **Default Tipo 1 \- Covenant Breach:** verificar si DSCR \< covenant\_threshold por X per√≠odos consecutivos (t√≠picamente 2), si detectado marcar technical\_default \= True, severity \= moderate si DSCR entre 1.0-1.25, severity \= severe si DSCR \< 1.0  
* **Default Tipo 2 \- Payment Default:** verificar si en waterfall alg√∫n tranche no recibi√≥ pago completo de intereses, si senior\_interest\_paid \< senior\_interest\_due entonces payment\_default\_senior \= True con severity \= critical, mezz y sub defaults son menos cr√≠ticos  
* **Default Tipo 3 \- Insolvency:** verificar condici√≥n Merton V \< D indicando activos insuficientes para cubrir deuda, bankruptcy \= True, severity \= terminal, activar proceso de liquidaci√≥n  
* **Default Tipo 4 \- Reserve Depletion:** si DSRA\_balance \= 0 AND CFADS insuficiente para servicio entonces liquidity\_crisis \= True, puede ser temporary si CFADS recovers pronto  
* Implementar cure periods: algunos defaults pueden ser "curados" si condici√≥n se revierte r√°pidamente, ej: DSCR breach en 1 per√≠odo seguido por recovery en pr√≥ximo per√≠odo \= warning no default, solo marcar default si condition persists  
* M√©todo calculate\_default\_probability que compute frecuencia de cada tipo de default: P(covenant\_breach) \= count(scenarios con breach) / total\_scenarios, P(payment\_default) \= count(scenarios con payment miss) / total, calcular para cada tranche por separado  
* Implementar time-to-default tracking: para scenarios con default, registrar per√≠odo exacto cuando ocurre primer default, crear distribuci√≥n de time-to-default √∫til para pricing y risk management  
* M√©todo classify\_default\_severity usando score: score \= weighted\_sum(DSCR\_breach\_magnitude, payment\_miss\_amount, recovery\_rate\_shortfall), classify como mild/moderate/severe/catastrophic basado en score  
* Crear matriz de co-occurrence: calcular cu√°n frecuentemente diferentes tipos de default ocurren juntos, ej: payment default de mezz casi siempre accompanied by covenant breach, pero covenant breach no siempre lleva a payment default  
* Implementar contagion detection: marcar scenarios donde default de un tranche trigger cascade de defaults en otros tranches, √∫til para identificar systemic risk  
* M√©todo generate\_default\_timeline que para cada scenario con default cree narrativa temporal: "Per√≠odo 18: DSCR breach (1.22x), Per√≠odo 19: DSCR persists (1.18x) ‚Üí technical default declared, Per√≠odo 20: Payment miss en mezz, Per√≠odo 22: Bankruptcy declared"  
* Crear visualizaci√≥n de default patterns: heatmap mostrando per√≠odo de default en eje X, tipo de default en eje Y, intensidad de color indica frecuencia, identify vulnerable periods visualmente  
* Implementar recovery tracking post-default: si default no es terminal, rastrear si proyecto logra cure el default y recover, calculate recovery\_rate y recovery\_time statistics  
* Documentar taxonom√≠a de defaults: explicar diferencias entre tipos, severity implications para investors, typical remedies por tipo seg√∫n Project Finance standards

### **T-029: DSCR/LLCR por trayectoria**

**¬øEn qu√© consiste?**  
 Calcular ratios de cobertura de deuda (DSCR, LLCR) para cada trayectoria individual de Monte Carlo, generando distribuciones completas de estos ratios clave.

**Relaciones con otras tareas:**

* **Depende de:** T-021 (MC genera trayectorias), T-004 (c√°lculo de ratios establecido)  
* **Alimenta:** T-030 (probabilidades de breach), T-034 (m√©tricas de cola)  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Extender MonteCarloEngine para calcular ratios en cada scenario  
* Implementar c√°lculo vectorizado de DSCR: dado arrays de CFADS\[scenario, period\] y DebtService\[scenario, period\], calcular DSCR\[scenario, period\] \= CFADS / DebtService element-wise, manejar divisi√≥n por cero si DebtService \= 0 en grace period  
* M√©todo calculate\_dscr\_distribution que para cada per√≠odo t agregue sobre scenarios: compute mean\_DSCR\_t, median\_DSCR\_t, std\_DSCR\_t, percentiles (10th, 25th, 75th, 90th, 95th, 99th), histogram de DSCR\_t mostrando shape de distribuci√≥n  
* Implementar tracking de DSCR m√≠nimo: para cada scenario identificar min\_DSCR \= minimum ratio observado durante toda la vida del proyecto, agregar min\_DSCR across scenarios para obtener distribuci√≥n de "worst moment" DSCR  
* M√©todo calculate\_llcr\_per\_trajectory que compute LLCR en cada per√≠odo de cada scenario: requiere calcular NPV de flujos futuros desde t hasta maturity dentro de cada trayectoria, descontar usando tasa apropiada por tranche, dividir por saldo de deuda en t  
* Implementar c√°lculo eficiente de LLCR: pre-calcular discount factors, usar cumulative sum reversa para compute NPV r√°pidamente, evitar loops anidados para performance  
* Crear estructura RatioDistributions como dataclass conteniendo: DSCR\_by\_period con estad√≠sticas, LLCR\_by\_period con estad√≠sticas, PLCR\_distribution, min\_DSCR\_distribution, max\_DSCR\_distribution, time\_series\_percentiles  
* M√©todo identify\_breach\_scenarios que filtre scenarios donde DSCR cae por debajo de covenant en alg√∫n momento: compute breach\_rate \= count(scenarios con breach) / total\_scenarios, identificar caracter√≠sticas comunes de breach scenarios (t√≠picamente baja demanda \+ altos costos)  
* Implementar an√°lisis de persistence: dado breach en per√≠odo t, cu√°l es probabilidad de breach en t+1, calcular auto-correlation de breach events, mide si breaches son transitorios o persistentes  
* M√©todo calculate\_coverage\_headroom que compute margen de seguridad: headroom\_t \= (DSCR\_t \- covenant\_threshold) / covenant\_threshold, positivo \= OK, negativo \= breach, analizar distribuci√≥n de headroom en different per√≠odos  
* Crear visualizaci√≥n de fan charts: plot mostrando percentiles de DSCR en el tiempo, l√≠neas para 10th, 25th, 50th, 75th, 90th percentiles creando "fan" que muestra uncertainty increasing con horizonte, l√≠nea horizontal en covenant threshold para reference  
* Implementar comparaci√≥n vs benchmark: overlaying DSCR distribution del proyecto con distribuciones t√≠picas de Project Finance comparables, evaluar si proyecto es m√°s o menos riskier que peers  
* M√©todo generate\_stress\_overlay que superponga distribuci√≥n base vs distribuci√≥n bajo stress scenarios: mostrar c√≥mo stress shift distribution downward y increase tail risk  
* Documentar interpretaci√≥n de distribuciones: explicar que median DSCR indica central tendency, percentiles bajos indican tail risk, volatilidad de DSCR indica stability del proyecto, persistent low DSCR indica structural problems

### **T-030: Probabilidades breach**

**¬øEn qu√© consiste?**  
 Calcular probabilidades de breach de covenants en diferentes per√≠odos y bajo diferentes condiciones, usando resultados de Monte Carlo.

**Relaciones con otras tareas:**

* **Depende de:** T-029 (distribuciones de DSCR/LLCR)  
* **Utilizado por:** T-033 (p√©rdida esperada requiere PD), T-034 (tail risk analysis)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase BreachProbabilityAnalyzer en archivo pftoken/analysis/breach\_probability.py  
* M√©todo calculate\_period\_breach\_probability que para cada per√≠odo t compute: P(DSCR\_t \< threshold) \= count(scenarios donde DSCR\_t \< 1.25) / total\_scenarios, produce time series de probabilidades mostrando c√≥mo riesgo evoluciona  
* Implementar c√°lculo de cumulative breach probability: P(breach antes de per√≠odo T) \= P(breach en alg√∫n momento hasta T), compute usando counting o usando survival analysis framework  
* M√©todo calculate\_conditional\_breach\_probability: P(breach en t | no breach hasta t-1), mide probabilidad marginal de breach dado survival hasta ahora, captura increasing hazard rate si proyecto se deteriora  
* Implementar breach probability por covenant type: P(DSCR breach), P(LLCR breach), P(DSRA depletion), algunos covenants m√°s f√°ciles de violar que otros, ranking por severity  
* M√©todo analyze\_breach\_correlations: calcular correlation entre breach events de diferentes covenants, t√≠picamente DSCR breach correlaciona con LLCR breach pero no perfectamente, identify independent vs correlated risks  
* Implementar survival analysis: usar Kaplan-Meier estimator para compute survival function S(t) \= P(no breach hasta t), plot survival curve mostrando declining survival probability, hazard rate h(t) \= instantaneous risk de breach en t  
* M√©todo calculate\_expected\_time\_to\_breach: integrar sobre distribuci√≥n de time-to-default, produce metric como "proyecto esperado a breach covenant en a√±o 4.2 en promedio"  
* Implementar segmentation por caracter√≠sticas: compute breach probability conditional en initial conditions, ej: P(breach | high\_initial\_leverage) vs P(breach | low\_initial\_leverage), identify risk factors  
* M√©todo compare\_breach\_risk\_structures: calcular breach probabilities para Traditional vs Tokenized structures, argumentar cu√°l tiene lower breach risk, quantify difference in basis points  
* Crear tabla de breach probabilities: filas \= per√≠odos, columnas \= percentiles de distribuci√≥n (10th, 50th, 90th), celdas \= probabilidad de breach, formato condicional resaltando high-risk periods  
* Implementar confidence intervals sobre probabilidades: usar bootstrap resampling de simulaciones para compute uncertainty en probability estimates, reportar probabilities como point estimate ¬± margin of error  
* M√©todo generate\_breach\_heatmap: matriz con per√≠odo en X, covenant type en Y, color intensity \= breach probability, visually identify danger zones  
* Documentar interpretaci√≥n: explicar que 10% breach probability es material risk requiring attention, 50% probability indica high likelihood de stress, \> 90% suggests structure is undersized

### **T-031: Pipeline end-to-end MC**

**¬øEn qu√© consiste?**  
 Integrar todos los componentes de Monte Carlo en un pipeline cohesivo end-to-end que ejecute simulaci√≥n completa desde par√°metros hasta outputs finales con una sola llamada.

**Relaciones con otras tareas:**

* **Depende de:** T-026 (pricing estoc√°stico), T-029 (ratios por trayectoria), T-030 (probabilidades)  
* **Culminaci√≥n de:** WP-07 Simulaci√≥n Monte Carlo completo  
* **Utilizado por:** T-040 (stress testing usa este pipeline), T-048 (notebook final)  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase MonteCarloPipeline en archivo pftoken/simulation/pipeline.py que orquesta flujo completo  
* M√©todo run\_complete\_analysis que reciba solo ProjectParams y SimulationParams, ejecute toda la cadena, retorne ComprehensiveResults  
* **Step 1 \- Initialization:** validar par√°metros de entrada, setup random seeds, initialize result containers, log configuration  
* **Step 2 \- Generate Scenarios:** llamar T-022 para sample variables aleatorias, aplicar T-023 para correlations, generar matriz completa de scenarios \[N\_scenarios √ó N\_periods √ó N\_variables\]  
* **Step 3 \- Execute Trajectories:** para cada scenario calcular CFADS usando T-003B, ejecutar Waterfall usando T-006, calcular ratios usando T-004, detectar defaults usando T-025, calcular Merton PD/LGD usando T-024, almacenar todos los resultados  
* **Step 4 \- Price Tranches:** para cada scenario valorar cada tranche usando T-007 y T-026, generar distribuci√≥n de precios por tranche, calcular expected prices y price volatility  
* **Step 5 \- Calculate Risk Metrics:** agregar resultados para compute EL usando T-033, VaR/CVaR usando T-034, breach probabilities usando T-030, HHI usando T-037  
* **Step 6 \- Generate Analytics:** compute all derived metrics, perform comparisons, identify patterns, generate insights  
* **Step 7 \- Export Results:** save to CSV/HDF5, generate summary statistics, create visualizations, compile final report  
* Implementar progress tracking comprehensivo: mostrar progress en cada step principal, estimate remaining time, log warnings y errors  
* Implementar error handling robusto: try-catch en cada step, si un scenario falla no crash entire simulation, log failed scenarios para investigaci√≥n, continue con scenarios restantes  
* M√©todo validate\_results que ejecute sanity checks: verificar que distribuciones son razonables (no todos escenarios default, no todos perfectos), verificar conservation laws (suma de flujos en waterfall), identificar anomal√≠as estad√≠sticas  
* Implementar caching inteligente: si re-running simulation con algunos par√°metros cambiados, reusar c√°lculos que no cambiaron, detectar qu√© steps necesitan re-ejecuci√≥n  
* Crear comprehensive logging: log todas las decisiones importantes, timing de cada step, memory usage, warnings about potential issues, facilitar debugging de simulaciones complejas  
* M√©todo generate\_executive\_summary que produzca one-page summary: key metrics (mean DSCR, VaR\_95, default probability, WACD), comparison Traditional vs Tokenized, traffic-light indicators (green/yellow/red) para key risks, executive recommendation  
* Implementar comparator mode: run simulation para m√∫ltiples estructuras simult√°neamente, produce side-by-side comparison, highlight differences, facilitate decision-making  
* Crear configuraci√≥n por templates: definir templates predefinidos como "conservative", "balanced", "aggressive", cada template con par√°metros calibrados, usuarios pueden select template o customize  
* Implementar sensitivity analyzer integrado: vary key parameters one-at-a-time, re-run pipeline para cada variation, produce tornado chart mostrando sensitivity de NPV, DSCR, VaR a each parameter  
* Documentar workflow completo: crear diagrama de flujo mostrando todos los steps del pipeline, dependencies entre steps, decisiones en cada punto, facilitar entendimiento del modelo completo

## **WP-08: PRICING ESTOC√ÅSTICO**

### **T-026: Valuaci√≥n estoc√°stica**

**¬øEn qu√© consiste?**  
 Extender pricing determin√≠stico de T-007 para incorporar incertidumbre estoc√°stica de flujos futuros usando resultados de Monte Carlo, generando distribuciones de precios de tranches.

**Relaciones con otras tareas:**

* **Depende de:** T-021 (MC genera distribuci√≥n de flujos), T-007 (pricing base determin√≠stico)  
* **Contin√∫a en:** T-027 (duration/convexity), T-028 (calibraci√≥n de spreads)  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase StochasticPricing en archivo pftoken/pricing/stochastic\_pricing.py  
* M√©todo price\_tranche\_distribution que reciba distribuci√≥n completa de cashflows por tranche desde MC: para cada scenario calcular PV del tranche descontando sus flujos con tasa apropiada, generar distribuci√≥n de N precios (uno por scenario)  
* Implementar ajuste de tasa de descuento por riesgo: en cada scenario usar tasa \= risk\_free\_rate \+ spread donde spread puede variar con condiciones del scenario, scenarios con higher default risk requieren higher spread  
* M√©todo calculate\_expected\_price que compute E\[Price\] \= mean de distribuci√≥n de precios, este es valor justo del tranche considerando toda la incertidumbre, comparar con precio par para determinar si emisi√≥n debe ser over/under par  
* Implementar c√°lculo de price volatility: œÉ\_price \= std dev de distribuci√≥n de precios, alta volatilidad indica mayor riesgo de marking-to-market, inversores requieren mayor compensation  
* M√©todo calculate\_price\_percentiles que extraiga percentiles clave: P10, P25, P50 (median), P75, P90 de distribuci√≥n de precios, P10 representa downside scenario para inversores, P90 representa upside  
* Implementar an√°lisis de probability of loss: P(Price \< Par) \= probabilidad de que precio caiga por debajo de 100, mide riesgo de principal loss para investors, complementa VaR analysis  
* M√©todo decompose\_price\_uncertainty que atribuya variance de precio a diferentes fuentes: variance debido a revenue uncertainty, variance debido a cost uncertainty, variance debido a rate uncertainty, usar ANOVA o regression-based decomposition  
* Crear visualizaci√≥n de distribuciones de precio: histogram para cada tranche mostrando shape de distribuci√≥n, Senior t√≠picamente tight distribution (low risk), Sub tiene fat tails (high risk), overlay distributions para comparar tranches  
* Implementar pricing con early redemption options: si tranche tiene call provision permitiendo prepayment, modelar decision end√≥gena de ejercer call, price \= E\[min(PV\_hold, PV\_call)\], incorpora option value  
* M√©todo calculate\_yield\_distribution usando iteraci√≥n: para cada precio simulado, resolver para YTM que hace NPV \= ese precio, generar distribuci√≥n de yields, compute expected yield, yield volatility  
* Implementar spread\_distribution analysis: calcular spread sobre benchmark para cada scenario, genera distribuci√≥n de spreads, mean spread refleja riesgo promedio, tail of spread distribution refleja tail risk  
* M√©todo compare\_deterministic\_vs\_stochastic: plot precio determin√≠stico (usando expected cashflows) vs expected precio estoc√°stico (E\[PV(CF)\]), Jensen's inequality implica que pueden diferir si utility es nonlinear, discutir cu√°ndo difieren materialmente  
* Crear price-yield scatter: plot con yield en X-axis, price en Y-axis, uno por scenario, muestra relaci√≥n inversa yield-price y dispersi√≥n due to credit risk differences  
* Implementar risk-neutral pricing: ajustar probabilidades de scenarios para reflejar risk aversion, usar stochastic discount factor, compute risk-neutral expected price \= E^Q\[PV\], comparar con physical expected price \= E^P\[PV\]  
* Documentar diferencias con pricing determin√≠stico: explicar que usar expected cashflows subestima riesgo vs pricing cada scenario por separado, convexity effects pueden causar material differences

### **T-027: Duration y convexity**

**¬øEn qu√© consiste?**  
 Calcular duration efectiva y convexidad efectiva de cada tranche como medidas de sensibilidad de precio a cambios en tasas de inter√©s.

**Relaciones con otras tareas:**

* **Depende de:** T-008 (curva spot), T-026 (pricing estoc√°stico)  
* **Complementa:** T-044 (sensibilidad a tasas), T-045 (pricing de hedges)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Extender StochasticPricing con m√©todos de duration/convexity  
* M√©todo calculate\_effective\_duration usando bump-and-revalue: shift curva de tasas \+25bps, reprice tranche ‚Üí Price\_up, shift curva \-25bps, reprice ‚Üí Price\_down, duration \= (Price\_down \- Price\_up) / (2 √ó Price\_base √ó 0.0025)  
* Implementar para cada tranche por separado: Senior t√≠picamente tiene duration \~5-7 a√±os (depende de amortization), Mezz duration ligeramente mayor, Sub puede tener duration distinta si tiene different maturity  
* M√©todo calculate\_macaulay\_duration anal√≠ticamente: Duration\_Macaulay \= Œ£\[t √ó PV(CF\_t)\] / Price donde t es tiempo, peso cada per√≠odo por su contribuci√≥n a PV, mide weighted average time to receive cashflows  
* Calcular modified duration: Duration\_modified \= Duration\_Macaulay / (1 \+ yield), esta es la sensibilidad de precio a peque√±os cambios en yield, ŒîP/P ‚âà \-Duration\_modified √ó Œîy  
* M√©todo calculate\_effective\_convexity: Convexity \= (Price\_up \+ Price\_down \- 2√óPrice\_base) / (Price\_base √ó Œîy¬≤) donde Œîy \= 0.0025, mide curvature de relaci√≥n precio-yield, positiva para bonds normales  
* Implementar interpretaci√≥n de convexity: positive convexity es deseable, significa que losses de rate increases son menos que gains de rate decreases (asymmetric payoff favorable), negative convexity (ej: prepayment risk) es unfavorable  
* M√©todo calculate\_key\_rate\_durations: en lugar de shift paralelo de toda la curva, shift individual key rates (2y, 5y, 10y, etc) uno a la vez, compute sensibilidad a cada, suma de key rate durations \= effective duration, permite hedging granular  
* Crear tabla de risk metrics por tranche: columnas \= Duration, Convexity, DV01, rows \= Senior/Mezz/Sub, facilita comparaci√≥n de risk profiles  
* Implementar an√°lisis de duration gap: comparar duration de activos (proyecto CFADS) vs duration de pasivos (deuda), positive gap indica asset-liability mismatch risk, negative gap tambi√©n risky, objetivo es matching  
* M√©todo simulate\_rate\_scenarios para validar duration: generar \+100bps, \+200bps, \-50bps, \-100bps scenarios, compute actual price changes, verificar que duration aproximation is accurate for small changes pero breaks down para large shifts  
* Crear visualizaci√≥n de price-yield curve: plot mostrando precio del tranche como funci√≥n de yield level, tangent line en current yield \= duration, curvature \= convexity, ilustra second-order effects  
* Implementar duration hedging recommendation: dado duration de tranche, calcular notional de interest rate swap necesario para hedge, duration of swap √ó notional\_swap \= duration of tranche √ó notional\_tranche  
* Documentar diferencias entre duration types: Macaulay (time measure), Modified (yield sensitivity), Effective (rate curve sensitivity), explicar cu√°ndo usar cada uno

### **T-028: Calibraci√≥n spreads**

**¬øEn qu√© consiste?**  
 Calibrar spreads de cr√©dito de cada tranche sobre curva base usando modelo risk-based que relaciona spread con PD, LGD y otras m√©tricas de riesgo calculadas.

**Relaciones con otras tareas:**

* **Depende de:** T-026 (pricing), T-010 (m√©tricas de riesgo EL/PD/LGD), T-008 (curva base)  
* **Utilizado por:** T-036 (optimizaci√≥n de WACD requiere spreads calibrados)  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase SpreadCalibrator en archivo pftoken/pricing/spread\_calibration.py  
* M√©todo calibrate\_credit\_spread usando modelo reducido: spread\_tranche \= Œ± √ó PD \+ Œ≤ √ó LGD \+ Œ≥ √ó illiquidity\_premium \+ Œ¥ donde Œ±, Œ≤, Œ≥, Œ¥ son par√°metros a calibrar  
* Implementar estimaci√≥n de par√°metros: si hay datos de mercado de spreads observados para comparables, correr regresi√≥n de spreads vs (PD, LGD, illiquidity) para estimar Œ±, Œ≤, Œ≥, si no hay data usar benchmark values de literatura  
* M√©todo calculate\_default\_adjusted\_spread: spread que iguala expected return de bond riesgoso a bond libre de riesgo considerando probability de default, spread \= \-ln(1 \- PD √ó LGD) / duration, aproximation lineal \= PD √ó LGD para small PD  
* Implementar illiquidity premium calculation: tranche tokenizado puede tener menor illiquidity vs loan tradicional, cuantificar usando bid-ask spreads de mercados secundarios, t√≠picamente 10-50bps dependiendo de market depth  
* M√©todo calibrate\_to\_ratings: si tranches tienen ratings (AAA, A, BBB), usar historical default rates por rating desde Moody's/S\&P, mapear PD ‚Üí rating ‚Üí typical spread para ese rating  
* Implementar ajuste por seniority: Senior spread baseline, Mezz spread \= Senior \+ seniority\_premium (t√≠picamente 200-400bps), Sub spread \= Mezz \+ additional premium (400-600bps), reflejar structural subordination  
* M√©todo calibrate\_structural\_model usando Merton: relacionar spread con distance-to-default del modelo Merton, spread increase exponentially cuando DD decrease, implementar funci√≥n spread(DD) \= a √ó exp(-b √ó DD) donde a,b calibrados  
* Crear curva de term structure de spreads: spreads t√≠picamente increase con maturity reflejando cumulative default risk, implementar curva spread(tenor) usando interpolaci√≥n entre tenors observados  
* Implementar credit migration modeling: si rating puede cambiar en futuro (upgrade/downgrade), modelar probabilidades de migraci√≥n, ajustar spread por expected rating changes, incorporar en pricing  
* M√©todo validate\_spread\_calibration: verificar que spreads calibrados est√°n en rango razonable vs benchmarks de mercado, ej: Senior spread 100-200bps, Mezz 300-500bps, Sub 600-1000bps para infrastructure projects, flag si fuera de rango  
* Implementar sensibilidad de spreads: c√≥mo cambiar√≠an spreads si PD aumenta 50%, si LGD se deteriora, generar table mostrando elasticidades, √∫til para stress testing de pricing  
* M√©todo compare\_implied\_vs\_model\_spreads: si hay precios de mercado observados, extraer implied spreads, comparar con spreads predichos por modelo, compute pricing errors, refinar modelo si errors son grandes  
* Crear visualizaci√≥n de spread composition: stacked bar chart mostrando breakdown de spread en components: base spread por PD, add-on por LGD, add-on por illiquidity, total spread, facilita understanding de drivers  
* Documentar metodolog√≠a de calibraci√≥n: citar referencias acad√©micas sobre credit spread modeling, discutir limitaciones como ignoring spread volatility o recovery risk, acknowledge uncertainty en estimates

### **T-044: Sensibilidad tasas**

**¬øEn qu√© consiste?**  
 Analizar exhaustivamente c√≥mo cambios en curva de tasas de inter√©s afectan valoraci√≥n de tranches, DSCR, y viabilidad del proyecto.

**Relaciones con otras tareas:**

* **Depende de:** T-027 (duration calculada), T-008 (curva spot)  
* **Prepara para:** T-045 (hedging con caps requiere entender sensibilidad)  
* **Duraci√≥n:** 2 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase InterestRateSensitivity en archivo pftoken/analysis/rate\_sensitivity.py  
* M√©todo analyze\_parallel\_shift que shift curva completa por Œîr \= {-100, \-50, 0, \+50, \+100, \+200}bps, para cada shift recalcular: precio de cada tranche, DSCR de proyecto, WACD de estructura, compare resultados across shifts  
* Implementar c√°lculo de elasticidad: Œµ\_price \= (ŒîP/P) / (Œîr/r), mide sensibilidad porcentual, t√≠picamente senior tiene |Œµ| mayor que sub porque duration mayor (parad√≥jicamente menos risky bond m√°s sensible a rates)  
* M√©todo analyze\_non\_parallel\_shifts: generar scenarios de steepening (short rates stable, long rates up), flattening (short rates up, long rates stable), inversion (short \> long), cada shape tiene different impact on project con different maturity structure  
* Implementar simulation de trayectorias de tasas: usando mean reversion model de T-022, simular paths posibles de tasas futuras, para cada path recalcular DSCR stream, identificar paths que causan stress  
* M√©todo calculate\_rate\_var que compute "Valor en Riesgo por Tasa": cu√°nto puede caer valor del tranche con 95% confidence debido a movimientos adversos de tasas en pr√≥ximo a√±o, separa rate risk de credit risk  
* Crear tornado chart de sensibilidades: bars mostrando impacto en NPV de proyecto ante \+1% change en diferentes rates (Fed funds, 5y Treasury, 10y Treasury, etc), identifica cu√°l rate tiene mayor impacto  
* Implementar an√°lisis de cash flow timing impact: proyecto con early high CFADs es menos sensible a late rate increases vs proyecto con backend-loaded CFADs, cuantificar usando cash flow duration  
* M√©todo decompose\_total\_sensitivity: separar efecto de tasa en: (1) costo de deuda increase reduce CFADS disponible, (2) discount rate increase reduce PV de flujos, identificar cu√°l efecto domina  
* Crear scenario matrix: tabla con nivel de tasa en filas (3%, 5%, 7%, 9%), tranche en columnas, celdas \= valor del tranche, formato condicional mostrando green/red seg√∫n ganancia/p√©rdida  
* Implementar an√°lisis de break-even rate: encontrar nivel de tasa que hace DSCR \= covenant threshold exactly, ej: proyecto viable si tasas \< 8.5% pero breach covenant si tasas \> 8.5%, crucial para risk assessment  
* M√©todo simulate\_fed\_policy\_scenarios: modelar scenarios de Fed policy como "hawkish" (fast rate increases), "dovish" (stable/decreasing rates), "volatile" (frequent changes), evaluar project performance bajo cada regime  
* Documentar implicaciones estrat√©gicas: explicar que alta sensibilidad a tasas indica necesidad de hedging mediante swaps o caps, cuantificar beneficio potencial de hedging vs costo

## **WP-09: VISUALIZACIONES**

### **T-032: Dashboard resultados**

**¬øEn qu√© consiste?**  
 Crear dashboard interactivo comprehensivo que presente todos los resultados del an√°lisis de manera visual e intuitiva para stakeholders no-t√©cnicos.

**Relaciones con otras tareas:**

* **Depende de:** T-031 (pipeline MC completo), T-041 (resultados stress), T-043 (stress h√≠bridas), TAMM11 (resultados AMM)  
* **Culminaci√≥n de:** Todos los m√≥dulos anal√≠ticos, integraci√≥n final  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Crear notebook Jupyter interactivo dashboard.ipynb en carpeta notebooks/ que sirva como dashboard principal  
* **Panel 1 \- Executive Summary:** KPIs principales en formato "card": DSCR promedio con indicador verde/amarillo/rojo, probabilidad de default con gauge chart, WACD con benchmark comparison, NPV del proyecto, IRR vs hurdle rate  
* Implementar traffic light indicators: verde si m√©trica cumple target (DSCR \> 1.35), amarillo si close to threshold (1.25-1.35), rojo si breach (\< 1.25)  
* **Panel 2 \- Cash Flow Waterfall:** gr√°fico de cascada mostrando c√≥mo fluye CFADS t√≠pico a trav√©s de waterfall, barras para OPEX, intereses de cada tranche, fondeo de reservas, principal, dividendos, usar colores distintivos por categor√≠a  
* **Panel 3 \- Ratio Evolution:** l√≠neas temporales mostrando DSCR, LLCR en el tiempo, fan chart con percentiles 10-90 mostrando uncertainty band, l√≠nea horizontal marcando covenant threshold, sombrear per√≠odos con breach risk  
* **Panel 4 \- Risk Metrics:** tabla con m√©tricas de riesgo por tranche (EL, VaR\_95, CVaR\_95, PD, LGD), formato condicional resaltando tranches con mayor riesgo, gr√°fico de barras comparando EL por tranche  
* **Panel 5 \- Monte Carlo Distributions:** histogramas de distribuciones clave (DSCR m√≠nimo, NPV, default time), overlay normal fit para comparar, marcar percentiles importantes, mostrar skewness y kurtosis  
* **Panel 6 \- Stress Testing:** heatmap mostrando impacto de cada escenario de estr√©s en m√©tricas clave, ejes \= escenario √ó m√©trica, color intensity \= magnitude de impacto, tabla de resilience comparison Traditional vs Tokenized  
* **Panel 7 \- Structure Comparison:** gr√°ficos side-by-side comparando Traditional vs Tokenized en multiple dimensions (costo, riesgo, liquidez, flexibilidad), radar chart agregado, recommendation summary  
* **Panel 8 \- Pricing Analysis:** distribuciones de precios por tranche con box plots, tabla de yields y spreads, duration/convexity metrics, sensibilidad a rate changes con tornado chart  
* **Panel 9 \- AMM Liquidity:** resultados de an√°lisis de mercado secundario (de WP-14), gr√°ficos de convergencia DCF vs market price, impacto de panic sells, profundidad de liquidez por pool  
* Implementar interactividad usando widgets: sliders para ajustar par√°metros key y ver impacto en real-time, dropdowns para seleccionar scenarios diferentes, checkboxes para toggle visibility de elementos  
* Crear visualizaciones con Plotly para interactividad: hover tooltips mostrando valores exactos, zoom y pan para explorar detalles, click legends para show/hide series, export a PNG para reportes  
* Implementar tabla din√°mica explorable: usuarios pueden drill-down en m√©tricas, sort por diferentes columnas, filter por ranges, √∫til para an√°lisis ad-hoc  
* M√©todo generate\_static\_report que exporte dashboard a HTML est√°tico o PDF para sharing con stakeholders sin Python, preservar interactividad en HTML, layout profesional en PDF  
* Documentar cada visualizaci√≥n con markdown cells: explicar qu√© muestra el gr√°fico, c√≥mo interpretarlo, qu√© insights se derivan, referencias a secciones relevantes del informe acad√©mico  
* Implementar narrative flow: organizar visualizaciones en orden l√≥gico que cuenta historia del proyecto, empezar con overview, profundizar en detalles, concluir con recomendaciones

## **WP-10: OPTIMIZACI√ìN**

### **T-036: B√∫squeda WACD m√≠nimo**

**¬øEn qu√© consiste?**  
 Implementar optimizaci√≥n de estructura de capital buscando combinaci√≥n √≥ptima de pesos de tranches que minimiza WACD considerando feedback loop entre estructura, riesgo y spreads.

**Relaciones con otras tareas:**

* **Depende de:** T-028 (spreads calibrados), T-009 (WACD calculation)  
* **Relacionado con:** T-035 (frontera eficiente provee context)  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase CapitalStructureOptimizer en archivo pftoken/optimization/capital\_optimizer.py  
* Definir problema de optimizaci√≥n: minimize WACD(w\_senior, w\_mezz, w\_sub) sujeto a: w\_senior \+ w\_mezz \+ w\_sub \= 1, w\_senior \>= 0.40 (minimum por requerimiento lenders), w\_mezz \>= 0.15, w\_sub \>= 0 (can be zero), w\_i ‚àà \[0,1\]  
* Implementar feedback loop cr√≠tico: spreads NO son constantes, dependen de leverage y riesgo que a su vez dependen de pesos, ej: m√°s Senior (menor leverage) ‚Üí menor PD ‚Üí menor spread\_senior, pero tambi√©n menos tax shield y weight shifts  
* M√©todo calculate\_implied\_spreads que dado vector de pesos (w), calcule leverage ratio, ejecute MC simulation con esa estructura, obtenga PD/LGD por tranche, calibre spreads usando T-028, retorne spread\_vector \= \[s\_senior, s\_mezz, s\_sub\]  
* Implementar WACD calculation end√≥gena: WACD(w) \= Œ£\[w\_i √ó (base\_rate \+ spread\_i(w))\] donde spread\_i depende de w v√≠a feedback loop, esto hace optimization no-trivial  
* M√©todo optimize\_using\_sequential\_quadratic que use scipy.optimize.minimize con method='SLSQP': maneja constraints nonlinear y bounds, converge bien para problems smooth, especificar bounds, constraints, initial guess  
* Implementar iterative refinement: empezar con grid search grueso para identificar regi√≥n prometedora, refinar usando gradient-based optimizer, validar convergencia verificando que gradient ‚Üí 0  
* M√©todo validate\_optimum verificando que soluci√≥n satisface KKT conditions: gradient of objective \= linear combination of gradients of constraints en √≥ptimo, constraints est√°n binding o slack correctamente  
* Implementar sensibilidad del √≥ptimo: perturbar par√°metros (ej: base\_rate, PD multiplier) ligeramente, re-optimize, measure cu√°nto cambia √≥ptimo, robusto si √≥ptimo se mueve poco ante perturbaciones  
* Crear visualizaci√≥n 3D de superficie de WACD: ejes X,Y \= (w\_senior, w\_mezz), eje Z \= WACD, plot surface mostrando landscape, marcar √≥ptimo con punto rojo, mostrar contour lines en projection  
* M√©todo compare\_optimal\_vs\_current: compute WACD de estructura actual (baseline), compute WACD de estructura √≥ptima, calculate savings \= WACD\_current \- WACD\_optimal, translate a NPV savings \= PV(interest\_savings)  
* Implementar an√°lisis de trade-offs en √≥ptimo: cu√°nto cambiar√≠a WACD si agregamos constraint adicional (ej: minimum 20% sub for diversification), useful para entender cost of constraints  
* M√©todo generate\_optimization\_report: narrative explicando estructura √≥ptima encontrada, comparaci√≥n vs baseline, justificaci√≥n de por qu√© es √≥ptima, sensibilidad a assumptions, recommendations para implementaci√≥n  
* Documentar limitaciones: optimizaci√≥n asume que todos los pesos son continuously adjustable, en realidad market preferences pueden limitar, regulaciones pueden imponer constraints no modelados, model risk en spread calibration afecta √≥ptimo

## **WP-11: DERIVADOS**

### **T-045: Modelo Interest Rate Cap**

**¬øEn qu√© consiste?**  
 Modelar y analizar uso de un Interest Rate Cap como instrumento de cobertura para mitigar riesgo de aumento de tasas de inter√©s identificado en an√°lisis de sensibilidad.

**Relaciones con otras tareas:**

* **Depende de:** T-044 (sensibilidad a tasas cuantificada)  
* **Referencia:** T-008 (curva para pricing), T-027 (duration para hedging)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase InterestRateCap en archivo pftoken/derivatives/interest\_rate\_cap.py  
* Definir estructura de cap: cap es portafolio de caplets, cada caplet es call option en tasa de inter√©s, if rate\_observed \> strike entonces payoff \= notional √ó (rate \- strike) √ó period\_fraction, else payoff \= 0  
* M√©todo price\_caplet usando Black's formula: caplet\_value \= notional √ó period √ó \[F √ó N(d1) \- K √ó N(d2)\] √ó DF donde F \= forward rate, K \= strike, N \= normal CDF, d1 y d2 dependen de volatility  
* Implementar pricing de cap completo: cap\_value \= Œ£ caplet\_values sumando sobre todos los per√≠odos de reset, t√≠picamente cap tiene tenors de 3m o 6m alineados con pagos de deuda  
* M√©todo calibrate\_volatility: necesitamos volatilidad de tasas para pricing, extraer implied volatilities desde market quotes de caps comparables, si no hay market data usar historical volatility de tasas  
* Implementar selecci√≥n de strike √≥ptimo: strike bajo \= cobertura amplia pero premium alto, strike alto \= cobertura limitada pero premium bajo, analizar trade-off mediante cost-benefit analysis  
* M√©todo calculate\_hedge\_effectiveness: simular scenarios con rate increases, compare outcomes con vs sin cap, measure variance reduction, t√≠picamente cap elimina 60-80% de tail risk de rates pero costo ongoing  
* Implementar an√°lisis de carry cost: cap premium es upfront cost que reduce CFADS inicial, amortizar costo over life de cap, calcular impact en DSCR, verificar que cap no causa liquidity problems  
* M√©todo design\_hedging\_strategy: dado duration de deuda y exposure a rate risk, determinar notional √≥ptimo de cap, t√≠picamente hedge 50-100% de exposici√≥n dependiendo de risk appetite  
* Crear scenario analysis con hedge: re-run stress scenarios de T-038 pero con cap en place, mostrar que outcomes severos (high rate scenarios) est√°n mitigados, quantify benefit en terms de reducci√≥n de VaR  
* Implementar comparaci√≥n cap vs swap: cap es one-sided protection (paga solo cuando rates suben), swap es two-sided (fixed for floating), comparar costo y efectividad, t√≠picamente cap preferido para Project Finance porque preserva upside de rate decreases  
* M√©todo calculate\_break\_even\_rate: determinar nivel de rate increase necesario para que benefit de cap exceda su costo, if rates increase less que break-even entonces cap is money lost, if more entonces beneficial  
* Crear visualizaci√≥n de payoff profile: plot mostrando payoff de cap como funci√≥n de rate level, non-linear kinked profile (flat cuando rate \< strike, linear cuando rate \> strike), compare con linear profile de swap  
* Documentar recomendaciones: recomendar si proyecto debe comprar cap based en an√°lisis, qu√© strike y notional son √≥ptimos, cu√°ndo ejecutar hedge (upfront o esperar), monitoreo ongoing necesario

## **WP-14: AMM SIMPLIFICADO (REDEFINIDO)**

### **T-053: Dise√±o arquitectura AMM**

**¬øEn qu√© consiste?**  
 Dise√±ar la arquitectura conceptual del m√≥dulo AMM (Automated Market Maker) que simular√° mercado secundario de tokens de deuda usando modelos DeFi, sin implementaci√≥n blockchain real.

**Relaciones con otras tareas:**

* **Depende de:** T-041 (stress testing completo para poder aplicar shocks a AMM)  
* **Habilita:** TAMM01-TAMM11 (todas las tareas AMM subsiguientes)  
* **Duraci√≥n:** 2 d√≠as seg√∫n Gantt (tarea NUEVA en versi√≥n actualizada)

**Qu√© se debe implementar:**

* Crear m√≥dulo pftoken/amm/ con subpaquetes para pools/, pricing/, simulation/  
* Documento de dise√±o especificando objetivos del m√≥dulo AMM: (1) demostrar pricing din√°mico de mercado vs pricing fundamental DCF, (2) cuantificar liquidez disponible para diferentes tama√±os de orden, (3) simular panic sells y stress de liquidez, (4) evaluar viabilidad econ√≥mica para liquidity providers  
* Definir scope expl√≠cito: modelo es TE√ìRICO y ACAD√âMICO, no requiere deployment en blockchain real, no maneja transactions reales, no involucra wallets o gas fees reales, objetivo es explorar CONCEPTOS cuantitativos de market dynamics  
* Especificar dos arquitecturas de pool a modelar: Pool V2 (Constant Product x√óy=k estilo Uniswap V2), Pool V3 (Concentrated Liquidity con rangos estilo Uniswap V3), comparar trade-offs entre ambos designs  
* Definir flujo de datos: DCF pricing de T-007 provee "precio fundamental" de cada token, Pool inicializado a ese precio como baseline, Swaps simulados mueven precio de mercado, Arbitraje traer precio de vuelta hacia DCF  
* Documento de limitaciones y asunciones: ignora MEV y front-running, asume gas costs \= 0 (simplificaci√≥n acad√©mica), asume oracle perfecto para DCF price, asume liquidez no fragmentada en m√∫ltiples DEXs, asume arbitrajistas respond instant√°neamente  
* Especificar par√°metros configurables: liquidez inicial en cada pool (ej: $1M USDC), fee tier (ej: 30 bps), rango de liquidez para V3 (ej: \[P\_DCF √ó 0.98, P\_DCF √ó 1.02\]), estos ser√°n inputs de simulaci√≥n  
* Crear diagrama de arquitectura mostrando: m√≥dulo DCF pricing ‚Üí inicializa pools ‚Üí traders ejecutan swaps ‚Üí precio de mercado diverge ‚Üí arbitrajistas cierran brecha ‚Üí loop contin√∫a, overlayed con stress scenarios que trigger volume spikes  
* Implementar clase base Pool abstracta con m√©todos: get\_price(), execute\_swap(amount\_in, token\_in), add\_liquidity(amount0, amount1), remove\_liquidity(shares), calculate\_fees\_earned(), subclases implementan specifics  
* Documentar casos de uso principales del m√≥dulo: Caso 1 \= stress test de liquidez (100 vendedores simult√°neos), Caso 2 \= dise√±o √≥ptimo de rango de liquidez, Caso 3 \= an√°lisis de breakeven para LPs, Caso 4 \= impacto de eventos crediticios  
* Definir m√©tricas de output esperadas: precio de mercado vs DCF spread (bps), slippage por orden size, tiempo de convergencia post-shock, impermanent loss acumulado, fees earned por LPs, todas estas m√©tricas facilitan comparaci√≥n de designs

### **TAMM01: Pool V2 Constant Product**

**¬øEn qu√© consiste?**  
 Implementar pool AMM tipo Uniswap V2 usando f√≥rmula constant product x√óy=k para simular mercado secundario b√°sico de tokens.

**Relaciones con otras tareas:**

* **Depende de:** T-053 (arquitectura definida)  
* **Baseline para:** TAMM02 (V3 concentrated liquidity como extensi√≥n)  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase ConstantProductPool en archivo pftoken/amm/pools/constant\_product.py heredando de Pool  
* Atributos del pool: reserve\_token (cantidad de tokens de deuda en pool), reserve\_usdc (cantidad de USDC en pool), k\_constant \= reserve\_token √ó reserve\_usdc (invariante), total\_shares (liquidity provider shares), fee\_rate (ej: 0.003 para 30bps)  
* M√©todo initialize\_pool que dado precio\_DCF y liquidez\_total cree pool balanceado: calcular reserve\_token y reserve\_usdc tal que price \= reserve\_usdc / reserve\_token \= precio\_DCF, y reserve\_token √ó price\_DCF \= liquidez\_total / 2  
* M√©todo get\_spot\_price que retorne precio actual: price \= reserve\_usdc / reserve\_token, este es precio marginal para swap infinitesimal  
* M√©todo calculate\_swap\_output usando f√≥rmula constant product: dado amount\_in de token\_in, calcular amount\_out de token\_out preservando k \= (reserve\_in \+ amount\_in √ó (1-fee)) √ó (reserve\_out \- amount\_out), solve para amount\_out  
* Implementar execute\_swap que actualice reservas: reserve\_in \+= amount\_in, reserve\_out \-= amount\_out, verificar que k se preserva (o aumenta ligeramente por fees), actualizar precio impl√≠cito  
* M√©todo calculate\_price\_impact que compute slippage: price\_impact \= (execution\_price \- spot\_price) / spot\_price, para √≥rdenes grandes impacto puede ser significativo (varios %), √∫til para evaluar profundidad de mercado  
* Implementar add\_liquidity permitiendo LP depositar ambos tokens proporcionalmente: shares\_minted \= (liquidity\_added / total\_liquidity) √ó total\_shares, LP recibe shares representing su fracci√≥n del pool  
* M√©todo remove\_liquidity permitiendo LP retirar: recibe proporci√≥n de ambas reservas igual a su share fraction, burn shares correspondientes, √∫til para calcular retornos de LP  
* Implementar tracking de fees acumulados: cada swap contribuye fee \= amount\_in √ó fee\_rate, fees se acumulan en pool aumentando reserves, LPs capturan fees proporcionalmente cuando withdraw  
* M√©todo calculate\_impermanent\_loss que compare valor actual de LP position vs hodling: IL \= (value\_LP \- value\_hodl) / value\_hodl, t√≠picamente IL ocurre cuando price diverge significativamente de precio inicial, puede ser offset por fees earned  
* Crear simulaci√≥n de swap sequence: generar serie de trades simulando actividad normal, apply a pool, track evoluci√≥n de precio, reserves, IL over time  
* Implementar visualizaci√≥n de bonding curve: plot mostrando relaci√≥n entre reserves (x√óy=k hyperbola), price como pendiente de tangente, trade como movimiento along curve  
* Validar implementaci√≥n: verificar que swaps en ambas direcciones reversan el estado, que k aumenta monot√≥nicamente por fees, que arbitraje opportunity desaparece cuando price converge

### **TAMM02: Pool V3 Concentrated Liq**

**¬øEn qu√© consiste?**  
 Implementar pool AMM tipo Uniswap V3 con concentrated liquidity permitiendo LPs especificar rango de precios donde proveen liquidez, m√°s capital-efficient que V2.

**Relaciones con otras tareas:**

* **Depende de:** TAMM01 (baseline V2 para comparar)  
* **M√°s sofisticado que:** V2, require c√°lculos adicionales  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase ConcentratedLiquidityPool en archivo pftoken/amm/pools/concentrated\_liquidity.py  
* Modelo matem√°tico: liquidity L activa solo en rango \[P\_lower, P\_upper\], f√≥rmula: x \= L √ó (‚àöP\_upper \- ‚àöP) / (‚àöP\_upper √ó ‚àöP), y \= L √ó (‚àöP \- ‚àöP\_lower), donde x \= token reserves, y \= USDC reserves, P \= price  
* Atributos: tick\_spacing (granularidad de precios, ej: 60 ticks \= \~0.6% steps), liquidity\_positions \= lista de (P\_lower, P\_upper, L\_amount) para cada LP, active\_liquidity \= suma de L para posiciones activas en precio actual  
* M√©todo initialize\_position permitiendo LP crear position con rango espec√≠fico: LP escoge \[P\_a, P\_b\], deposita tokens seg√∫n f√≥rmula arriba, recibe position\_id, posici√≥n es activa solo cuando P ‚àà \[P\_a, P\_b\]  
* Implementar tick mathematics: precio se representa como tick \= log\_base(price), conversi√≥n P \= 1.0001^tick permite arithmetic eficiente, liquidity transitions at ticks  
* M√©todo execute\_swap\_v3 que calcule output considerando concentrated liquidity: si swap crosses multiple ranges con different liquidity, integrate piecewise, m√°s complejo que V2 pero m√°s eficiente si well-concentrated  
* Implementar capital efficiency calculation: compare V3 con liquidez concentrada vs V2 con liquidez dispersa, para mismo notional V3 puede proveer Nx m√°s liquidez en rango relevante, t√≠picamente N \= 3-10x  
* M√©todo calculate\_fee\_apr que compute retorno anualizado para LP: fees earned dependen de fracci√≥n de tiempo que position est√° in-range y de volumen de trades, APR \= (fees\_24h √ó 365\) / capital\_deployed  
* Implementar IL calculation para V3: IL depende de cu√°nto precio se movi√≥ fuera del rango, si price exits range entonces LP se queda solo con un token (peor IL que V2), trade-off entre efficiency y robustness  
* M√©todo optimize\_range\_width: dado volatilidad esperada y fee rate, encontrar rango √≥ptimo que maximiza (fees earned \- expected IL), tight range \= m√°s fees pero m√°s IL risk, wide range \= menos fees pero menos IL  
* Crear comparaci√≥n V2 vs V3: para mismo stress scenario (ej: panic sell), simular impacto en precio en ambos pools, V3 con narrow range puede tener peor price impact pero V2 desperdicia capital, cuantificar trade-off  
* Implementar just-in-time liquidity: modelar LP que agregan liquidez justo antes de grandes swaps y remueven despu√©s, capturan fees sin mucha IL exposure, controversial pero posible en V3  
* Visualizar posiciones V3: plot con precio en X-axis, liquidez en Y-axis, mostrar distribution de liquidity across price range, resaltar current price, mostrar gaps donde no hay liquidez  
* Documentar conclusiones sobre design: V3 superior para stable assets con poco price volatility, V2 mejor para volatile assets donde predecir range es dif√≠cil, recomendar based en proyecto

### **TAMM05: Market Price Calculator**

**¬øEn qu√© consiste?**  
 Implementar calculadora que extraiga pricing de mercado desde pools AMM y compare con pricing fundamental DCF para detectar discrepancias y oportunidades de arbitraje.

**Relaciones con otras tareas:**

* **Depende de:** TAMM02 (pools implementados), T-007 (DCF pricing como benchmark)  
* **Core de:** an√°lisis de convergencia precio mercado vs fundamental  
* **Duraci√≥n:** 2 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase MarketPriceAnalyzer en archivo pftoken/amm/pricing/market\_price.py  
* M√©todo get\_spot\_price que extraiga precio instant√°neo de pool: para V2 es simple ratio de reserves, para V3 es m√°s complejo requiriendo active liquidity en tick actual, normalizar a mismo numeraire (USDC por token)  
* Implementar TWAP calculation (Time-Weighted Average Price): mantener hist√≥rico de precios con timestamps, compute promedio ponderado por duraci√≥n en cada precio, TWAP smooths volatility y resiste manipulation  
* M√©todo calculate\_execution\_price que para orden de tama√±o dado compute precio promedio efectivo: simulate el swap, compare price\_average \= total\_usdc\_out / total\_tokens\_in con spot price, diferencia es slippage  
* Implementar profundidad de mercado analysis: para grid de order sizes \[100, 500, 1k, 5k, 10k tokens\], calcular execution price y slippage cada uno, plot slippage vs size mostrando liquidity curve  
* M√©todo compare\_to\_dcf que reciba precio fundamental del m√≥dulo DCF y compare: spread \= (price\_market \- price\_DCF) / price\_DCF en basis points, clasificar magnitude: \< 50bps \= tight, 50-200bps \= normal, \> 200bps \= wide/arbitrable  
* Implementar tracking hist√≥rico: almacenar time series de (timestamp, price\_market, price\_DCF, spread), √∫til para an√°lisis de convergencia temporal, persistencia de mispricing, efectividad de arbitraje  
* M√©todo detect\_arbitrage\_opportunity que identifique cuando spread excede threshold (ej: 100bps): si price\_market \> price\_DCF \+ threshold entonces tokens overpriced ‚Üí sell en mercado buy at fundamental, if price\_market \< price\_DCF \- threshold entonces underpriced ‚Üí buy en mercado sell at fundamental  
* Implementar c√°lculo de arbitrage profit potencial: profit \= |price\_market \- price\_DCF| √ó size \- transaction\_costs, para simplificaci√≥n acad√©mica asumir transaction\_costs \= pool\_fee √∫nicamente, real world tendr√≠a gas \+ slippage  
* M√©todo calculate\_convergence\_metrics: tiempo promedio para que spread revierte a \< 50bps post-shock, volatilidad del spread, maximum divergence observado, half-life de mean reversion  
* Crear visualizaci√≥n dual-axis: l√≠nea azul \= price\_market evolving over time, l√≠nea roja \= price\_DCF (m√°s estable), √°rea sombreada entre ellas \= spread, resaltar per√≠odos de wide spreads  
* Implementar an√°lisis de eficiencia de mercado: si spread persiste despite arbitrage opportunities entonces mercado inefficient, podr√≠a deberse a capital constraints de arbitrajistas, friction costs, informaci√≥n asim√©trica  
* Documentar casos donde divergencia es justificada: si hay event risk no reflejado en DCF model (ej: rumores de technical issues), price\_market puede incorporate info faster, spread no es pure arbitrage

### **TAMM06: DCF Integration Convergence**

**¬øEn qu√© consiste?**  
 Implementar mecanismo de arbitraje que simula c√≥mo traders cierran brecha entre precio de mercado y precio fundamental DCF, analizando din√°mica de convergencia.

**Relaciones con otras tareas:**

* **Depende de:** TAMM05 (detecci√≥n de mispricing), T-007 (DCF como anchor)  
* **Core conceptual:** del m√≥dulo AMM, demuestra convergencia teor√≠a-pr√°ctica  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase ArbitrageEngine en archivo pftoken/amm/arbitrage/arbitrage\_engine.py  
* M√©todo execute\_arbitrage\_trade que detecte y ejecute arbitrage: si price\_market \> price\_DCF entonces sell tokens en pool comprando USDC, si price\_market \< price\_DCF entonces buy tokens en pool vendiendo USDC, size de trade proporcional a magnitude de mispricing  
* Implementar sizing heuristic: trade\_size \= Œ± √ó liquidity √ó |spread| donde Œ± es aggressiveness parameter (0-1), larger spreads justify larger trades, limited by available liquidity en pool  
* M√©todo simulate\_arbitrage\_dynamics con loop iterativo: (1) medir spread actual, (2) si spread \> threshold ejecutar arbitrage trade, (3) actualizar precio de pool, (4) repetir hasta spread \< threshold o max iterations  
* Implementar realism via capital constraints: arbitrajistas tienen budget limitado, track capital\_available que decrece con cada trade, replenish over time simulando fundraising  
* M√©todo calculate\_convergence\_speed: medir cu√°ntos periods (o trades) requiere para que spread reduzca a \< 10bps, faster convergence indica mercado m√°s eficiente, slower indica friction  
* Implementar variantes de arbitrage strategy: aggressive (large trades closing spread r√°pidamente), conservative (small trades gradualmente), adaptive (adjust size based on volatility), comparar effectiveness  
* M√©todo analyze\_profit\_from\_arbitrage: sumar profits de todas las trades ejecutadas, subtract transaction costs (fees), compute ROI \= net\_profit / capital\_deployed, determinar si arbitrage is economically viable  
* Crear escenario de shock ex√≥geno: DCF precio suddenly drops por 10% (credit event simulation), observe c√≥mo r√°pido market price converges down, alternately DCF precio increases (positive news), observe convergence up  
* Implementar partial arbitrage: en realidad arbitrajistas no pueden close 100% de spread instant√°neamente por capital y risk constraints, modelar fractional closing: spread\_new \= spread\_old √ó (1 \- Œ±) donde Œ± \< 1  
* M√©todo propagate\_scenarios\_to\_amm que tome stress scenarios de T-038 y aplique a AMM: ej: Demand Shock \-20% ‚Üí CFADS reduce ‚Üí DCF price reduce ‚Üí spread widens ‚Üí arbitrage activity intensifies, full propagation loop  
* Crear visualizaci√≥n de convergence dynamics: plot temporal mostrando price\_market (vol√°til) converging toward price\_DCF (estable), trade points marcados, √°rea de spread disminuyendo over time  
* Implementar an√°lisis de equilibrium: identificar precio de equilibrio donde buy pressure \= sell pressure, verificar que equilibrio est√° cerca de DCF price (valida modelo), desviaciones persistentes indican market inefficiency o model error  
* Documentar hallazgos sobre speed of convergence: cuantificar half-life t√≠pica de mispricing (ej: 50% de spread cierra en 2 per√≠odos), relacionar con literatura de market microstructure sobre price discovery

### **TAMM09: Impermanent Loss Calculator**

**¬øEn qu√© consiste?**  
 Calcular Impermanent Loss (p√©rdida impermanente) sufrida por liquidity providers cuando precio de token diverge de precio inicial, crucial para evaluar viabilidad econ√≥mica de proveer liquidez.

**Relaciones con otras tareas:**

* **Depende de:** TAMM06 (din√°mica de precios establecida), TAMM01/02 (c√°lculo de LP positions)  
* **Cr√≠tico para:** An√°lisis de viabilidad de LPs, dise√±o de incentivos  
* **Duraci√≥n:** 2 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase ImpermanentLossCalculator en archivo pftoken/amm/analytics/impermanent\_loss.py  
* M√©todo calculate\_il\_v2 para constant product pools: dado price\_initial y price\_final, IL \= 2√ó‚àö(price\_ratio) / (1 \+ price\_ratio) \- 1 donde price\_ratio \= price\_final / price\_initial, resultado negativo indica loss  
* Ejemplo: si precio doubles (price\_ratio \= 2), IL \= 2√ó‚àö2 / 3 \- 1 ‚âà \-5.7%, si precio 4x entonces IL ‚âà \-20%, IL is convex function de price change  
* M√©todo calculate\_il\_v3 para concentrated liquidity: IL depends si price stays in-range o exits, if in-range IL similar a V2 pero amplificado por concentration, if out-of-range entonces LP se queda con single asset y IL es m√°ximo  
* Implementar f√≥rmula para V3 in-range: IL\_v3 \= IL\_v2 √ó concentration\_factor donde concentration\_factor \= 1 / (range\_width\_percentage), narrow ranges \= mayor IL risk per unit price change  
* M√©todo calculate\_fees\_earned que offset IL: fees \= Œ£(volume\_i √ó fee\_rate), LPs capturan fees proporcionalmente a su liquidity share, fees acumulan over time, pueden compensar IL si volume es suficiente  
* Implementar c√°lculo de break-even volume: volumen necesario tal que fees \= IL, breakeven\_volume \= IL\_absolute / fee\_rate, si actual volume \> breakeven entonces LP es profitable despite IL  
* M√©todo calculate\_total\_return\_lp que combine IL y fees: return \= (value\_final\_LP \- value\_initial) / value\_initial donde value\_final incluye both withdrawn assets \+ accrued fees, compare vs hodl return  
* Crear simulaci√≥n de diferentes price paths: (1) price stable \= no IL pero low fees, (2) price volatile but mean-reverting \= moderate IL moderate fees, (3) price trending \= high IL but potentially high fees, identify cu√°l profile mejor para LPs  
* Implementar an√°lisis de sensibilidad de IL: c√≥mo IL var√≠a con range width (V3), con volatilidad de precio, con fee tier, generar surface plots mostrando IL como funci√≥n de (price\_change, range\_width)  
* M√©todo recommend\_lp\_strategy: dado caracter√≠sticas del token (volatilidad esperada, volumen esperado), recomendar si LP should use V2 o V3, qu√© range elegir si V3, qu√© fee tier √≥ptimo  
* Crear visualizaci√≥n cl√°sica de IL curve: plot con price ratio en X-axis (0.5x, 1x, 2x, 4x), IL percentage en Y-axis, curva mostrando symmetric IL loss en ambas direcciones away from initial price  
* Implementar comparaci√≥n IL entre tranches: Senior tokens m√°s estables ‚Üí menor IL, Sub tokens m√°s vol√°tiles ‚Üí mayor IL, recomendar que LPs provean liquidez para Senior si risk-averse  
* Documentar conclusi√≥n sobre LPs: proveer liquidez es profitable si fees \> IL, requiere volumen consistente de trades, riesgo principal es price trending fuertemente en una direcci√≥n, diversificaci√≥n across m√∫ltiples pools reduce risk

### **TAMM11: Liquidity Stress Testing**

**¬øEn qu√© consiste?**  
 Simular panic sells y eventos de liquidez extremos para evaluar robustez de pools AMM bajo stress, culminaci√≥n del an√°lisis de mercado secundario.

**Relaciones con otras tareas:**

* **Depende de:** TAMM09 (IL calculado), T-038 (escenarios de stress como input), TAMM06 (arbitrage dynamics)  
* **Culminaci√≥n de:** WP-14 AMM module completo  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Clase LiquidityStressTester en archivo pftoken/amm/stress/liquidity\_stress.py  
* **Escenario 1 \- Coordinated Panic Sell:** simular 100 small holders vendiendo simult√°neamente tokens por fear, cada uno vende $10k, total \= $1M selling pressure, ejecutar todas las ventas sequentially en pool, observar price impact  
* M√©todo simulate\_panic\_sell que ejecute batch de sell orders: para cada order calcular slippage incremental, aggregate total slippage, measure final price vs initial, time to recovery post-panic  
* Implementar c√°lculo de cascading effects: large price drop puede trigger stop-losses de otros holders, modelar secondary wave de selling amplifying initial shock, feedback loop potencialmente causing market crash  
* **Escenario 2 \- Liquidity Withdrawal:** simular LPs retirando liquidez during crisis, reducing pool depth right when needed most, model withdrawal\_rate \= funci√≥n de IL y fear, self-reinforcing spiral  
* M√©todo simulate\_liquidity\_drain: start con pool de $2M liquidez, LPs withdraw 50% seeing high IL, re-simulate panic sell con menor liquidez, price impact es mucho peor, demonstrate fragility  
* Implementar c√°lculo de critical liquidity threshold: identificar m√≠nima liquidez requerida para que pool sobreviva panic sell sin price falling \> 50%, below threshold \= death spiral likely  
* **Escenario 3 \- Flash Crash Recovery:** simular sharp drop seguido por arbitrage recovery, measure resilience \= cu√°n r√°pido price rebounds to fundamental, compare V2 vs V3 recovery speed  
* M√©todo analyze\_recovery\_dynamics: despu√©s de stress, track (1) time to 50% recovery, (2) time to 90% recovery, (3) overshoot si hay, (4) final stabilization level, faster recovery \= more resilient design  
* Implementar c√°lculo de max drawdown durante crisis: max\_DD \= (peak\_price \- trough\_price) / peak\_price, measure worst-case loss para LP que entered at peak, compare across pool designs  
* **Escenario 4 \- Asymmetric Stress:** stress uno de los tranches (ej: Mezz downgraded) while Senior stays stable, observe contagion effects, does stress en Mezz pool afecta Senior pool via correlated selling  
* M√©todo propagate\_credit\_event\_to\_pools: dado credit downgrade de un tranche (from T-038 stress scenarios), model selling pressure en ese tranche's pool, spillover effects a otros pools v√≠a risk-off sentiment  
* Crear dashboard de stress results: tabla mostrando para cada scenario (stress type √ó pool design), m√©tricas: max price impact, recovery time, LP losses, arbitrajeur profits, system stability score  
* Implementar circuit breaker analysis: evaluar si trading pauses ayudar√≠an durante extreme stress, model scenario donde trading se pausa when price drops \> 20% en 1 per√≠odo, permite time for arbitrage capital to arrive  
* M√©todo recommend\_pool\_parameters: basado en stress testing results, recomendar √≥ptimo (liquidity amount, range width if V3, fee tier), trade-off between capital efficiency y robustness  
* Documentar conclusiones finales sobre AMM viability: bajo qu√© condiciones mercado secundario tokenizado es viable, cu√°ndo liquidity es insuficiente y estructura tradicional preferible, requisitos m√≠nimos de volumen y liquidez para sustainable market

## **WP-12: ENTREGABLES**

### **T-048: Notebook final integrado**

**¬øEn qu√© consiste?**  
 Crear notebook Jupyter ejecutivo que integra todos los an√°lisis en narrativa cohesiva, sirve como documento principal de presentaci√≥n del trabajo.

**Relaciones con otras tareas:**

* **Depende de:** T-032 (dashboard), T-043 (stress h√≠bridas completo), TAMM11 (AMM completo)  
* **Culminaci√≥n:** De todo el proyecto, integraci√≥n final  
* **Duraci√≥n:** 5 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Crear notebook TP\_Quant\_Final.ipynb en carpeta notebooks/ con estructura acad√©mica profesional  
* **Secci√≥n 1 \- Executive Summary:** resumen de una p√°gina con objetivos del TP, metodolog√≠a general, hallazgos principales, recomendaci√≥n final sobre tokenizaci√≥n, m√©tricas clave en tabla  
* **Secci√≥n 2 \- Introducci√≥n y Motivaci√≥n:** contexto del proyecto LEO IoT, justificaci√≥n de caso de uso, relevancia de tokenizaci√≥n en Project Finance, objetivos cuantitativos espec√≠ficos, estructura del documento  
* **Secci√≥n 3 \- Metodolog√≠a:** descripci√≥n detallada de arquitectura del modelo, m√≥dulos implementados (CFADS, Waterfall, MC, Pricing, Risk, Stress, AMM), par√°metros calibrados, supuestos clave con justificaciones acad√©micas  
* **Secci√≥n 4 \- An√°lisis Base Case:** resultados determin√≠sticos usando par√°metros esperados, CFADS proyectados con visualizaciones, Waterfall t√≠pico execution, ratios DSCR/LLCR, pricing de tranches, WACD de cada estructura  
* **Secci√≥n 5 \- Simulaci√≥n Monte Carlo:** descripci√≥n de setup estoc√°stico con distribuciones y correlaciones, resultados agregados (distribuciones de DSCR, NPV, precios), an√°lisis de sensibilidad a par√°metros clave  
* **Secci√≥n 6 \- M√©tricas de Riesgo:** EL/VaR/CVaR por tranche, an√°lisis de distribuci√≥n de p√©rdidas, concentraci√≥n de riesgo (HHI), probabilidades de breach, comparaci√≥n Traditional vs Tokenized en riesgo  
* **Secci√≥n 7 \- Stress Testing:** descripci√≥n de escenarios de estr√©s dise√±ados, resultados de cada escenario, an√°lisis de resilience, reverse stress testing findings, conclusiones sobre robustez  
* **Secci√≥n 8 \- Optimizaci√≥n de Estructura:** an√°lisis de frontera eficiente, estructura √≥ptima encontrada, comparaci√≥n con estructuras base, sensibilidad del √≥ptimo, recomendaciones  
* **Secci√≥n 9 \- An√°lisis de Mercado Secundario (AMM):** conceptual framework de AMM para tokens, resultados de simulaci√≥n de liquidez, convergencia DCF vs market price, viability de LPs, limitaciones del an√°lisis  
* **Secci√≥n 10 \- Comparaci√≥n Final:** tabla comprehensiva Traditional vs Tokenized en todas dimensiones (costo, riesgo, liquidez, flexibilidad, complejidad), scoring ponderado, recomendaci√≥n final con justificaci√≥n  
* **Secci√≥n 11 \- Conclusiones y Trabajo Futuro:** s√≠ntesis de hallazgos principales, limitaciones del modelo, extensiones propuestas, implicaciones para pr√°ctica de Project Finance  
* **Secci√≥n 12 \- Referencias:** bibliograf√≠a completa en formato APA, citar Gatti, Yescombe, Esty, Allen et al, BIS, OECD, papers relevantes de structured finance  
* Implementar ejecuci√≥n end-to-end: notebook debe ser ejecutable de principio a fin sin errores, carga datos, ejecuta an√°lisis, genera visualizaciones, produce outputs  
* Usar markdown cells extensively: explicar cada paso del an√°lisis, interpretar resultados, conectar con teor√≠a, proveer insights, mantener narrativa fluida  
* Implementar estilo acad√©mico profesional: usar LaTeX para ecuaciones, citar apropiadamente, mantener objetividad, reconocer limitaciones, evitar afirmaciones no soportadas  
* Crear TOC (table of contents) al inicio con hyperlinks a secciones, facilitar navegaci√≥n  
* Implementar secci√≥n de reproducibilidad: especificar versiones de software, proveer instrucciones de setup, listar dependencias, asegurar que resultados son replicables  
* Optimizar para presentaci√≥n: usar figuras de alta resoluci√≥n, tablas bien formateadas, colores consistentes, layout limpio, evitar clutter  
* Generar versi√≥n exportada a HTML con interactividad preservada para sharing f√°cil sin requerir Python

### **T-049: Informe PDF**

**¬øEn qu√© consiste?**  
 Generar informe acad√©mico formal en PDF de 10-15 p√°ginas que documenta trabajo pr√°ctico siguiendo est√°ndares acad√©micos, complementa notebook con formalidad.

**Relaciones con otras tareas:**

* **Depende de:** T-048 (notebook es fuente de contenido)  
* **Entregable acad√©mico:** Para evaluaci√≥n formal del TP  
* **Duraci√≥n:** 4 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Crear documento LaTeX o Word TP\_Quant\_Informe.tex/.docx en carpeta docs/  
* **Portada:** t√≠tulo completo del trabajo, subt√≠tulo indicando caso de estudio, nombres de autores, afiliaci√≥n institucional, fecha, logo institucional si aplica  
* **Resumen Ejecutivo (Abstract):** m√°ximo 250 palabras, s√≠ntesis del problema, metodolog√≠a, hallazgos principales, conclusi√≥n, keywords  
* **Tabla de Contenidos:** numerada con p√°ginas  
* **Cap√≠tulo 1 \- Introducci√≥n:** contexto de Project Finance para infraestructura satelital, tendencia hacia tokenizaci√≥n en finanzas, gap que este trabajo busca llenar, objetivos espec√≠ficos y alcance, organizaci√≥n del documento  
* **Cap√≠tulo 2 \- Marco Te√≥rico:** revisi√≥n de literatura sobre Project Finance (Gatti, Yescombe), tokenizaci√≥n de activos (OECD, BIS), market making y AMMs (Uniswap whitepapers), credit risk modeling (Merton 1973, 1974), establecer fundamentos te√≥ricos del an√°lisis  
* **Cap√≠tulo 3 \- Metodolog√≠a:** descripci√≥n formal del modelo cuantitativo, ecuaciones principales (CFADS, waterfall, pricing DCF, Merton PD), algoritmo de Monte Carlo, stress testing framework, AMM design, par√°metros calibrados con fuentes, diagrama de arquitectura del sistema  
* **Cap√≠tulo 4 \- Resultados:** presentaci√≥n estructurada de hallazgos, subsecciones para cada tipo de an√°lisis (base case, MC, risk metrics, stress, AMM), tablas y gr√°ficos selectos m√°s relevantes (no incluir todo, ser selectivo), interpretaci√≥n de cada resultado clave  
* **Cap√≠tulo 5 \- Discusi√≥n:** an√°lisis cr√≠tico de implicaciones, comparaci√≥n Traditional vs Tokenized con argumento balanceado, trade-offs identificados, contexto de resultados en literatura existente, limitaciones del modelo reconocidas expl√≠citamente  
* **Cap√≠tulo 6 \- Conclusiones:** s√≠ntesis final de si tokenizaci√≥n es ventajosa para este caso, condiciones bajo las cuales recomendaci√≥n aplica, contribuciones del trabajo, trabajo futuro sugerido  
* **Referencias Bibliogr√°ficas:** formato APA consistente, m√≠nimo 15-20 referencias incluyendo papers acad√©micos, libros de texto, reportes de instituciones, websites con dates de acceso  
* **Ap√©ndices:** (A) tabla de par√°metros completa, (B) detalles t√©cnicos de implementaci√≥n, (C) c√≥digo selecto si aplica, (D) outputs adicionales no incluidos en cuerpo principal  
* Implementar figuras profesionales: todas las figuras con captions descriptivos, numeradas secuencialmente, referenciadas en texto, alta resoluci√≥n, ejes labeled claramente, leyendas incluidas  
* Implementar tablas formales: formato acad√©mico con headers, unidades especificadas, fuentes citadas en notas al pie, formato num√©rico consistente (decimales, separadores)  
* Usar referencias cruzadas: referir a "Figura 3.2", "Tabla 4.1", "Secci√≥n 2.3", facilitar navegaci√≥n, mantener consistencia  
* Implementar estilo acad√©mico formal: tercera persona, voz pasiva cuando apropiado, lenguaje t√©cnico preciso, evitar coloquialismos, mantener objetividad  
* Generar PDF final: si LaTeX usar pdflatex con referencias y bibliograf√≠a correctamente compiladas, si Word exportar a PDF preservando formato, verificar que todos los links funcionan  
* Implementar revisi√≥n de calidad: spell check, grammar check, verificar consistencia de terminolog√≠a, revisar flujo l√≥gico entre secciones, asegurar que argumentos est√°n claramente articulados

---

### **T-050: README y docs**

**¬øEn qu√© consiste?**  
 Actualizar y finalizar toda la documentaci√≥n del proyecto para que sea completa, clara y profesional, incluyendo README principal, gu√≠as de usuario y documentaci√≥n t√©cnica.

**Relaciones con otras tareas:**

* **Actualiza:** T-002 (documentaci√≥n inicial)  
* **Depende de:** T-048 (notebook), T-049 (informe), proyecto completo finalizado  
* **Cierre:** Documentaci√≥n final del proyecto

**Qu√© se debe implementar:**

* Actualizar README.md principal en ra√≠z del repositorio con contenido comprehensivo  
* **Secci√≥n \- Descripci√≥n del Proyecto:** p√°rrafo ejecutivo explicando qu√© es, para qu√© sirve, qu√© problema resuelve, audiencia target  
* **Secci√≥n \- Caracter√≠sticas Principales:** lista bullet de capacidades clave del modelo (Monte Carlo con 10k scenarios, pricing estoc√°stico, stress testing con 6 escenarios, AMM simulation, optimizaci√≥n de estructura, etc)  
* **Secci√≥n \- Instalaci√≥n:** instrucciones paso a paso para setup, requirements system (Python 3.10+), clonar repositorio, crear virtual environment, instalar dependencias desde requirements.txt, verificar instalaci√≥n ejecutando tests  
* **Secci√≥n \- Quick Start:** ejemplo m√≠nimo de uso mostrando c√≥mo ejecutar an√°lisis b√°sico, cargar par√°metros default, run simulation, visualizar resultados, 5-10 l√≠neas de c√≥digo suficientes  
* **Secci√≥n \- Estructura del Repositorio:** √°rbol de directorios con descripci√≥n de cada carpeta y su prop√≥sito (pftoken/ \= source code, tests/ \= test suite, notebooks/ \= an√°lisis, data/ \= inputs, docs/ \= documentaci√≥n)  
* **Secci√≥n \- Documentaci√≥n Adicional:** links a notebook final, informe PDF, gu√≠a de usuario detallada, documentaci√≥n de API  
* **Secci√≥n \- Casos de Uso:** ejemplos concretos de c√≥mo usar el modelo para diferentes prop√≥sitos (evaluar estructura tradicional, comparar con tokenizada, optimizar pesos, stress test personalizado)  
* **Secci√≥n \- Requisitos y Dependencias:** lista de todas las librer√≠as necesarias con versiones espec√≠ficas, justificaci√≥n de por qu√© cada una es necesaria  
* **Secci√≥n \- Testing:** c√≥mo ejecutar test suite, coverage esperado, c√≥mo agregar nuevos tests  
* **Secci√≥n \- Contribuci√≥n:** guidelines si otros quieren contribuir, est√°ndares de c√≥digo, proceso de pull request, no obligatorio para TP pero profesional incluirlo  
* **Secci√≥n \- Licencia:** especificar licencia del proyecto (MIT, GPL, propietaria), t√©rminos de uso  
* **Secci√≥n \- Autores y Contacto:** nombres de autores, afiliaci√≥n, emails de contacto  
* **Secci√≥n \- Agradecimientos:** reconocer profesores, fuentes de datos, papers que inspiraron trabajo  
* **Secci√≥n \- Changelog:** versiones del proyecto con cambios principales en cada versi√≥n  
* Crear archivo CONTRIBUTING.md con gu√≠as detalladas: c√≥digo style (PEP 8), convenciones de naming, estructura de commits, process de testing antes de commit  
* Crear archivo API.md documentando funciones p√∫blicas principales: para cada funci√≥n proveer signature, descripci√≥n, par√°metros con tipos y descripciones, valor de retorno, ejemplos de uso, excepciones que puede lanzar  
* Crear archivo USER\_GUIDE.md con tutorial paso a paso: c√≥mo modificar par√°metros del proyecto, c√≥mo agregar nuevos escenarios de stress, c√≥mo customizar visualizaciones, c√≥mo interpretar outputs, troubleshooting de problemas comunes  
* Agregar badges al README: build status (if CI/CD), test coverage percentage, license type, Python version, makes proyecto look professional  
* Verificar que todos los links funcionan: links internos entre docs, links externos a papers y websites, fix broken links  
* Implementar consistency check: verificar que terminolog√≠a es consistente across todos los documentos, que versiones mencionadas coinciden, que ejemplos funcionan

## **WP-13: TESTING Y QA**

### **T-051: Suite completa de tests**

**¬øEn qu√© consiste?**  
 Consolidar y expandir todos los tests del proyecto asegurando cobertura comprehensiva de funcionalidad cr√≠tica.

**Relaciones con otras tareas:**

* **Depende de:** T-031 (pipeline MC como major functionality a testear)  
* **Continuo:** Testing debe ocurrir durante todo el desarrollo  
* **Duraci√≥n:** 5 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Crear suite de tests organizada en carpeta tests/ con estructura paralela a pftoken/  
* **tests/test\_cfads.py:** unit tests para m√≥dulo CFADS, test\_calculate\_revenues con inputs conocidos verifica output correcto, test\_grace\_period verifica que durante grace solo intereses se pagan, test\_ramping verifica funci√≥n phi correcta, test\_edge\_cases como CFADS negativo  
* **tests/test\_waterfall.py:** test\_priority\_of\_payments verifica orden correcto (OPEX \> intereses \> reservas \> principal), test\_default\_detection verifica que flags se activan correctamente, test\_covenant\_breach simula DSCR \< threshold y verifica remedies, test\_mra\_funding verifica l√≥gica MRA  
* **tests/test\_pricing.py:** test\_dcf\_pricing con bond simple verifica pricing matches f√≥rmula anal√≠tica, test\_yield\_calculation verifica que resolver para YTM funciona, test\_curve\_interpolation verifica smoothness  
* **tests/test\_monte\_carlo.py:** test\_distribution\_properties verifica que samples generados tienen mean/variance correctos, test\_correlation\_matrix verifica que correlaciones se preservan, test\_convergence verifica que resultados stable al aumentar N  
* **tests/test\_merton.py:** test\_distance\_to\_default con valores conocidos, test\_pd\_calculation verifica que PD increase cuando DD decrease, test\_recovery\_waterfall verifica prelaci√≥n  
* **tests/test\_stress.py:** test\_apply\_shocks verifica que shocks modifican par√°metros correctamente, test\_stress\_execution verifica que stress scenarios ejecutan sin crash  
* **tests/test\_amm.py:** test\_constant\_product verifica que k se preserva en swaps, test\_price\_impact calculation correcta, test\_liquidity\_provision verifica que shares se calculan bien  
* **tests/test\_integration.py:** integration tests que ejecutan pipeline completo end-to-end con dataset peque√±o, verifican que no hay crashes, outputs est√°n en rangos razonables, tiempo de ejecuci√≥n aceptable  
* Implementar fixtures usando pytest: crear @pytest.fixture para ProjectParams default, DebtStructure t√≠pica, evitar duplicaci√≥n de setup code  
* Implementar parametrized tests: @pytest.mark.parametrize para correr mismo test con m√∫ltiples inputs, ej: test pricing con diferentes tenors, tasas, estructuras  
* Usar mocking para tests r√°pidos: mock m√≥dulos lentos como MC simulation durante unit tests, permite testear l√≥gica sin ejecutar 10k simulaciones  
* Implementar assertions comprehensivas: no solo assert result \== expected, tambi√©n assert type(result) correcto, assert no NaN/inf en outputs, assert conservaci√≥n de cantidades (suma de flujos en waterfall)  
* Crear tests de regression: guardar outputs de versi√≥n estable, verificar que cambios futuros no rompen funcionalidad existente, alert si m√©tricas clave cambian \> threshold  
* Implementar test coverage measurement usando pytest-cov: ejecutar pytest \--cov=pftoken \--cov-report=html, generar reporte mostrando qu√© l√≠neas est√°n covered, objetivo \> 80% coverage para code cr√≠tico  
* Crear tests de performance: marcar tests lentos con @pytest.mark.slow, medir tiempo de ejecuci√≥n de funciones cr√≠ticas, alert si tiempo aumenta significativamente (regression de performance)  
* Implementar continuous testing en CI/CD: configurar GitHub Actions o similar para ejecutar tests autom√°ticamente en cada push, bloquear merge si tests fail, mantener calidad  
* Documentar cada test: docstring explicando qu√© se testea, por qu√© es importante, qu√© edge cases cubre, facilita mantenimiento futuro

---

### **T-052: Code review refactor**

**¬øEn qu√© consiste?**  
 Realizar revisi√≥n final comprehensiva del c√≥digo, refactorizar para mejorar calidad, eliminar code smells, optimizar performance, asegurar adherencia a best practices.

**Relaciones con otras tareas:**

* **Depende de:** T-051 (tests pasan antes de refactor)  
* **Final:** Limpieza y polish antes de entrega  
* **Duraci√≥n:** 3 d√≠as seg√∫n Gantt

**Qu√© se debe implementar:**

* Ejecutar linters autom√°ticos: pylint, flake8, mypy sobre todo el codebase, corregir warnings y errors, configurar para enforcing PEP 8 style, type hints correctos  
* Implementar revisi√≥n de complejidad: usar herramienta como radon para medir complejidad ciclom√°tica, funciones con complejidad \> 10 son candidatas a refactoring, split en sub-funciones m√°s simples  
* Identificar y eliminar c√≥digo duplicado: buscar bloques de c√≥digo repetidos, extract a funciones reusables, aplicar DRY principle (Don't Repeat Yourself)  
* Refactorizar funciones largas: funciones \> 50 l√≠neas probablemente hacen demasiado, split en componentes m√°s peque√±os con single responsibility, mejorar legibilidad  
* Mejorar naming: variables con nombres no descriptivos (ej: x, tmp, data) renombrar a nombres significativos (ej: tranche\_cashflows, temp\_price, simulation\_results), funciones deben tener nombres verbosos explicando qu√© hacen  
* Optimizar imports: eliminar imports no usados, organizar en orden (standard library, third-party, local), usar imports absolutos cuando posible  
* Implementar lazy evaluation: calcular valores solo cuando necesarios, avoid computations que luego no se usan, especialmente en loops  
* Optimizar loops cr√≠ticos: identificar loops que ejecutan muchas veces (ej: dentro de MC), vectorizar usando NumPy, eliminar Python loops cuando posible, puede mejorar performance 10-100x  
* Implementar caching de resultados costosos: usar @lru\_cache para funciones puras que se llaman repetidamente con mismos inputs (ej: c√°lculo de discount factors)  
* Refactorizar clases grandes: clases con muchos m√©todos (\> 20\) posiblemente violan single responsibility, considerar split en clases m√°s cohesivas  
* Mejorar error handling: reemplazar bare except: con excepciones espec√≠ficas, agregar mensajes de error descriptivos, raise excepciones custom en lugar de gen√©ricas  
* Implementar logging strategically: agregar logging.info en puntos clave del flujo, logging.debug para detalles √∫tiles en debugging, logging.warning para situaciones anormales, configurar nivel apropiadamente  
* Documentar funciones p√∫blicas: todas las funciones p√∫blicas deben tener docstrings en formato NumPy/Google style, describir par√°metros, retorno, raises, examples  
* Eliminar c√≥digo comentado: c√≥digo obsoleto en comentarios debe ser eliminado, mantiene repo limpio, confusi√≥n innecesaria  
* Implementar consistencia de estilo: verificar que todo el c√≥digo sigue mismo style guide, formatear usando black o autopep8 para consistencia autom√°tica  
* Optimizar uso de memoria: identificar objetos grandes que se mantienen en memoria innecesariamente, usar generators en lugar de listas cuando posible, del variables cuando ya no se necesitan  
* Refactorizar basado en feedback de tests: √°reas del c√≥digo donde tests fallaron frecuentemente probablemente tienen design issues, refactor para hacer m√°s testeable  
* Crear checklist de quality gates: c√≥digo debe pasar tests, linters, no memory leaks, performance aceptable, documentation completa, antes de considerar done  
* Documentar decisiones de refactoring: mantener log de cambios mayores, raz√≥n del cambio, impacto en performance/legibilidad, facilita entendimiento futuro

---

**FIN DE LA GU√çA DE IMPLEMENTACI√ìN \- 62 TAREAS TOTALES**

* 

